{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mrc_Evaluations.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNd6D1hxxEHruvVyT3yMEO7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7431f5f7993b4c2d999a88a4510014fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a1d7eaa1e1984c4dad7403d607ab582e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_aa4a1e22f1b2469586f072a64809233b",
              "IPY_MODEL_8be286fb70864984a3ba53fe80ab00ec"
            ]
          }
        },
        "a1d7eaa1e1984c4dad7403d607ab582e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aa4a1e22f1b2469586f072a64809233b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_00f93bc37d374241a0cbcf69223ff85b",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_52750d32123b4207b2ee6737d03e3e3c"
          }
        },
        "8be286fb70864984a3ba53fe80ab00ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2803e6c469d64988b53e0a45d310e164",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 696kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f500eccae8749d3820ea1d8788ff591"
          }
        },
        "00f93bc37d374241a0cbcf69223ff85b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "52750d32123b4207b2ee6737d03e3e3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2803e6c469d64988b53e0a45d310e164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f500eccae8749d3820ea1d8788ff591": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "55e1fc1c90da402881388308e9da6b1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6e0e4b56ff1d4c0fae9637c3d2688522",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ad903cfdb19a4937bc3f518fc372396b",
              "IPY_MODEL_a8527a340b224c3dbb7865f49b3b0769"
            ]
          }
        },
        "6e0e4b56ff1d4c0fae9637c3d2688522": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ad903cfdb19a4937bc3f518fc372396b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f9c413b10db4416bb513c085254fd499",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 443,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 443,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ba8220e9b6944bf999a979c1171c29a4"
          }
        },
        "a8527a340b224c3dbb7865f49b3b0769": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6776bb1447904f63a56b51e3ca892812",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 443/443 [00:29&lt;00:00, 15.1B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6b2f14eb098f41c182e9b1d3e19a3deb"
          }
        },
        "f9c413b10db4416bb513c085254fd499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ba8220e9b6944bf999a979c1171c29a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6776bb1447904f63a56b51e3ca892812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6b2f14eb098f41c182e9b1d3e19a3deb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "079680a0b13a493fa4a9b265af61c83f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cf0426839dce434697acc33bc5ab5d80",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8dbefbd57c7a450988d21d62f31d01b8",
              "IPY_MODEL_f3b005f2be1342499b0f0b8332538b16"
            ]
          }
        },
        "cf0426839dce434697acc33bc5ab5d80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8dbefbd57c7a450988d21d62f31d01b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9e7346295f024b6d869012da89b48c29",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1341090760,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1341090760,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e62800fdbe77433b96d87f1892c46b74"
          }
        },
        "f3b005f2be1342499b0f0b8332538b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5b05926ea12141029160eebe5061c392",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.34G/1.34G [00:28&lt;00:00, 46.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea3ae93f048d41a8a1684e63b76fa606"
          }
        },
        "9e7346295f024b6d869012da89b48c29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e62800fdbe77433b96d87f1892c46b74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b05926ea12141029160eebe5061c392": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea3ae93f048d41a8a1684e63b76fa606": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanbasu/aiml-capstone/blob/master/mrc_Evaluations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DRLVpTJznGZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "db0f2aa2-50a4-46eb-836b-7f2ef51f2a23"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOGCXAgWaIsd",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaepomVyaJfm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "210c5315-060e-4f77-fff8-b8b239bc2577"
      },
      "source": [
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "tf.debugging.set_log_device_placement(False)\n",
        "import pickle\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pprint\n",
        "from tensorflow.keras.layers import Bidirectional,LSTM,Dense,Dropout,BatchNormalization,Flatten,Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from numpy import array\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import f1_score,accuracy_score,precision_score\n",
        "from prettytable import PrettyTable\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "model_path = \"/content/drive/My Drive/AIML-MRC-Capstone/models/\"\n",
        "dataset_path = \"/content/drive/My Drive/AIML-MRC-Capstone/datasets/\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuIsS7WFh76y",
        "colab_type": "text"
      },
      "source": [
        "# Load Google Bucket as drive \n",
        "\n",
        "<font color=red>SKIP THIS SECTION</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCaxxOEXh8GK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 548
        },
        "outputId": "4daf1eaa-3b6f-482d-b161-dbb5162207d1"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = 'ai-ml-capstone'\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil ls\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-70c76435d261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthenticate_user\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mproject_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ai-ml-capstone'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gcloud config set project {project_id}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36mauthenticate_user\u001b[0;34m(clear_output)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mcontext_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemporary\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mclear_output\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_noop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m       \u001b[0m_gcloud_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0m_install_adc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mcolab_tpu_addr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'COLAB_TPU_ADDR'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/auth.py\u001b[0m in \u001b[0;36m_gcloud_login\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;31m# https://github.com/jupyter/notebook/issues/3159\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_getpass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mgcloud_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mgetpass\u001b[0;34m(self, prompt, stream)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m         )\n\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3NaG4vBiPvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdNVzqJ_jfs2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir gbucket\n",
        "!gcsfuse --implicit-dirs aiml-capstone gbucket \n",
        "# !umount gbucket"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in-L5Mi9kEir",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58f97f72-6ef3-42a4-bafe-2104ab57d132"
      },
      "source": [
        "!ls gbucket/lstmbaseline-0/tf-serve/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saved_model.pb\tvariables\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUPKTSR-GWt5",
        "colab_type": "text"
      },
      "source": [
        "# List of Models\n",
        "\n",
        "As part of our capstone, we are in process of evaluating the following models\n",
        "\n",
        "Data | Model | On GPU | Masking | Padding | Epoch | Location | \n",
        "--- | --- | --- | --- | --- | --- | --- | \n",
        "Without stopwords | SVM | No | - | - | - | [here](https://storage.cloud.google.com/aiml-capstone/svm/)\n",
        "Without stopwords | LSTM Baseline | No | No | Pre | 25 | [here](https://storage.cloud.google.com/aiml-capstone/lstmbaseline-0/full_context_withoutstopwords_model_epoch_lstmbaseline0_nomask_gpu.h5)\n",
        "Without stopwords | Deep LSTM + GloVe | Yes | No | Pre | 25 | [here](https://storage.cloud.google.com/aiml-capstone/deeplstm/full_context_withoutstopwords_model_epoch_deeplstm_gpu.h5)\n",
        "Without stopwords | Bi-LSTM + GloVe | No | No | Pre | 10 | [here](https://storage.cloud.google.com/aiml-capstone/bilstm/full_context_withoutstopwords_model_epoch_10_bilstm_cpu.h5)\n",
        "Without stopwords | Bi-LSTM + GloVe + Q2C Attention | Yes | No | Pre | 25 | [here](https://storage.cloud.google.com/aiml-capstone/bilstm-q2c-attention-glove/full_context_withoutstopwords_nomask_epoch_25_bilstm_q2c-attention_glove_gpu.h5)\n",
        "Without stopwords | Bi-LSTM + GloVe + Q2C-C2Q Attention | Yes | No | Pre | 25 | [here](https://storage.cloud.google.com/aiml-capstone/bilstm-bidaf-glove/full_context_withoutstopwords_model_epoch_25_bilstm_bidaf_glove_nomask_gpu.h5)\n",
        "--- | --- | --- | --- | --- | --- | --- | \n",
        "With Stopwords | Bi-LSTM + Universal Sentence Encoder + Q2C Attention\n",
        "With Stopwords | Bi-LSTM + Universal Sentence Encoder + Q2C-C2Q Attention\n",
        "With Stopwords | BERT + Universal Sentence Encoder \n",
        "With Stopwords | GPT2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_GOOYs6cXfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "26de0c80-f9c5-4c92-c946-dd2cf75e1903"
      },
      "source": [
        "# List of all models and its meta info\n",
        "list_of_models = [\n",
        "                  # {\n",
        "                  #   \"id\":\"lstm-baseline\",\n",
        "                  #   \"name\":\"LSTM Baseline\",\n",
        "                  #   \"type\":\"Without stopwords\",\n",
        "                  #   \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-0/full_context_withoutstopwords_model_epoch_lstmbaseline0_nomask_gpu.h5\"\n",
        "                  # },\n",
        "                  # {\n",
        "                  #   \"id\":\"deeplstm-glove\",\n",
        "                  #   \"name\":\"Deep LSTM + GloVe\",\n",
        "                  #   \"type\":\"Without stopwords\",\n",
        "                  #   \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/deeplstm/full_context_withoutstopwords_model_epoch_25_deeplstm_glove_nomask_gpu.h5\"\n",
        "                  # },  \n",
        "                  # {\n",
        "                  #   \"id\":\"bilstm-glove\",\n",
        "                  #   \"name\":\"Bi-LSTM + GloVe\",\n",
        "                  #   \"type\":\"Without stopwords\",\n",
        "                  #   \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm/full_context_withoutstopwords_model_epoch_25_bilstm_glove_nomask_gpu.h5\"\n",
        "                  # },\n",
        "                  # {\n",
        "                  #   \"id\":\"bilstm-glove-q2c-attention\",\n",
        "                  #   \"name\":\"Bi-LSTM + GloVe + Q2C Attention\",\n",
        "                  #   \"type\":\"Without stopwords\",\n",
        "                  #   \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-glove/full_context_withoutstopwords_model_epoch_25_bilstm_q2c-attention_glove.h5\"\n",
        "                  # },  \n",
        "                  # {\n",
        "                  #   \"id\":\"bilstm-bidaf-glove\",\n",
        "                  #   \"name\":\"Bi-LSTM + GloVe + Q2C-C2Q Attention\",\n",
        "                  #   \"type\":\"Without stopwords\",\n",
        "                  #   \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-bidaf-glove/full_context_withoutstopwords_model_epoch_25_bilstm_bidaf_glove_nomask_gpu_after_fix.h5\"\n",
        "                  # },\n",
        "                  # {\n",
        "                  #   \"id\":\"lstmbaseline-use-withstop\",\n",
        "                  #   \"name\":\"LSTM Baseline + Universal Sentence Encode\",\n",
        "                  #   \"type\":\"With stopwords\",\n",
        "                  #   \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-use-withstop/full_context_withstopwords_model_epoch_25_lstmbaseline0_nomask_gpu.h5\"\n",
        "                  # }, \n",
        "                  # {\n",
        "                  #   \"id\":\"bilstm-use-withstop\",\n",
        "                  #   \"name\":\"Bi-LSTM + Universal Sentence Encode\",\n",
        "                  #   \"type\":\"With stopwords\",\n",
        "                  #   \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-use-withstop/full_context_withstopwords_model_epoch_25_bilstm_use_nomask_gpu.h5\"\n",
        "                  # },  \n",
        "                  # {\n",
        "                  #   \"id\":\"bilstm-q2c-attention-use-withstop\",\n",
        "                  #   \"name\":\"Bi-LSTM + Q2C Attention + Universal Sentence Encode\",\n",
        "                  #   \"type\":\"With stopwords\",\n",
        "                  #   \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-use-withstop/full_context_withstopwords_model_epoch_25_bilstm_q2c-attention_use.h5\"\n",
        "                  # },                                                       \n",
        "                  # {\n",
        "                  #   \"id\":\"bert-deeppavlov\",\n",
        "                  #   \"name\":\"BERT + Cased_L-12_H-768_A-12 + DeepPavlov\",\n",
        "                  #   \"type\":\"BERT\",\n",
        "                  #   \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/bert/bert-results.csv\"\n",
        "                  # },\n",
        "                  {\n",
        "                    \"id\":\"bert-huggingface\",\n",
        "                    \"name\":\"BERT + Uncased_L-24_H-1024_A-24 + Huggingface\",\n",
        "                    \"type\":\"BERT\",\n",
        "                    \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/bert/bert-huggingface-results.csv\"\n",
        "                  }                                                                                          \n",
        "                  ]\n",
        "\n",
        "list_of_models\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'bert-huggingface',\n",
              "  'loc': '/content/drive/My Drive/AIML-MRC-Capstone/models/bert/bert-huggingface-results.csv',\n",
              "  'name': 'BERT + Uncased_L-24_H-1024_A-24 + Huggingface',\n",
              "  'type': 'BERT'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xfm8h4IBSGbh",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEtw1J-aPecV",
        "colab_type": "text"
      },
      "source": [
        "## bAbi \n",
        "\n",
        "Reference Paper = https://arxiv.org/pdf/1502.05698.pdf \n",
        "\n",
        "GitHub - https://github.com/facebookarchive/bAbI-tasks\n",
        "\n",
        "![alt text](https://storage.cloud.google.com/aiml-capstone/Screenshot%202020-06-20%20at%2011.11.50%20AM.png)\n",
        "![alt text](https://storage.cloud.google.com/aiml-capstone/Screenshot%202020-06-20%20at%2011.11.03%20AM.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ5nidQ8SVog",
        "colab_type": "text"
      },
      "source": [
        "## SQuAD test dataset and published dev set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACmlhKB1S1ZF",
        "colab_type": "text"
      },
      "source": [
        "Refer to this eval metrics - https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
        "\n",
        "Eval dataset from SQuAD - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
        "\n",
        "Test Dataset from training = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGFyqInm7ZD0",
        "colab_type": "text"
      },
      "source": [
        "## News Domain Specific Evaluations\n",
        "\n",
        "Test on Sample News Context, Question and Answer pairs ??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Yw8C9nY7uXT",
        "colab_type": "text"
      },
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDyvisumUImJ",
        "colab_type": "text"
      },
      "source": [
        "# Common Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XtBcy2Qwm2-c"
      },
      "source": [
        "## Custom function for preprocessing of context and question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LFr3-S_Gm9FX",
        "colab": {}
      },
      "source": [
        "# remove unwanted chars\n",
        "# convert to lowercase\n",
        "# remove unwanted spaces\n",
        "# remove stop words\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "## reference \n",
        "def decontracted(phrase):\n",
        "    \"\"\"\n",
        "    This function remooves punctuation from given sentence.\n",
        "    \"\"\"\n",
        "\n",
        "    if(phrase is np.nan):\n",
        "      return 'impossible'      \n",
        "\n",
        "    try:      \n",
        "      # specific\n",
        "      phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "      phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "      # general\n",
        "      phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "      phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "      phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "      phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "      phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "      phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "      phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "      phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "      \n",
        "      # string operation\n",
        "      phrase = phrase.replace('\\\\r', ' ')\n",
        "      phrase = phrase.replace('\\\\\"', ' ')\n",
        "      phrase = phrase.replace('\\\\n', ' ')\n",
        "\n",
        "      phrase = re.sub('[^A-Za-z0-9]+', ' ', phrase.lower())\n",
        "    except:\n",
        "      print(phrase)  \n",
        "    \n",
        "    return phrase\n",
        "\n",
        "def preprocess_text(corpus, text_lower_case=True, \n",
        "                      special_char_removal=True, stopword_removal=True, remove_digits=False):    \n",
        "    normalized_text = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # doc = decontracted(doc)\n",
        "        # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        # remove special characters and\\or digits    \n",
        "        if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = remove_special_characters(doc, remove_digits=remove_digits) \n",
        "\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc)\n",
        "\n",
        "        normalized_text.append(doc)\n",
        "        \n",
        "    return normalized_text\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    #Using regex\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text):  \n",
        "    word_tokens = word_tokenize(text) \n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]   \n",
        "    filtered_sentence = [] \n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(w)                 \n",
        "    return ' '.join(filtered_sentence)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "20-D8HBeOf_p"
      },
      "source": [
        "## Answer Span from Context and Answer, and reverse for predicted spans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vbM8z2AEjxKK",
        "colab": {}
      },
      "source": [
        "def tokenize(sentence):\n",
        "    \"\"\"\n",
        "    Returns tokenised words.\n",
        "    \"\"\"\n",
        "    return nltk.word_tokenize(sentence)\n",
        "\n",
        "def answer_span(context,ans):\n",
        "    \"\"\"\n",
        "    This funtion returns anwer span start index and end index.\n",
        "    \"\"\"\n",
        "    ans_token = tokenize(ans)\n",
        "    con_token = tokenize(context)\n",
        "    ans_len = len(ans_token)\n",
        "    \n",
        "    if ans_len!=0 and ans_token[0] in con_token:\n",
        "    \n",
        "        indices = [i for i, x in enumerate(con_token) if x == ans_token[0]]        \n",
        "        try:\n",
        "\n",
        "            if(len(indices)>1):\n",
        "                start = [i for i in indices if (con_token[i:i+ans_len] == ans_token) ]\n",
        "                end = start[0] + ans_len - 1\n",
        "                return start[0],end\n",
        "\n",
        "            else:\n",
        "                start = con_token.index(ans_token[0])\n",
        "                end = start + ans_len - 1\n",
        "                return start,end\n",
        "        except:\n",
        "            return -1,-1\n",
        "    else:\n",
        "        return -1,-1\n",
        "\n",
        "def span_to_answer(span, context):\n",
        "  con_token = tokenize(context)  \n",
        "  return ' '.join(con_token[span[0]:span[1]+1])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_sveyxKK5j8q"
      },
      "source": [
        "## Update and persist params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K170rfN15qGP",
        "colab": {}
      },
      "source": [
        "### SAVE PARAMS\n",
        "# Writing to sample.json \n",
        "\n",
        "def updateparams():\n",
        "  with open(model_path + \"params.json\", \"w\") as p: \n",
        "    p.write(json.dumps(params))\n",
        "  print(\"params.jsop updated and can be found in \", model_path + \"params.json\")  \n",
        "\n",
        "# updateparams()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yYMDmMJNU9mw",
        "colab": {}
      },
      "source": [
        "def showparams(params):\n",
        "  pprint.pprint(params)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SklWt_NbVzOU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "fde5fa7a-fdd2-44b0-f6fe-33bb34106ab1"
      },
      "source": [
        "def loadparams(name='params_withoutstopwords.json'):\n",
        "  with open(model_path + name) as f:\n",
        "    params = json.load(f)\n",
        "  return params  \n",
        "\n",
        "params = loadparams(name='params_withoutstopwords.json')\n",
        "showparams(params)\n",
        "showparams(loadparams())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 300,\n",
            " 'prediction.accuracy.score': 0.3322461821809531,\n",
            " 'prediction.macrof1.score': 0.011907387665813255,\n",
            " 'prediction.microf1.score': 0.25672221926414995,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': [26062, 16],\n",
            " 'test_span_outofrange': 0,\n",
            " 'train_shape': [78183, 16],\n",
            " 'train_span_outofrange': 0,\n",
            " 'training.batch_size': 64,\n",
            " 'training.epochs': 25,\n",
            " 'training.train_length': 78183,\n",
            " 'training.train_steps': 1221,\n",
            " 'training.val_length': 26061,\n",
            " 'training.val_steps': 814,\n",
            " 'val_shape': [26061, 16],\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 100850}\n",
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 300,\n",
            " 'prediction.accuracy.score': 0.3322461821809531,\n",
            " 'prediction.macrof1.score': 0.011907387665813255,\n",
            " 'prediction.microf1.score': 0.25672221926414995,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': [26062, 16],\n",
            " 'test_span_outofrange': 0,\n",
            " 'train_shape': [78183, 16],\n",
            " 'train_span_outofrange': 0,\n",
            " 'training.batch_size': 64,\n",
            " 'training.epochs': 25,\n",
            " 'training.train_length': 78183,\n",
            " 'training.train_steps': 1221,\n",
            " 'training.val_length': 26061,\n",
            " 'training.val_steps': 814,\n",
            " 'val_shape': [26061, 16],\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSJXR52VTWsf",
        "colab_type": "text"
      },
      "source": [
        "## Load Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5FJHi14rHNTX",
        "colab": {}
      },
      "source": [
        "def load_tokenizer(params=params, name=\"glove\"):\n",
        "  if(name=='glove'):\n",
        "    print('Loading GloVe 300D')\n",
        "    with open(model_path + \"tokenizer.pkl\",\"rb\") as infile:\n",
        "        tokenizer = pickle.load(infile)\n",
        "    print('Vocab Loaded - ',len(tokenizer.word_index))\n",
        "    if(params):\n",
        "      params['vocab_size'] = len(tokenizer.word_index)\n",
        "    return tokenizer\n",
        "  elif (name=='use'):\n",
        "    print('Loading Universal Sentence Encoder')\n",
        "    with open(model_path + \"tokenizerwithstopwordspunct.pkl\",\"rb\") as infile:\n",
        "        tokenizer = pickle.load(infile)\n",
        "    print('Vocab Loaded - ',len(tokenizer.word_index))\n",
        "    if(params):\n",
        "      params['vocab_size'] = len(tokenizer.word_index)    \n",
        "    return tokenizer       "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0TSjNkdXzuR"
      },
      "source": [
        "## Create a common function to generate sequences (useful in prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sq97Vq4kXTST",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2339a0bd-bf2c-42ef-edff-ef8b41135516"
      },
      "source": [
        "# function to generate sequences withg appropiate padding\n",
        "tokenizer = load_tokenizer()\n",
        "def generate_question_context_sequence(context,question,question_max_length,padding,context_max_length):\n",
        "  question_seq = tokenizer.texts_to_sequences(question)\n",
        "  context_seq = tokenizer.texts_to_sequences(context)\n",
        "  question_seq = preprocessing.sequence.pad_sequences(question_seq,\n",
        "                                                      maxlen=question_max_length,\n",
        "                                                      padding=padding)\n",
        "  context_seq = preprocessing.sequence.pad_sequences(context_seq,\n",
        "                                                     maxlen=context_max_length,\n",
        "                                                     padding=padding)\n",
        "  return context_seq, question_seq"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading GloVe 300D\n",
            "Vocab Loaded -  100850\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v78fhG3Tdhx5"
      },
      "source": [
        "## Create a common function to predict and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v6KDfM25dhZr",
        "colab": {}
      },
      "source": [
        "def predit_test(context, question, modeltoUse, params=params):\n",
        "  # get sequence for context and question\n",
        "  c_ = preprocess_text(context,stopword_removal=False)\n",
        "  q_ = preprocess_text(question,stopword_removal=False)\n",
        "  answer=[]\n",
        "  spans=[]\n",
        "\n",
        "  \n",
        "\n",
        "  c,q = generate_question_context_sequence(context=c_,\n",
        "                                           question=q_,\n",
        "                                           question_max_length=params['question_max_length'],\n",
        "                                           padding=params['question_pad_seq'],\n",
        "                                           context_max_length=params['context_max_length'])\n",
        "\n",
        "  context_token = []\n",
        "  print(c)\n",
        "  # tokenizer.word_docs\n",
        "  tokenizer.word_index['how']\n",
        "  print(list(tokenizer.word_index.keys())[list(tokenizer.word_index.values()).index(39)])\n",
        "  for ci in c:    \n",
        "    tokens = {}\n",
        "    for i in ci:\n",
        "      if(i != 0):\n",
        "        print(tokenizer.word_docs[i])\n",
        "        # tokens[tokenizer.word_index[i]] = i\n",
        "    # context_token.append(ci)\n",
        "  # print(context_token)\n",
        "\n",
        "\n",
        "  y_ = modeltoUse.predict([q,c])    \n",
        "  # print(y_)\n",
        "  pred_token = {}\n",
        "  for i in range(len(context)):    \n",
        "    s = np.argmax(y_[i,:params['context_max_length']])\n",
        "    e = np.argmax(y_[i,params['context_max_length']:])\n",
        "    answer.append(span_to_answer((s,e),c_[i]))\n",
        "    spans.append([s,e])\n",
        "    # pred_token[]\n",
        "  \n",
        "  # print(c.shape,q.shape,y_.shape,s,e,answer)  \n",
        "  # print(s, e)\n",
        "  return c_,q_,spans,y_,answer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIjvBaQ4WFZQ",
        "colab_type": "text"
      },
      "source": [
        "## Load Test Data with Ground Truth Known"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYkWpLxxWFii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import resample,shuffle\n",
        "\n",
        "def load_data_withoutstopwords():\n",
        "  #### NOTE THE 2 data frames's\n",
        "  df_nostopwords = 'test_squad_data_final_context_withoutstopwords.csv'\n",
        "  # df_withstopwords = 'squad_data_final_withstopword_withpunctuation.csv'\n",
        "  test_squad_df = pd.read_csv(project_path+df_nostopwords)\n",
        "  test_squad_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  test_squad_df[\"answer_word_span\"] = test_squad_df[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "  print(test_squad_df.info())\n",
        "  return test_squad_df"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH025QzmyMAJ",
        "colab_type": "text"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDX397bcyMZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logits_loss(y_true,logits):\n",
        "    \"\"\"\n",
        "    Custom loss function which minimises log_loss.\n",
        "    Referance https://stackoverflow.com/questions/50063613/add-loss-function-in-keras\n",
        "    \"\"\"\n",
        "    \n",
        "    #y_true = tf.cast(y_true,dtype=tf.int32)\n",
        "    #logits = tf.cast(logits,dtype=tf.float32)\n",
        "    \n",
        "    # breaking the tensor into two half's to get start and end label.\n",
        "    start_label = y_true[:,:params['context_max_length']]\n",
        "    end_label = y_true[:,params['context_max_length']:]\n",
        "    \n",
        "    # braking the logits tensor into start and end part for loss calcultion.\n",
        "    start_logit = logits[:,:params['context_max_length']]\n",
        "    end_logit = logits[:,params['context_max_length']:]\n",
        "    \n",
        "    start_loss = tf.keras.backend.categorical_crossentropy(start_label,start_logit)\n",
        "    end_loss = tf.keras.backend.categorical_crossentropy(end_label,end_logit)\n",
        "    \n",
        "#     start_loss = tf.losses.sparse_softmax_cross_entropy(labels=start_label, logits=start_logit)\n",
        "#     end_loss = tf.losses.sparse_softmax_cross_entropy(labels=end_label, logits=end_logit)\n",
        "    \n",
        "    # as per paer\n",
        "    \n",
        "    loss = start_loss + end_loss\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP43qWYkewja",
        "colab_type": "text"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pSzIAlTe07P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "def load_mrc_model(model_path):\n",
        "  custom_objects = {\"logits_loss\": logits_loss}\n",
        "  new_model = keras.models.load_model(model_path,custom_objects=custom_objects)\n",
        "  ### Check its architecture\n",
        "  # new_model.summary()  \n",
        "  return new_model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2fUNm7uPWSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# strategy = tf.distribute.MirroredStrategy()\n",
        "# with strategy.scope():\n",
        "#   model = load_mrc_model('/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm/full_context_withoutstopwords_model_epoch_25_bilstm_glove_nomask_gpu.h5')\n",
        "  \n",
        "# model.summary()\n",
        "# # list_of_models['bilstm-glove-q2c-c2q-attention']['loc'].replace('https://storage.cloud.google.com/aiml-capstone/','gbucket/')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DURR8zGyUpy6",
        "colab_type": "text"
      },
      "source": [
        "## Load Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWgeXhRLUrj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_test_data(name='test-withoutstopwords.csv'):\n",
        "  test = pd.read_csv(model_path +name)\n",
        "  test.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "  test[\"answer_word_span\"] = test[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "  test.loc[test['answer'].isna(), 'answer'] = ''\n",
        "  test.loc[test['plausible_answer'].isna(), 'plausible_answer'] = ''\n",
        "  print(test.shape)\n",
        "  return test"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2P1tCiIn3a3",
        "colab_type": "text"
      },
      "source": [
        "## Load CNN dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdQS49J4n4D8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cnndf = pd.read_csv(dataset_path + 'CNN/TrainingDatasets/cnnTrainingData.csv')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFKbElDgn35o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cnndf.head(5)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-EuIRXxPFZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# del cnndf"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I2iBIhFlpE_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import textwrap\n",
        "# wrapper = textwrap.TextWrapper(width=80) \n",
        "# print(wrapper.fill(cnndf['context'].iloc[0]))\n",
        "# print()\n",
        "# print(wrapper.fill(cnndf['question'].iloc[4]))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoXsJRF_Vah4",
        "colab_type": "text"
      },
      "source": [
        "## Create Answer Sequence for test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rregOZyRS_Da",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_answer_sequence(test, context_length):\n",
        "  # for test data\n",
        "  y_test = []\n",
        "  for i in range(len(test)):    \n",
        "      s = np.zeros(context_length,dtype = \"int\")\n",
        "      e = np.zeros(context_length,dtype = \"int\")        \n",
        "      start,end = test[\"answer_word_span\"].iloc[i]    \n",
        "      s[start] = 1\n",
        "      e[end] = 1\n",
        "      y_test.append(np.concatenate((s,e)))\n",
        "  return y_test\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDeMi7IWeb6_",
        "colab_type": "text"
      },
      "source": [
        "## Create a combined y_test array having both start and end vectors in OHE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3S3SB2cfFR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combine_y(y, test_sample_size):\n",
        "  # argmax is used to get the index where the max value in a list appears, and hence \n",
        "  # for every index i, we can get the place of start and end token of the max probab\n",
        "  start = []\n",
        "  end = []\n",
        "  for i in range(test_sample_size):\n",
        "      start.append(np.argmax(y[i,:params['context_max_length']]))\n",
        "      end.append(np.argmax(y[i,params['context_max_length']:]))\n",
        "      \n",
        "  y_new = np.zeros((test_sample_size,params['context_max_length']))\n",
        "  for i in range(test_sample_size):\n",
        "      y_new[i,start[i]:end[i]+1] = 1\n",
        "  return y_new,start,end"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUd9-eqtioBM",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0e-TuininW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_metrics(y_true,y_pred):\n",
        "  acc_score = accuracy_score(y_true,y_pred)\n",
        "  macro_f1_score = f1_score(y_true,y_pred,average=\"macro\")\n",
        "  micro_f1_score = f1_score(y_true,y_pred,average=\"micro\")\n",
        "  return acc_score,macro_f1_score,micro_f1_score\n",
        "  "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqs4OxyjszkY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ccdfbc99-969e-465d-d01d-8d01a621a9e6"
      },
      "source": [
        "accuracy_metrics([1,1,1],[0,1,1])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6666666666666666, 0.4, 0.6666666666666666)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnoYgva4_iEI",
        "colab_type": "text"
      },
      "source": [
        "## Exact Match Count per Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf3h0HWD_iyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def checkEMMatch(test, start, end):\n",
        "  field_names = [\"True Answer\",\n",
        "                \"True AS and AE\",\n",
        "                \"Predict Answer\",\n",
        "                \"Predict AS and AE\"]\n",
        "  result_df = pd.DataFrame(columns=field_names)\n",
        "  print('Checking for equality match percentage')\n",
        "  for i in tqdm(range(test.shape[0])):  \n",
        "    values = [test['clean_answer'].iloc[i], \n",
        "              test['answer_word_span'].iloc[i],\n",
        "              span_to_answer([start_pred[i],end_pred[i]],test['clean_context'].iloc[i]),\n",
        "              (start_pred[i],end_pred[i])]\n",
        "    zipped = zip(field_names, values)\n",
        "    a_dictionary = dict(zipped)\n",
        "    result_df = result_df.append(a_dictionary,ignore_index=True)\n",
        "\n",
        "  ematch = result_df[result_df['Predict Answer'] == result_df['True Answer']].shape[0]  / test.shape[0]\n",
        "  del result_df\n",
        "\n",
        "  return ematch\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvVeDCNqX5pr",
        "colab_type": "text"
      },
      "source": [
        "## Generate y_preds in text from predicted start and end span"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eDXknqOX5Cq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_y_preds_text(test, start, end):\n",
        "  y_preds = []\n",
        "  for i in tqdm(range(test.shape[0])):\n",
        "    y_preds.append(span_to_answer([start_pred[i],end_pred[i]],test['context'].iloc[i]))\n",
        "  return y_preds"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvA32P7IjPSZ",
        "colab_type": "text"
      },
      "source": [
        "## Load all model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xDin7-igafk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_all_models():\n",
        "  loaded_models = {}\n",
        "  for model in list_of_models:\n",
        "    print('Loading model ',model['name'],' from ', model['loc'])  \n",
        "    if(model['type'] == 'BERT'):\n",
        "      print('BERT is WIP !!')\n",
        "      continue    \n",
        "    tfmodel = load_mrc_model(model['loc'])\n",
        "    loaded_models[model['name']] = tfmodel\n",
        "  return loaded_models"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p87BPm-enj0t",
        "colab_type": "text"
      },
      "source": [
        "## SQuAD Official Evaluation Script\n",
        "\n",
        "Refer to https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsd_6KDXtTOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import collections\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import sys"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AJyEUWDzIrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getpred(test, y_predict):\n",
        "  pred = {}\n",
        "  for i in range(test.shape[0]):\n",
        "    pred[test['id'].iloc[i]] = y_predict[i]\n",
        "\n",
        "  return pred  "
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGn2mA7atJEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "    return re.sub(regex, ' ', text)\n",
        "  def white_space_fix(text):\n",
        "    if(text != text):\n",
        "      return ''\n",
        "    return ' '.join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKsIlANKnvuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_qid_to_has_ans(test):\n",
        "  qid_to_has_ans = {}\n",
        "  for i in range(test.shape[0]):\n",
        "    qid_to_has_ans[test['id'].iloc[i]] = bool(test['answer'].iloc[i])\n",
        "  return qid_to_has_ans"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHL-JtnCx70o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tokens(s):\n",
        "  if not s: return []\n",
        "  return normalize_answer(s).split()"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWjFQTjLx51X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2201ecbe-1571-41f6-fb62-9f83ea6fe6a1"
      },
      "source": [
        "def compute_exact(a_gold, a_pred, type='a'):\n",
        "  if(type=='a'):\n",
        "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "  else:\n",
        "    return int((normalize_answer(a_gold) != '') and (normalize_answer(a_gold) == normalize_answer(a_pred)))\n",
        "\n",
        "def compute_f1(a_gold, a_pred,  type='a'):\n",
        "  gold_toks = get_tokens(a_gold)\n",
        "  pred_toks = get_tokens(a_pred)\n",
        "  # find number of common tokens\n",
        "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "  # count the common tokens\n",
        "  num_same = sum(common.values())\n",
        "  if (len(gold_toks) == 0 or len(pred_toks) == 0) and type == 'a':\n",
        "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "    return int(gold_toks == pred_toks)\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  # precision is always to y_pred - 1 * (ratio of common token / total tokens in y_pred)\n",
        "  precision = 1.0 * num_same / len(pred_toks)\n",
        "  # recal is always to y_true - 1 * (ratio of common token / total tokens in y_true)\n",
        "  recall = 1.0 * num_same / len(gold_toks)\n",
        "  # F1 score formulais as below - 2*((precision*recall)/(precision+recall))\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "# Test the sklearn f1 versus SQuAD F1 metrics func\n",
        "print('scklearn F1 =',f1_score(['by marriage through coburgs'],['by marriage through the Coburgs'],average=\"macro\"))\n",
        "print('custom F1 =',compute_f1('by marriage through coburgs','by marriage through the Coburgs'))  \n",
        "\n",
        "# equals\n",
        "print('custom exact =',compute_exact('by marriage through coburgs','by marriage through the Coburgs'))  \n",
        "print('exact exact =',int('by marriage through coburgs' == 'by marriage through the Coburgs'))  \n",
        "\n",
        "## Clearly the custom f1 and exact makes so much sense "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scklearn F1 = 0.0\n",
            "custom F1 = 1.0\n",
            "custom exact = 1\n",
            "exact exact = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHykDpVjUQz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_eval_dict(exact_scores, f1_scores,exact_scores_pa,f1_scores_pa, qid_list=None):\n",
        "  total = len(exact_scores)\n",
        "  # return collections.OrderedDict([\n",
        "  #     ('exact', 100.0 * sum(exact_scores.values()) / total),\n",
        "  #     ('f1', 100.0 * sum(f1_scores.values()) / total),\n",
        "  #     ('total', total),\n",
        "  # ])\n",
        "  exact = 100.0 * sum(exact_scores.values()) / total\n",
        "  f1 = 100.0 * sum(f1_scores.values()) / total\n",
        "  exact_pa = 100.0 * sum(exact_scores_pa.values()) / total\n",
        "  f1_pa = 100.0 * sum(f1_scores_pa.values()) / total\n",
        "  return exact, f1, exact_pa, f1_pa, total"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpGEMJJasDap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  preds = {\n",
        "#     \"id\" : 'predicted answer'    \n",
        "#      }\n",
        "def get_raw_scores(test, preds):\n",
        "    exact_scores = {}\n",
        "    f1_scores = {}    \n",
        "    exact_scores_pa = {}\n",
        "    f1_scores_pa = {}        \n",
        "    for i in range(test.shape[0]):\n",
        "      if 'id' in test.columns:\n",
        "        qid = test['id'].iloc[i]\n",
        "      else:\n",
        "        qid = i\n",
        "      gold_a = normalize_answer(test['answer'].iloc[i])\n",
        "      gold_pa = normalize_answer(test['plausible_answer'].iloc[i])\n",
        "      if not gold_a:\n",
        "        # For unanswerable questions, only correct answer is empty string\n",
        "        gold_a = ''\n",
        "\n",
        "      exact_scores[qid] = compute_exact(gold_a, preds[i])\n",
        "      f1_scores[qid] = compute_f1(gold_a, preds[i])\n",
        "      exact_scores_pa[qid] = compute_exact(gold_pa, preds[i], type='pa')\n",
        "      f1_scores_pa[qid] = compute_f1(gold_pa, preds[i], type='pa')      \n",
        "\n",
        "    return exact_scores, f1_scores,exact_scores_pa,f1_scores_pa"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C42zkDJ-o4JF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "50c80cf3-76c4-414b-cb41-ad92731d5e3b"
      },
      "source": [
        "test = load_test_data()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(26062, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_XZUPY400oC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "d2d6fbec-05b0-4f73-92ab-1fce8324409c"
      },
      "source": [
        "test = test.head(5)\n",
        "test"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer</th>\n",
              "      <th>plausible_answer_start</th>\n",
              "      <th>plausible_answer</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>clean_context</th>\n",
              "      <th>clean_question</th>\n",
              "      <th>clean_answer</th>\n",
              "      <th>answer_len</th>\n",
              "      <th>answer_end</th>\n",
              "      <th>answer_span</th>\n",
              "      <th>answer_word_span</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Queen_Victoria</td>\n",
              "      <td>Internationally, Victoria took a keen interest...</td>\n",
              "      <td>How was the House of Orleans and the British R...</td>\n",
              "      <td>5722d1770dadf01500fa1f04</td>\n",
              "      <td>218</td>\n",
              "      <td>by marriage through the Coburgs</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>internationally victoria took keen interest im...</td>\n",
              "      <td>how was the house of orleans and the british r...</td>\n",
              "      <td>by marriage through the coburgs</td>\n",
              "      <td>31</td>\n",
              "      <td>249</td>\n",
              "      <td>(218, 249)</td>\n",
              "      <td>(-1, -1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Southampton</td>\n",
              "      <td>Southampton's largest retail centre, and 35th ...</td>\n",
              "      <td>What's the largest retail center in Southampton?</td>\n",
              "      <td>56f8afe39e9bad19000a031d</td>\n",
              "      <td>72</td>\n",
              "      <td>WestQuay Shopping Centre</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>southamptons largest retail centre 35th larges...</td>\n",
              "      <td>whats the largest retail center in southampton</td>\n",
              "      <td>westquay shopping centre</td>\n",
              "      <td>24</td>\n",
              "      <td>96</td>\n",
              "      <td>(72, 96)</td>\n",
              "      <td>(7, 9)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Immunology</td>\n",
              "      <td>Other immune system disorders include various ...</td>\n",
              "      <td>What characterizes a hypersensitivity?</td>\n",
              "      <td>5706aa0a75f01819005e7ce8</td>\n",
              "      <td>110</td>\n",
              "      <td>respond inappropriately to otherwise harmless ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>immune system disorders include various hypers...</td>\n",
              "      <td>what characterizes a hypersensitivity</td>\n",
              "      <td>respond inappropriately to otherwise harmless ...</td>\n",
              "      <td>56</td>\n",
              "      <td>166</td>\n",
              "      <td>(110, 166)</td>\n",
              "      <td>(8, 13)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Steven_Spielberg</td>\n",
              "      <td>Spielberg won the Academy Award for Best Direc...</td>\n",
              "      <td>When was Jaws released?</td>\n",
              "      <td>57318afba5e9cc1400cdc01f</td>\n",
              "      <td>143</td>\n",
              "      <td>1975</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>spielberg academy award best director schindle...</td>\n",
              "      <td>when was jaws released</td>\n",
              "      <td>1975</td>\n",
              "      <td>4</td>\n",
              "      <td>147</td>\n",
              "      <td>(143, 147)</td>\n",
              "      <td>(15, 15)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Electric_motor</td>\n",
              "      <td>Because the rotor is much lighter in weight (m...</td>\n",
              "      <td>What advantage doesn't a coreless rotor have o...</td>\n",
              "      <td>5ad168ad645df0001a2d1a12</td>\n",
              "      <td>-1</td>\n",
              "      <td></td>\n",
              "      <td>141.0</td>\n",
              "      <td>accelerate much more rapidly</td>\n",
              "      <td>True</td>\n",
              "      <td>rotor much lighter weight mass conventional ro...</td>\n",
              "      <td>what advantage doesnt a coreless rotor have ov...</td>\n",
              "      <td>IMPOSSIBLE</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>(-1, -1)</td>\n",
              "      <td>(-1, -1)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              title  ... answer_word_span\n",
              "0    Queen_Victoria  ...         (-1, -1)\n",
              "1       Southampton  ...           (7, 9)\n",
              "2        Immunology  ...          (8, 13)\n",
              "3  Steven_Spielberg  ...         (15, 15)\n",
              "4    Electric_motor  ...         (-1, -1)\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIYtmXgrQ9i_",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'>**DO NOT EXECUTE THIS**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57EsT35XyxGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = ['',\n",
        "          'WestQuay Shopping Centre',\n",
        "          'inappropriately to otherwise harmless compounds.',\n",
        "          '1975',\n",
        "          'accelerate much more rapidly']\n",
        "# preds = getpred(test,y_pred)\n",
        "# preds"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6ahjxlWq03p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "051eec57-0841-489f-e1d8-87d63f4e62a4"
      },
      "source": [
        "test[test['id']=='5a56d7bd6349e2001acdcf92']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer</th>\n",
              "      <th>plausible_answer_start</th>\n",
              "      <th>plausible_answer</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>clean_context</th>\n",
              "      <th>clean_question</th>\n",
              "      <th>clean_answer</th>\n",
              "      <th>answer_len</th>\n",
              "      <th>answer_end</th>\n",
              "      <th>answer_span</th>\n",
              "      <th>answer_word_span</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>611</th>\n",
              "      <td>Ministry_of_Defence_(United_Kingdom)</td>\n",
              "      <td>Henry VIII's wine cellar at the Palace of Whit...</td>\n",
              "      <td>What was the Palace of Whitehall built with in...</td>\n",
              "      <td>5a56d7bd6349e2001acdcf92</td>\n",
              "      <td>-1</td>\n",
              "      <td></td>\n",
              "      <td>225.0</td>\n",
              "      <td>steel and concrete</td>\n",
              "      <td>True</td>\n",
              "      <td>henry viiis wine cellar palace whitehall built...</td>\n",
              "      <td>what was the palace of whitehall built with in...</td>\n",
              "      <td>IMPOSSIBLE</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>(-1, -1)</td>\n",
              "      <td>(-1, -1)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    title  ... answer_word_span\n",
              "611  Ministry_of_Defence_(United_Kingdom)  ...         (-1, -1)\n",
              "\n",
              "[1 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHK2TaOho-9j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "16d1f6cc-b7cb-4861-d519-9152905168cf"
      },
      "source": [
        "# preds = getpred(test,y_pred)\n",
        "# qid_to_has_ans = make_qid_to_has_ans(test)  # maps qid to True/False\n",
        "# has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
        "# no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
        "exact_raw, f1_raw,exact_scores_pa,f1_scores_pa = get_raw_scores(test, y_pred)\n",
        "pprint.pprint(exact_raw)\n",
        "pprint.pprint(f1_raw)\n",
        "pprint.pprint(exact_scores_pa)\n",
        "pprint.pprint(f1_scores_pa)\n",
        "exact, f1,exact_pa, f1_pa, total = make_eval_dict(exact_raw,f1_raw,exact_scores_pa,f1_scores_pa)\n",
        "\n",
        "print(exact, f1,exact_pa, f1_pa, total)\n",
        "\n",
        "# print(test['id'].iloc[2] + ' = ', compute_f1(test['answer'].iloc[2],y_pred[2]))\n",
        "# print(test['id'].iloc[0] + ' = ', compute_f1(test['answer'].iloc[0],y_pred[2]))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'56f8afe39e9bad19000a031d': 1,\n",
            " '5706aa0a75f01819005e7ce8': 0,\n",
            " '5722d1770dadf01500fa1f04': 0,\n",
            " '57318afba5e9cc1400cdc01f': 1,\n",
            " '5ad168ad645df0001a2d1a12': 0}\n",
            "{'56f8afe39e9bad19000a031d': 1.0,\n",
            " '5706aa0a75f01819005e7ce8': 0.9090909090909091,\n",
            " '5722d1770dadf01500fa1f04': 0,\n",
            " '57318afba5e9cc1400cdc01f': 1.0,\n",
            " '5ad168ad645df0001a2d1a12': 0}\n",
            "{'56f8afe39e9bad19000a031d': 0,\n",
            " '5706aa0a75f01819005e7ce8': 0,\n",
            " '5722d1770dadf01500fa1f04': 0,\n",
            " '57318afba5e9cc1400cdc01f': 0,\n",
            " '5ad168ad645df0001a2d1a12': 1}\n",
            "{'56f8afe39e9bad19000a031d': 0,\n",
            " '5706aa0a75f01819005e7ce8': 0,\n",
            " '5722d1770dadf01500fa1f04': 0,\n",
            " '57318afba5e9cc1400cdc01f': 0,\n",
            " '5ad168ad645df0001a2d1a12': 1.0}\n",
            "40.0 58.18181818181819 20.0 20.0 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVPxnPRWWJ1V",
        "colab_type": "text"
      },
      "source": [
        "# Evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIVrjx6YNTsi",
        "colab_type": "text"
      },
      "source": [
        "## Import BERT pretrained from Hugging Face for QnA whole word masking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeJNCZS9NSif",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "13157739-f276-49af-c74a-549b107271f4"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/25/89050e69ed53c2a3b7f8c67844b3c8339c1192612ba89a172cf85b298948/transformers-3.0.1-py3-none-any.whl (757kB)\n",
            "\u001b[K     |████████████████████████████████| 757kB 7.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.0-rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 33.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=13eb86dc9e9a5df874ba309623f4fd65ba5e6122df47217b6f2b7b83a53da350\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.0rc4 transformers-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlmI9YDNNxrd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252,
          "referenced_widgets": [
            "7431f5f7993b4c2d999a88a4510014fe",
            "a1d7eaa1e1984c4dad7403d607ab582e",
            "aa4a1e22f1b2469586f072a64809233b",
            "8be286fb70864984a3ba53fe80ab00ec",
            "00f93bc37d374241a0cbcf69223ff85b",
            "52750d32123b4207b2ee6737d03e3e3c",
            "2803e6c469d64988b53e0a45d310e164",
            "3f500eccae8749d3820ea1d8788ff591",
            "55e1fc1c90da402881388308e9da6b1e",
            "6e0e4b56ff1d4c0fae9637c3d2688522",
            "ad903cfdb19a4937bc3f518fc372396b",
            "a8527a340b224c3dbb7865f49b3b0769",
            "f9c413b10db4416bb513c085254fd499",
            "ba8220e9b6944bf999a979c1171c29a4",
            "6776bb1447904f63a56b51e3ca892812",
            "6b2f14eb098f41c182e9b1d3e19a3deb",
            "079680a0b13a493fa4a9b265af61c83f",
            "cf0426839dce434697acc33bc5ab5d80",
            "8dbefbd57c7a450988d21d62f31d01b8",
            "f3b005f2be1342499b0f0b8332538b16",
            "9e7346295f024b6d869012da89b48c29",
            "e62800fdbe77433b96d87f1892c46b74",
            "5b05926ea12141029160eebe5061c392",
            "ea3ae93f048d41a8a1684e63b76fa606"
          ]
        },
        "outputId": "793c660c-4e1b-4312-bfec-88565318bd22"
      },
      "source": [
        "from transformers import BertTokenizer, TFBertForQuestionAnswering\n",
        "\n",
        "berthugtokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "berthugmodel = TFBertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7431f5f7993b4c2d999a88a4510014fe",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "55e1fc1c90da402881388308e9da6b1e",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=443.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "079680a0b13a493fa4a9b265af61c83f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1341090760.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing TFBertForQuestionAnswering.\n",
            "\n",
            "All the weights of TFBertForQuestionAnswering were initialized from the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad.\n",
            "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV04yElJPeze",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "cfc32338-7d92-4e89-f2dd-114a41ef2da4"
      },
      "source": [
        "berthugmodel.summary()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_question_answering\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "bert (TFBertMainLayer)       multiple                  335141888 \n",
            "_________________________________________________________________\n",
            "qa_outputs (Dense)           multiple                  2050      \n",
            "=================================================================\n",
            "Total params: 335,143,938\n",
            "Trainable params: 335,143,938\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI_FkpHkPKms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predBERTHuggingface(question, context):  \n",
        "  input_dict = berthugtokenizer(question, context, return_tensors='tf')\n",
        "  start_scores, end_scores = berthugmodel(input_dict)\n",
        "  all_tokens = berthugtokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n",
        "  # print(all_tokens)\n",
        "  answer = ' '.join(all_tokens[tf.math.argmax(start_scores, 1)[0] : tf.math.argmax(end_scores, 1)[0]+1])\n",
        "  # print(answer)\n",
        "  # Find the tokens with the highest `start` and `end` scores.\n",
        "  answer_start = np.argmax(start_scores)\n",
        "  answer_end = np.argmax(end_scores)\n",
        "  # Combine the tokens in the answer and print it out.\n",
        "  answer = ' '.join(all_tokens[answer_start:answer_end+1])\n",
        "  # Start with the first token.\n",
        "  answer = all_tokens[answer_start]\n",
        "\n",
        "  # Select the remaining answer tokens and join them with whitespace.\n",
        "  for i in range(answer_start + 1, answer_end + 1):      \n",
        "      # If it's a subword token, then recombine it with the previous token.\n",
        "      if all_tokens[i][0:2] == '##':\n",
        "          answer += all_tokens[i][2:]    \n",
        "      # Otherwise, add a space then the token.\n",
        "      else:\n",
        "          answer += ' ' + all_tokens[i]\n",
        "\n",
        "  return answer "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udQXeS9CPVa3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c832ff7-41d4-49c6-a9c0-063496a1fb79"
      },
      "source": [
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=80) \n",
        "# print(wrapper.fill(cnndf['context'].iloc[0]))\n",
        "q1, c1  = \"what was the death toll\", \"With a record 15968 coronavirus cases reported in the past 24 hours,the total count in India crossed the 4.5 lakh mark. The death toll has gone up to 14476\"\n",
        "q2, c2 = 'how many cases in mumbai', 'Delhi, which already is the second worst hit state in terms of coronavirus caseload and fatalities, overtook Mumbai after the number of cases soared by 3788 to touch 70390. Mumbai has so far recorded 69625 cases, according to official figures.'\n",
        "\n",
        "# input_dict = berthugtokenizer(text=[q1,q2],text_pair =[c1,c2],is_pretokenized=True, return_tensors='tf')\n",
        "# print(input_dict)\n",
        "\n",
        "# input_dict_noar = berthugtokenizer(q2,c2, return_tensors='tf')\n",
        "# print(input_dict_noar)\n",
        "\n",
        "\n",
        "# all_tokens = berthugtokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"][0])\n",
        "# print(all_tokens)\n",
        "\n",
        "# all_tokens = berthugtokenizer.convert_ids_to_tokens(input_dict_noar[\"input_ids\"][0])\n",
        "# print(all_tokens)\n",
        "\n",
        "print(predBERTHuggingface(q1,c1))\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3OYR2U5Tnhs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "84088ed0-d8e4-474d-92ba-0db1a0cd8251"
      },
      "source": [
        "# bert_pred=pd.DataFrame(columns=['c','q','true','pred'])\n",
        "test = load_test_data(name='test-withstopwordspunct.csv')\n",
        "bert_h_pred = test[['id','context','question','answer','plausible_answer']].copy()\n",
        "preds = []\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "# a better way is to batch its and tokenize \n",
        "# for a batch a record at a time and do inference \n",
        "# I was able to do for deeppavlov, for hugginface I felt lazy\n",
        "with strategy.scope():\n",
        "  for i in tqdm(range(test.shape[0])):\n",
        "    try:\n",
        "      p = ''\n",
        "      p = predBERTHuggingface(test['question'].iloc[i],test['context'].iloc[i])\n",
        "    except:\n",
        "      print('error in prediction')\n",
        "    preds = preds + [p]\n",
        "\n",
        "bert_h_pred['prediction'] = preds\n",
        "\n",
        "bert_h_pred.head()\n",
        "# remove na pls\n",
        "bert_h_pred.loc[bert_h_pred['prediction'].isna(), 'prediction'] = ''\n",
        "bert_h_pred.to_csv(model_path + 'bert/bert-huggingface-results.csv')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(26062, 16)\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 73/26062 [00:22<2:12:52,  3.26it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 119/26062 [00:36<2:15:50,  3.18it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|▎         | 723/26062 [03:44<2:14:36,  3.14it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (29 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  3%|▎         | 732/26062 [03:47<2:10:45,  3.23it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|▎         | 932/26062 [04:49<2:13:24,  3.14it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (8 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  4%|▍         | 1004/26062 [05:12<2:12:01,  3.16it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  6%|▌         | 1536/26062 [07:59<2:07:52,  3.20it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 3185/26062 [16:36<2:00:35,  3.16it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (17 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 21%|██        | 5469/26062 [28:36<1:49:41,  3.13it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (8 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 21%|██▏       | 5580/26062 [29:11<1:46:09,  3.22it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 5994/26062 [31:21<1:46:25,  3.14it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (16 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n",
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 7218/26062 [37:46<1:38:18,  3.19it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 30%|███       | 7881/26062 [41:15<1:37:11,  3.12it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 35%|███▍      | 9047/26062 [47:24<1:27:16,  3.25it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 35%|███▍      | 9098/26062 [47:40<1:29:16,  3.17it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 9254/26062 [48:28<1:28:01,  3.18it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 9278/26062 [48:36<1:27:24,  3.20it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 9914/26062 [51:56<1:24:25,  3.19it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 39%|███▉      | 10255/26062 [53:43<1:21:21,  3.24it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 43%|████▎     | 11095/26062 [58:08<1:19:14,  3.15it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (8 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 11562/26062 [1:00:36<1:15:46,  3.19it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 45%|████▍     | 11679/26062 [1:01:12<1:15:38,  3.17it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (13 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 12220/26062 [1:04:04<1:15:08,  3.07it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (17 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 12287/26062 [1:04:25<1:13:52,  3.11it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 12600/26062 [1:06:04<1:10:50,  3.17it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (15 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 49%|████▉     | 12831/26062 [1:07:17<1:10:13,  3.14it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 13671/26062 [1:11:45<1:05:41,  3.14it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (7 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 14771/26062 [1:17:33<1:00:43,  3.10it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (14 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 67%|██████▋   | 17403/26062 [1:31:31<45:49,  3.15it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (7 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 71%|███████   | 18395/26062 [1:36:46<40:15,  3.17it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (24 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 18670/26062 [1:38:13<39:11,  3.14it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (8 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 18715/26062 [1:38:27<38:29,  3.18it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 18950/26062 [1:39:42<38:23,  3.09it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 73%|███████▎  | 19134/26062 [1:40:40<36:43,  3.14it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (12 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 20266/26062 [1:46:38<30:31,  3.16it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (6 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 79%|███████▉  | 20624/26062 [1:48:31<28:44,  3.15it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 79%|███████▉  | 20714/26062 [1:49:00<28:14,  3.16it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85%|████████▍ | 22055/26062 [1:56:05<21:22,  3.13it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (11 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 22932/26062 [2:00:44<16:13,  3.22it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 94%|█████████▍| 24594/26062 [2:09:35<07:40,  3.19it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 95%|█████████▍| 24642/26062 [2:09:50<07:31,  3.14it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (6 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 25079/26062 [2:12:08<05:08,  3.19it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (10 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 97%|█████████▋| 25340/26062 [2:13:31<03:45,  3.20it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (8 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 99%|█████████▉| 25840/26062 [2:16:10<01:10,  3.13it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (9 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "error in prediction\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 26062/26062 [2:17:20<00:00,  3.16it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>plausible_answer</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5722d1770dadf01500fa1f04</td>\n",
              "      <td>Internationally, Victoria took a keen interest...</td>\n",
              "      <td>How was the House of Orleans and the British R...</td>\n",
              "      <td>by marriage through the Coburgs</td>\n",
              "      <td></td>\n",
              "      <td>by marriage through the coburgs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56f8afe39e9bad19000a031d</td>\n",
              "      <td>Southampton's largest retail centre, and 35th ...</td>\n",
              "      <td>What's the largest retail center in Southampton?</td>\n",
              "      <td>WestQuay Shopping Centre</td>\n",
              "      <td></td>\n",
              "      <td>westquay shopping centre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5706aa0a75f01819005e7ce8</td>\n",
              "      <td>Other immune system disorders include various ...</td>\n",
              "      <td>What characterizes a hypersensitivity?</td>\n",
              "      <td>respond inappropriately to otherwise harmless ...</td>\n",
              "      <td></td>\n",
              "      <td>respond inappropriately to otherwise harmless ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>57318afba5e9cc1400cdc01f</td>\n",
              "      <td>Spielberg won the Academy Award for Best Direc...</td>\n",
              "      <td>When was Jaws released?</td>\n",
              "      <td>1975</td>\n",
              "      <td></td>\n",
              "      <td>1975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5ad168ad645df0001a2d1a12</td>\n",
              "      <td>Because the rotor is much lighter in weight (m...</td>\n",
              "      <td>What advantage doesn't a coreless rotor have o...</td>\n",
              "      <td></td>\n",
              "      <td>accelerate much more rapidly</td>\n",
              "      <td>accelerate much more rapidly</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         id  ...                                         prediction\n",
              "0  5722d1770dadf01500fa1f04  ...                    by marriage through the coburgs\n",
              "1  56f8afe39e9bad19000a031d  ...                           westquay shopping centre\n",
              "2  5706aa0a75f01819005e7ce8  ...  respond inappropriately to otherwise harmless ...\n",
              "3  57318afba5e9cc1400cdc01f  ...                                               1975\n",
              "4  5ad168ad645df0001a2d1a12  ...                       accelerate much more rapidly\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP13NC14X47t",
        "colab_type": "text"
      },
      "source": [
        "## Eval on test data\n",
        "\n",
        "Metrics would be F1 score micro, EM, Accuracy Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAUtNL4tkGut",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "b3302580-c9d4-4ec9-b1f5-e05dd0e5aee1"
      },
      "source": [
        "# test = load_test_data(name='train-withstopwordspunct.csv')\n",
        "# params = loadparams(name='params_withoutstopwords.json')\n",
        "# tokenizer = load_tokenizer(name='use')\n",
        "# showparams(params)\n",
        "# print(test.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(26062, 16)\n",
            "Loading Universal Sentence Encoder\n",
            "Vocab Loaded -  82505\n",
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 300,\n",
            " 'prediction.accuracy.score': 0.3322461821809531,\n",
            " 'prediction.macrof1.score': 0.011907387665813255,\n",
            " 'prediction.microf1.score': 0.25672221926414995,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': [26062, 16],\n",
            " 'test_span_outofrange': 0,\n",
            " 'train_shape': [78183, 16],\n",
            " 'train_span_outofrange': 0,\n",
            " 'training.batch_size': 64,\n",
            " 'training.epochs': 25,\n",
            " 'training.train_length': 78183,\n",
            " 'training.train_steps': 1221,\n",
            " 'training.val_length': 26061,\n",
            " 'training.val_steps': 814,\n",
            " 'val_shape': [26061, 16],\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 82505}\n",
            "26062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4xL4rRSV5JW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strategy = tf.distribute.MirroredStrategy()\n",
        "model_results = pd.DataFrame(columns=['Model Name','F1(Ans)','EM(Ans)','F1(Plau Ans)','EM(Plau Ans)'])\n",
        "with strategy.scope():\n",
        "\n",
        "  # y_test = create_answer_sequence(test,params['context_max_length'])\n",
        "  # y_test was a list changing to numpy array\n",
        "  # y_test_fixed = np.array(y_test)\n",
        "  # compute for y_test though in this case it the max of 0 and 1 for \n",
        "  # the frist half od array size for start, and rest for end\n",
        "  # y_test_new,_,_ = combine_y(y_test_fixed,test.shape[0])\n",
        "\n",
        "  test_map = {\n",
        "      'With stopwords': load_test_data(name='test-withstopwordspunct.csv'),\n",
        "      'Without stopwords': load_test_data(name='test-withoutstopwords.csv'),\n",
        "  }\n",
        "\n",
        "  test_seq_map = {\n",
        "      'With stopwords': [],\n",
        "      'Without stopwords': []\n",
        "  }\n",
        "\n",
        "  for model in list_of_models:\n",
        "    print('Loading model ',model['name'],' from ', model['loc'])\n",
        "    if((model['type']=='BERT')):\n",
        "      # print('BERT eval will be handled separately. Skipping')\n",
        "      bert_results = pd.read_csv(model['loc'])\n",
        "      bert_results.loc[bert_results['answer'].isna(), 'answer'] = '' \n",
        "      bert_results.loc[bert_results['plausible_answer'].isna(), 'plausible_answer'] = ''\n",
        "      bert_results.loc[bert_results['prediction'].isna(), 'prediction'] = ''\n",
        "      exact_raw, f1_raw, exact_scores_pa,f1_scores_pa = get_raw_scores(bert_results, bert_results['prediction'].values.tolist())\n",
        "      exact, f1,exact_pa, f1_pa, total = make_eval_dict(exact_raw,f1_raw, exact_scores_pa,f1_scores_pa)\n",
        "      values = [model['name'],              \n",
        "                f1,\n",
        "                exact,\n",
        "                f1_pa,\n",
        "                exact_pa]\n",
        "      zipped = zip(model_results.columns, values)\n",
        "      a_dictionary = dict(zipped)\n",
        "      model_results = model_results.append(a_dictionary,ignore_index=True)   \n",
        "      continue\n",
        "    \n",
        "    if(model['type']=='With stopwords'):\n",
        "      test = test_map['With stopwords']\n",
        "      params = loadparams(name='params_withstopwords_withuse.json')\n",
        "      tokenizer = load_tokenizer(name='use')\n",
        "      if not test_seq_map['With stopwords']:\n",
        "        print('Generating question sequnce for with stopwords ...')\n",
        "        test_context_sequence, test_question_sequence = generate_question_context_sequence(context=test[\"clean_context\"].values,\n",
        "                                      question=test[\"clean_question\"].values,\n",
        "                                      question_max_length=params['question_max_length'],\n",
        "                                      padding=params['question_pad_seq'],\n",
        "                                      context_max_length=params['context_max_length']\n",
        "                                      )      \n",
        "        test_seq_map['With stopwords'] = [test_context_sequence,test_question_sequence]\n",
        "      else:\n",
        "        print('Using question sequnce for with stopwords ...')\n",
        "        test_context_sequence = test_seq_map['With stopwords'][0]\n",
        "        test_question_sequence = test_seq_map['With stopwords'][1]\n",
        "    elif(model['type']=='Without stopwords'):\n",
        "      test = test_map['Without stopwords']\n",
        "      params = loadparams(name='params_withoutstopwords.json')\n",
        "      tokenizer = load_tokenizer(name='glove')\n",
        "      if not test_seq_map['Without stopwords']:\n",
        "        print('Generating question sequnce for without stopwords ...')\n",
        "        test_context_sequence, test_question_sequence = generate_question_context_sequence(context=test[\"clean_context\"].values,\n",
        "                                      question=test[\"clean_question\"].values,\n",
        "                                      question_max_length=params['question_max_length'],\n",
        "                                      padding=params['question_pad_seq'],\n",
        "                                      context_max_length=params['context_max_length']\n",
        "                                      )      \n",
        "        test_seq_map['Without stopwords'] = [test_context_sequence,test_question_sequence]\n",
        "      else:\n",
        "        print('Using question sequnce for without stopwords ...')\n",
        "        test_context_sequence = test_seq_map['Without stopwords'][0]\n",
        "        test_question_sequence = test_seq_map['Without stopwords'][1]      \n",
        "\n",
        "    tfmodel = load_mrc_model(model['loc'])\n",
        "    print(model['name'],' model successfuly. Starting eval')\n",
        "    # tfmodel.summary()  \n",
        "    y_prediction = tfmodel.predict([test_question_sequence,test_context_sequence])\n",
        "    y_prediction_new,start_pred,end_pred = combine_y(y_prediction,test.shape[0])\n",
        "    # acc_score,macro_f1_score,micro_f1_score = accuracy_metrics(y_test_new,y_prediction_new)\n",
        "    y_pred_text = generate_y_preds_text(test,start_pred,end_pred)\n",
        "    exact_raw, f1_raw, exact_scores_pa,f1_scores_pa = get_raw_scores(test, y_pred_text)\n",
        "    exact, f1,exact_pa, f1_pa, total = make_eval_dict(exact_raw,f1_raw, exact_scores_pa,f1_scores_pa)\n",
        "    values = [model['name'],              \n",
        "              f1,\n",
        "              exact,\n",
        "              f1_pa,\n",
        "              exact_pa]\n",
        "    zipped = zip(model_results.columns, values)\n",
        "    a_dictionary = dict(zipped)\n",
        "    model_results = model_results.append(a_dictionary,ignore_index=True)   \n",
        "    del tokenizer\n",
        "    del params      \n",
        "    del y_prediction\n",
        "    del y_pred_text  \n",
        "    del start_pred\n",
        "    del end_pred\n",
        "    del tfmodel\n",
        "\n",
        "model_results.head(10)   \n",
        "model_results.to_csv(model_path + \"model_results_\" + datetime.now().strftime(\"%d-%m-%Y_%I-%M-%S_%p\") + \".csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHx9KbHZ8pNb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "c0efcb31-d47a-4adc-903d-9e211688b8de"
      },
      "source": [
        "model_results"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model Name</th>\n",
              "      <th>F1(Ans)</th>\n",
              "      <th>EM(Ans)</th>\n",
              "      <th>F1(Plau Ans)</th>\n",
              "      <th>EM(Plau Ans)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM Baseline</td>\n",
              "      <td>29.001121</td>\n",
              "      <td>28.225002</td>\n",
              "      <td>0.214623</td>\n",
              "      <td>0.053718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Deep LSTM + GloVe</td>\n",
              "      <td>26.666439</td>\n",
              "      <td>26.030236</td>\n",
              "      <td>0.276301</td>\n",
              "      <td>0.069066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bi-LSTM + GloVe</td>\n",
              "      <td>28.021961</td>\n",
              "      <td>27.526667</td>\n",
              "      <td>0.281106</td>\n",
              "      <td>0.126621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bi-LSTM + GloVe + Q2C Attention</td>\n",
              "      <td>33.095119</td>\n",
              "      <td>33.094160</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bi-LSTM + GloVe + Q2C-C2Q Attention</td>\n",
              "      <td>29.622157</td>\n",
              "      <td>29.130535</td>\n",
              "      <td>0.200039</td>\n",
              "      <td>0.069066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM Baseline + Universal Sentence Encode</td>\n",
              "      <td>24.545255</td>\n",
              "      <td>22.952958</td>\n",
              "      <td>0.618575</td>\n",
              "      <td>0.099762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Bi-LSTM + Universal Sentence Encode</td>\n",
              "      <td>32.025226</td>\n",
              "      <td>31.544010</td>\n",
              "      <td>0.068411</td>\n",
              "      <td>0.023022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Bi-LSTM + Q2C Attention + Universal Sentence E...</td>\n",
              "      <td>30.695975</td>\n",
              "      <td>30.120482</td>\n",
              "      <td>0.146946</td>\n",
              "      <td>0.042207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>BERT + Cased_L-12_H-768_A-12 + DeepPavlov</td>\n",
              "      <td>59.096928</td>\n",
              "      <td>51.362136</td>\n",
              "      <td>22.539289</td>\n",
              "      <td>18.179725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>BERT + Uncased_L-24_H-1024_A-24 + Huggingface</td>\n",
              "      <td>57.513153</td>\n",
              "      <td>49.769780</td>\n",
              "      <td>22.158168</td>\n",
              "      <td>17.753818</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Model Name  ...  EM(Plau Ans)\n",
              "0                                      LSTM Baseline  ...      0.053718\n",
              "1                                  Deep LSTM + GloVe  ...      0.069066\n",
              "2                                    Bi-LSTM + GloVe  ...      0.126621\n",
              "3                    Bi-LSTM + GloVe + Q2C Attention  ...      0.000000\n",
              "4                Bi-LSTM + GloVe + Q2C-C2Q Attention  ...      0.069066\n",
              "5          LSTM Baseline + Universal Sentence Encode  ...      0.099762\n",
              "6                Bi-LSTM + Universal Sentence Encode  ...      0.023022\n",
              "7  Bi-LSTM + Q2C Attention + Universal Sentence E...  ...      0.042207\n",
              "8          BERT + Cased_L-12_H-768_A-12 + DeepPavlov  ...     18.179725\n",
              "9      BERT + Uncased_L-24_H-1024_A-24 + Huggingface  ...     17.753818\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCkMUNLEFYwG",
        "colab_type": "text"
      },
      "source": [
        "## Out of domain handing \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bppIF0J_Fews",
        "colab_type": "text"
      },
      "source": [
        "### Different language \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmEXz0ORFhIw",
        "colab_type": "text"
      },
      "source": [
        "### Totally irrelevant question given a known context from train domain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OOCtgDAFj-X",
        "colab_type": "text"
      },
      "source": [
        "### Totally different context say from Medicine and ask answer from it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwvNDiQRFZql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WulPny3Z00mm",
        "colab_type": "text"
      },
      "source": [
        "## Eval on manual input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v33J0DrkgSh6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "19740284-575a-4bac-a955-7b1750783af5"
      },
      "source": [
        "all_models = load_all_models()\n",
        "all_models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model  LSTM Baseline  from  /content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-0/full_context_withoutstopwords_model_epoch_lstmbaseline0_nomask_gpu.h5\n",
            "Loading model  Deep LSTM + GloVe  from  /content/drive/My Drive/AIML-MRC-Capstone/models/deeplstm/full_context_withoutstopwords_model_epoch_25_deeplstm_glove_nomask_gpu.h5\n",
            "Loading model  Bi-LSTM + GloVe  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm/full_context_withoutstopwords_model_epoch_25_bilstm_glove_nomask_gpu.h5\n",
            "Loading model  Bi-LSTM + GloVe + Q2C Attention  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-glove/full_context_withoutstopwords_model_epoch_25_bilstm_q2c-attention_glove.h5\n",
            "Loading model  Bi-LSTM + GloVe + Q2C-C2Q Attention  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-bidaf-glove/full_context_withoutstopwords_model_epoch_25_bilstm_bidaf_glove_nomask_gpu_after_fix.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Bi-LSTM + GloVe': <tensorflow.python.keras.engine.training.Model at 0x7fcbe0ff4da0>,\n",
              " 'Bi-LSTM + GloVe + Q2C Attention': <tensorflow.python.keras.engine.training.Model at 0x7fcbe0c5c550>,\n",
              " 'Bi-LSTM + GloVe + Q2C-C2Q Attention': <tensorflow.python.keras.engine.training.Model at 0x7fcbe0812ac8>,\n",
              " 'Deep LSTM + GloVe': <tensorflow.python.keras.engine.training.Model at 0x7fcc8058fcf8>,\n",
              " 'LSTM Baseline': <tensorflow.python.keras.engine.training.Model at 0x7fcc8070bb70>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcuhAMQNrgi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del all_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R03OP9TxDc6P",
        "colab_type": "text"
      },
      "source": [
        "### TEST1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPHnb86cJdZL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "84dc5759-b688-48dd-94d7-8a547c00e68e"
      },
      "source": [
        "all_params = {\n",
        "    \"params_withoutstopwords\": loadparams(name='params_withoutstopwords.json'),\n",
        "    \"params_withstopwords_withuse\": loadparams(name='params_withstopwords_withuse.json')\n",
        "}\n",
        "all_tokenizers = {\n",
        "    \"glove\": load_tokenizer(all_params['params_withoutstopwords']),\n",
        "    \"use\": load_tokenizer(all_params['params_withstopwords_withuse'],'use')\n",
        "}\n",
        "all_params\n",
        "all_models = load_all_models()\n",
        "params = all_params['params_withoutstopwords']\n",
        "tokenizer = all_tokenizers['glove']"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading GloVe 300D\n",
            "Vocab Loaded -  100850\n",
            "Loading Universal Sentence Encoder\n",
            "Vocab Loaded -  82505\n",
            "Loading model  LSTM Baseline  from  /content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-0/full_context_withoutstopwords_model_epoch_lstmbaseline0_nomask_gpu.h5\n",
            "Loading model  Deep LSTM + GloVe  from  /content/drive/My Drive/AIML-MRC-Capstone/models/deeplstm/full_context_withoutstopwords_model_epoch_25_deeplstm_glove_nomask_gpu.h5\n",
            "Loading model  Bi-LSTM + GloVe  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm/full_context_withoutstopwords_model_epoch_25_bilstm_glove_nomask_gpu.h5\n",
            "Loading model  Bi-LSTM + GloVe + Q2C Attention  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-glove/full_context_withoutstopwords_model_epoch_25_bilstm_q2c-attention_glove.h5\n",
            "Loading model  Bi-LSTM + GloVe + Q2C-C2Q Attention  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-bidaf-glove/full_context_withoutstopwords_model_epoch_25_bilstm_bidaf_glove_nomask_gpu_after_fix.h5\n",
            "Loading model  LSTM Baseline + Universal Sentence Encode  from  /content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-use-withstop/full_context_withstopwords_model_epoch_25_lstmbaseline0_nomask_gpu.h5\n",
            "Loading model  Bi-LSTM + Universal Sentence Encode  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-use-withstop/full_context_withstopwords_model_epoch_25_bilstm_use_nomask_gpu.h5\n",
            "Loading model  Bi-LSTM + Q2C Attention + Universal Sentence Encode  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-use-withstop/full_context_withstopwords_model_epoch_25_bilstm_q2c-attention_use.h5\n",
            "Loading model  BERT + Cased_L-12_H-768_A-12 + DeepPavlov  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bert/bert-results.csv\n",
            "BERT is WIP !!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSw9oBmEc_BR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9817088c-1636-4a61-8ca0-43e371321aeb"
      },
      "source": [
        "test[test['id']=='57318afba5e9cc1400cdc01f']['question'].iloc[0]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'When was Jaws released?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTNhlpswRbMW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "outputId": "59061f79-2aed-4538-9841-895f1ccc7e51"
      },
      "source": [
        "c1='Spielberg born on 1993. Three of Spielbergs films Jaws released on 1975, E.T. the Extra-Terrestrial (1982), and Jurassic Park (1993)—achieved box office records, originated and came to epitomize the blockbuster film. The unadjusted gross of all Spielberg-directed films exceeds $9 billion worldwide, making him the highest-grossing director in history. His personal net worth is estimated to be more than $3 billion. He has been associated with composer John Williams since 1974, who composed music for all save five of Spielbergs feature films.'\n",
        "q1='when was jaws released'\n",
        "\n",
        "cs = [c1]\n",
        "qs = [q1]\n",
        "# c_,q_,span,y_,answer = predit_test(test['context'].iloc[39],test['question'].iloc[39])\n",
        "df = pd.DataFrame(columns=[\"Model\",\"Context\", \"Question\",\"Predicted Ans\"])\n",
        "# for m in all_models:\n",
        "m = 'Bi-LSTM + GloVe + Q2C Attention'\n",
        "c_,q_,span,y_,answer = predit_test(cs,qs, all_models[m], params=params)\n",
        "print('Prediction for model - ', m)\n",
        "for i in range(len(cs)):    \n",
        "  values = [m,cs[i],qs[i],answer[i]]\n",
        "  zipped = zip(df.columns, values)\n",
        "  a_dictionary = dict(zipped)\n",
        "  df = df.append(a_dictionary,ignore_index=True)     \n",
        "\n",
        "df"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0  1605   842   138  2155    51     3\n",
            "   4922   775  7170   238   138  2861   191 20474     1 11603  2687    66\n",
            "   4304   397  2155  1457  2499   384   418  1900    66   268    10 40435\n",
            "      1 12291   190     1 38358  4246     3  2119 37194   775  7027   403\n",
            "    406   834   410  8667     1 15716  1360     5   149  1100   737  3021\n",
            "   2715    14   646    10   300  1579  1705   111   406  2892   501  1262\n",
            "    427   172  4025   133  3627    42  2450    37  1230    57    59  2119\n",
            "   3097   242     3  4922   927   775]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-024aff593e61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# for m in all_models:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Bi-LSTM + GloVe + Q2C Attention'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mc_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredit_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Prediction for model - '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-1fb77f20c1c1>\u001b[0m in \u001b[0;36mpredit_test\u001b[0;34m(context, question, modeltoUse, params)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mci\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;31m# tokens[tokenizer.word_index[i]] = i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# context_token.append(ci)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1605"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkyTSbDp1Jfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkeEqGBKFbAo",
        "colab_type": "text"
      },
      "source": [
        "# Eval on News "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQJzz9F3FfW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import textwrap\n",
        "\n",
        "#Let's download the data from inshorts website.\n",
        "# Strictly inshort URL\n",
        "def load_news_from_url(url='https://inshorts.com/en/read/technology'):\n",
        "  # url = 'https://inshorts.com/en/read/technology'\n",
        "  news_category = url.split('/')[-1]\n",
        "  news_category\n",
        "  data = requests.get(url)\n",
        "  data.content\n",
        "  soup = BeautifulSoup(data.content, 'html.parser')\n",
        "  news_data = []\n",
        "  news_articles = [{'news_headline': headline.find('span', attrs={'itemprop': 'headline'}).string,\n",
        "                    'news_article': article.find('div', attrs={'itemprop': 'articleBody'}).string,\n",
        "                    'news_category': news_category} \n",
        "                  for headline, article in zip(soup.find_all('div', \n",
        "                                                              class_ = ['news-card-title news-right-box']), \n",
        "                                                soup.find_all('div', class_=['news-card-content news-right-box']))]\n",
        "  #Check news data\n",
        "  news_data.extend(news_articles)\n",
        "  news_data                    \n",
        "  #Building dataframe\n",
        "  df = pd.DataFrame(news_data, columns=['news_headline', 'news_article', 'news_category'])\n",
        "  df.info()  \n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2wTYX9wGpkN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "115ab5ad-7a05-408a-9f3f-f51c602994d8"
      },
      "source": [
        "df = load_news_from_url('https://inshorts.com/en/news/gileads-covid19-drug-remdesivir-to-cost-$2340-for-5day-treatment-1593438701842')\n",
        "wrapper = textwrap.TextWrapper(width=80) \n",
        "print(wrapper.fill(df['news_article'].iloc[0]))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1 entries, 0 to 0\n",
            "Data columns (total 3 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   news_headline  1 non-null      object\n",
            " 1   news_article   1 non-null      object\n",
            " 2   news_category  1 non-null      object\n",
            "dtypes: object(3)\n",
            "memory usage: 152.0+ bytes\n",
            "American biopharmaceutical company Gilead has said that it will charge $390\n",
            "(₹29,450) per vial or about $2,340 (over ₹1.76 lakh) for a typical five-day\n",
            "course of treatment of its coronavirus-fighting drug remdesivir. The company on\n",
            "Monday said it would offer this price to the US government and other developed\n",
            "countries. The $390 per vial price is for government entities.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0-v0tJkL86C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80edb6eb-6848-42a9-c4ac-13ed5584ce6e"
      },
      "source": [
        "tfm = all_models['Bi-LSTM + GloVe + Q2C Attention']\n",
        "params = all_params['params_withoutstopwords']\n",
        "tokenizer = all_tokenizers['glove']\n",
        "c_,q_,span,y_,answer = predit_test([df['news_article'].iloc[0]],['what is per vial price'],tfm, params=params)                \n",
        "print(answer)         "
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncB__cjC6T88",
        "colab_type": "text"
      },
      "source": [
        "# Serving API "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li214BwR8PaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "44e0fb6f-4ed4-4799-e0aa-930ea9cbf94e"
      },
      "source": [
        "# A nice and nifty way done by this fellow - https://medium.com/@kshitijvijay271199/flask-on-google-colab-f6525986797b\n",
        "# and of course ngrok saves the day !!\n",
        "!pip install flask-ngrok"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMeXDG-y6WAC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "0a46a458-4202-4daf-9efa-2e6c42b72289"
      },
      "source": [
        "all_params = {\n",
        "    \"params_withoutstopwords\": loadparams(name='params_withoutstopwords.json'),\n",
        "    \"params_withstopwords_withuse\": loadparams(name='params_withstopwords_withuse.json')\n",
        "}\n",
        "all_tokenizers = {\n",
        "    \"glove\": load_tokenizer(all_params['params_withoutstopwords']),\n",
        "    \"use\": load_tokenizer(all_params['params_withstopwords_withuse'],'use')\n",
        "}\n",
        "all_params\n",
        "all_models = load_all_models()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading GloVe 300D\n",
            "Vocab Loaded -  100850\n",
            "Loading Universal Sentence Encoder\n",
            "Vocab Loaded -  82505\n",
            "Loading model  LSTM Baseline  from  /content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-0/full_context_withoutstopwords_model_epoch_lstmbaseline0_nomask_gpu.h5\n",
            "Loading model  Deep LSTM + GloVe  from  /content/drive/My Drive/AIML-MRC-Capstone/models/deeplstm/full_context_withoutstopwords_model_epoch_25_deeplstm_glove_nomask_gpu.h5\n",
            "Loading model  Bi-LSTM + GloVe  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm/full_context_withoutstopwords_model_epoch_25_bilstm_glove_nomask_gpu.h5\n",
            "Loading model  Bi-LSTM + GloVe + Q2C Attention  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-glove/full_context_withoutstopwords_model_epoch_25_bilstm_q2c-attention_glove.h5\n",
            "Loading model  Bi-LSTM + GloVe + Q2C-C2Q Attention  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-bidaf-glove/full_context_withoutstopwords_model_epoch_25_bilstm_bidaf_glove_nomask_gpu_after_fix.h5\n",
            "Loading model  LSTM Baseline + Universal Sentence Encode  from  /content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-use-withstop/full_context_withstopwords_model_epoch_25_lstmbaseline0_nomask_gpu.h5\n",
            "Loading model  Bi-LSTM + Universal Sentence Encode  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-use-withstop/full_context_withstopwords_model_epoch_25_bilstm_use_nomask_gpu.h5\n",
            "Loading model  Bi-LSTM + Q2C Attention + Universal Sentence Encode  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-use-withstop/full_context_withstopwords_model_epoch_25_bilstm_q2c-attention_use.h5\n",
            "Loading model  BERT + Cased_L-12_H-768_A-12 + DeepPavlov  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bert/bert-results.csv\n",
            "BERT is WIP !!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cepdwTqD8Syx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flask import Flask\n",
        "from flask import request\n",
        "from flask import jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)  # Start ngrok when app is run\n",
        "\n",
        "# for / root, return Hello Word\n",
        "@app.route(\"/\")\n",
        "def root():\n",
        "    url = request.method\n",
        "    return f\"Welcome to the AIML Batch 4 - MRC Capstone\"\n",
        "\n",
        "@app.route(\"/models\")\n",
        "def listmodels():\n",
        "    url = request.method\n",
        "    return {'models':list_of_models}\n",
        "\n",
        "# for / root, return Hello Word\n",
        "@app.route(\"/predict\", methods=['POST'])\n",
        "def predict():\n",
        "    if request.method == 'POST':\n",
        "        try:\n",
        "            data = request.get_json()\n",
        "            modelid = data[\"model\"] \n",
        "            context = data[\"c\"]\n",
        "            question = data[\"q\"]\n",
        "\n",
        "            if(modelid == 'BERT'):\n",
        "              return jsonify(\"BERT is not yet supported\")            \n",
        "\n",
        "            thatmodel = [m for m in list_of_models if m['id']==modelid][0]\n",
        "            print('Running with model ',thatmodel['name'])\n",
        "            if(thatmodel['type']=='With stopwords'):\n",
        "              params = all_params['params_withstopwords_withuse']\n",
        "              tokenizer = all_tokenizers['use']\n",
        "            elif(thatmodel['type']=='Without stopwords'):\n",
        "              params = all_params['params_withoutstopwords']\n",
        "              tokenizer = all_tokenizers['glove']\n",
        "            \n",
        "            tfmodel = all_models[thatmodel['name']]            \n",
        "            c_,q_,span,y_,answer = predit_test([context],[question],tfmodel, params=params)                \n",
        "            print(context, question, answer)         \n",
        "            # lin_reg = joblib.load(\"./linear_regression_model.pkl\")\n",
        "        except err:\n",
        "            print(err)\n",
        "            return jsonify(\"Some thing is not right !!\")\n",
        "        finally:\n",
        "          del tfmodel\n",
        "\n",
        "        return jsonify({'context': context, 'question':question, 'answer': answer})\n",
        "\n",
        "# for / root, return Hello Word\n",
        "@app.route(\"/predict_from_url\", methods=['POST'])\n",
        "def predictfromnewurl():\n",
        "    if request.method == 'POST':\n",
        "        try:\n",
        "            data = request.get_json()\n",
        "            modelid = data[\"model\"] \n",
        "            url = data[\"url\"]\n",
        "            df = load_news_from_url(url)\n",
        "            context = df['news_article'].iloc[0]\n",
        "            question = data[\"q\"]\n",
        "\n",
        "            if(modelid == 'BERT'):\n",
        "              return jsonify(\"BERT is not yet supported\")            \n",
        "\n",
        "            thatmodel = [m for m in list_of_models if m['id']==modelid][0]\n",
        "            print('Running with model ',thatmodel['name'])\n",
        "            if(thatmodel['type']=='With stopwords'):\n",
        "              params = all_params['params_withstopwords_withuse']\n",
        "              tokenizer = all_tokenizers['use']\n",
        "            elif(thatmodel['type']=='Without stopwords'):\n",
        "              params = all_params['params_withoutstopwords']\n",
        "              tokenizer = all_tokenizers['glove']\n",
        "            \n",
        "            tfmodel = all_models[thatmodel['name']]            \n",
        "            c_,q_,span,y_,answer = predit_test([context],[question],tfmodel, params=params)                \n",
        "            print(context, question, answer)         \n",
        "            # lin_reg = joblib.load(\"./linear_regression_model.pkl\")\n",
        "        except err:\n",
        "            print(err)\n",
        "            return jsonify(\"Some thing is not right !!\")\n",
        "        finally:\n",
        "          del tfmodel\n",
        "\n",
        "        return jsonify({'context': context, 'question':question, 'answer': answer})\n",
        "\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqy24lzf_1yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}