{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mrc_Evaluations.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN2U3317dmv8kvPBpvL3OGF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanbasu/aiml-capstone/blob/master/mrc_Evaluations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DRLVpTJznGZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "2343e78a-877b-4c0b-c805-40044889774b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOGCXAgWaIsd",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaepomVyaJfm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "de8852bf-ec1c-4a5e-c85a-3f774e741665"
      },
      "source": [
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import tensorflow as tf\n",
        "tf.debugging.set_log_device_placement(False)\n",
        "import pickle\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pprint\n",
        "from tensorflow.keras.layers import Bidirectional,LSTM,Dense,Dropout,BatchNormalization,Flatten,Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from numpy import array\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import f1_score,accuracy_score,precision_score\n",
        "from prettytable import PrettyTable\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "model_path = \"/content/drive/My Drive/AIML-MRC-Capstone/models/\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuIsS7WFh76y",
        "colab_type": "text"
      },
      "source": [
        "# Load Google Bucket as drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCaxxOEXh8GK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4c809c87-cb50-4b65-c07b-693b1b99b005"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = 'ai-ml-capstone'\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil ls\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "gs://aiml-capstone/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3NaG4vBiPvw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "a314c2a2-e466-4a0c-9a6b-fd563909c278"
      },
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   653  100   653    0     0  18138      0 --:--:-- --:--:-- --:--:-- 18138\n",
            "OK\n",
            "50 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 4,274 kB of archives.\n",
            "After this operation, 12.8 MB of additional disk space will be used.\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 144328 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_0.28.1_amd64.deb ...\n",
            "Unpacking gcsfuse (0.28.1) ...\n",
            "Setting up gcsfuse (0.28.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdNVzqJ_jfs2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "58e6b169-3d42-4c2c-ea5e-3d8eb320285d"
      },
      "source": [
        "!mkdir gbucket\n",
        "!gcsfuse --implicit-dirs aiml-capstone gbucket \n",
        "# !umount gbucket"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using mount point: /content/gbucket\n",
            "Opening GCS connection...\n",
            "Opening bucket...\n",
            "Mounting file system...\n",
            "File system has been successfully mounted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in-L5Mi9kEir",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58f97f72-6ef3-42a4-bafe-2104ab57d132"
      },
      "source": [
        "!ls gbucket/lstmbaseline-0/tf-serve/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saved_model.pb\tvariables\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUPKTSR-GWt5",
        "colab_type": "text"
      },
      "source": [
        "# List of Models\n",
        "\n",
        "As part of our capstone, we are in process of evaluating the following models\n",
        "\n",
        "Data | Model | On GPU | Masking | Padding | Epoch | Location | \n",
        "--- | --- | --- | --- | --- | --- | --- | \n",
        "Without stopwords | SVM | No | - | - | - | [here](https://storage.cloud.google.com/aiml-capstone/svm/)\n",
        "Without stopwords | LSTM Baseline | No | No | Pre | 25 | [here](https://storage.cloud.google.com/aiml-capstone/lstmbaseline-0/full_context_withoutstopwords_model_epoch_lstmbaseline0_nomask_gpu.h5)\n",
        "Without stopwords | Deep LSTM + GloVe | Yes | No | Pre | 25 | [here](https://storage.cloud.google.com/aiml-capstone/deeplstm/full_context_withoutstopwords_model_epoch_deeplstm_gpu.h5)\n",
        "Without stopwords | Bi-LSTM + GloVe | No | No | Pre | 10 | [here](https://storage.cloud.google.com/aiml-capstone/bilstm/full_context_withoutstopwords_model_epoch_10_bilstm_cpu.h5)\n",
        "Without stopwords | Bi-LSTM + GloVe + Q2C Attention | Yes | No | Pre | 25 | [here](https://storage.cloud.google.com/aiml-capstone/bilstm-q2c-attention-glove/full_context_withoutstopwords_nomask_epoch_25_bilstm_q2c-attention_glove_gpu.h5)\n",
        "Without stopwords | Bi-LSTM + GloVe + Q2C-C2Q Attention | Yes | No | Pre | 25 | [here](https://storage.cloud.google.com/aiml-capstone/bilstm-bidaf-glove/full_context_withoutstopwords_model_epoch_25_bilstm_bidaf_glove_nomask_gpu.h5)\n",
        "--- | --- | --- | --- | --- | --- | --- | \n",
        "With Stopwords | Bi-LSTM + Universal Sentence Encoder + Q2C Attention\n",
        "With Stopwords | Bi-LSTM + Universal Sentence Encoder + Q2C-C2Q Attention\n",
        "With Stopwords | BERT + Universal Sentence Encoder \n",
        "With Stopwords | GPT2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_GOOYs6cXfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "outputId": "5192edf9-c194-4768-9a1a-a46ceac5868e"
      },
      "source": [
        "# List of all models and its meta info\n",
        "list_of_models = [\n",
        "                  {\n",
        "                    \"id\":\"lstm-baseline\",\n",
        "                    \"name\":\"LSTM Baseline\",\n",
        "                    \"type\":\"Without stopwords\",\n",
        "                    \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-0/full_context_withoutstopwords_model_epoch_lstmbaseline0_nomask_gpu.h5\"\n",
        "                  },\n",
        "                  {\n",
        "                    \"id\":\"deeplstm-glove\",\n",
        "                    \"name\":\"Deep LSTM + GloVe\",\n",
        "                    \"type\":\"Without stopwords\",\n",
        "                    \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/deeplstm/full_context_withoutstopwords_model_epoch_25_deeplstm_glove_nomask_gpu.h5\"\n",
        "                  },  \n",
        "                  {\n",
        "                    \"id\":\"bilstm-glove\",\n",
        "                    \"name\":\"Bi-LSTM + GloVe\",\n",
        "                    \"type\":\"Without stopwords\",\n",
        "                    \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm/full_context_withoutstopwords_model_epoch_25_bilstm_glove_nomask_gpu.h5\"\n",
        "                  },\n",
        "                  {\n",
        "                    \"id\":\"bilstm-glove-q2c-attention\",\n",
        "                    \"name\":\"Bi-LSTM + GloVe + Q2C Attention\",\n",
        "                    \"type\":\"Without stopwords\",\n",
        "                    \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-glove/full_context_withoutstopwords_model_epoch_25_bilstm_q2c-attention_glove.h5\"\n",
        "                  },  \n",
        "                  {\n",
        "                    \"id\":\"bilstm-bidaf-glove\",\n",
        "                    \"name\":\"Bi-LSTM + GloVe + Q2C-C2Q Attention\",\n",
        "                    \"type\":\"Without stopwords\",\n",
        "                    \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-bidaf-glove/full_context_withoutstopwords_model_epoch_25_bilstm_bidaf_glove_nomask_gpu_after_fix.h5\"\n",
        "                  },\n",
        "                  {\n",
        "                    \"id\":\"lstmbaseline-use-withstop\",\n",
        "                    \"name\":\"LSTM Baseline + Universal Sentence Encode\",\n",
        "                    \"type\":\"With stopwords\",\n",
        "                    \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-use-withstop/full_context_withstopwords_model_epoch_25_lstmbaseline0_nomask_gpu.h5\"\n",
        "                  }, \n",
        "                  {\n",
        "                    \"id\":\"bilstm-use-withstop\",\n",
        "                    \"name\":\"Bi-LSTM + Universal Sentence Encode\",\n",
        "                    \"type\":\"With stopwords\",\n",
        "                    \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-use-withstop/full_context_withstopwords_model_epoch_25_bilstm_use_nomask_gpu.h5\"\n",
        "                  },  \n",
        "                  {\n",
        "                    \"id\":\"bilstm-q2c-attention-use-withstop\",\n",
        "                    \"name\":\"Bi-LSTM + Q2C Attention + Universal Sentence Encode\",\n",
        "                    \"type\":\"With stopwords\",\n",
        "                    \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-use-withstop/full_context_withstopwords_model_epoch_25_bilstm_q2c-attention_use.h5\"\n",
        "                  },                                                       \n",
        "                  {\n",
        "                    \"id\":\"bert-deeppavlov\",\n",
        "                    \"name\":\"BERT + Cased_L-12_H-768_A-12 + DeepPavlov\",\n",
        "                    \"type\":\"BERT\",\n",
        "                    \"loc\":\"/content/drive/My Drive/AIML-MRC-Capstone/models/bert/bert-results.csv\"\n",
        "                  }                                                                        \n",
        "                  ]\n",
        "\n",
        "list_of_models\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 'lstm-baseline',\n",
              "  'loc': '/content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-0/full_context_withoutstopwords_model_epoch_lstmbaseline0_nomask_gpu.h5',\n",
              "  'name': 'LSTM Baseline',\n",
              "  'type': 'Without stopwords'},\n",
              " {'id': 'deeplstm-glove',\n",
              "  'loc': '/content/drive/My Drive/AIML-MRC-Capstone/models/deeplstm/full_context_withoutstopwords_model_epoch_25_deeplstm_glove_nomask_gpu.h5',\n",
              "  'name': 'Deep LSTM + GloVe',\n",
              "  'type': 'Without stopwords'},\n",
              " {'id': 'bilstm-glove',\n",
              "  'loc': '/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm/full_context_withoutstopwords_model_epoch_25_bilstm_glove_nomask_gpu.h5',\n",
              "  'name': 'Bi-LSTM + GloVe',\n",
              "  'type': 'Without stopwords'},\n",
              " {'id': 'bilstm-glove-q2c-attention',\n",
              "  'loc': '/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-glove/full_context_withoutstopwords_model_epoch_25_bilstm_q2c-attention_glove.h5',\n",
              "  'name': 'Bi-LSTM + GloVe + Q2C Attention',\n",
              "  'type': 'Without stopwords'},\n",
              " {'id': 'bilstm-bidaf-glove',\n",
              "  'loc': '/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-bidaf-glove/full_context_withoutstopwords_model_epoch_25_bilstm_bidaf_glove_nomask_gpu_after_fix.h5',\n",
              "  'name': 'Bi-LSTM + GloVe + Q2C-C2Q Attention',\n",
              "  'type': 'Without stopwords'},\n",
              " {'id': 'lstmbaseline-use-withstop',\n",
              "  'loc': '/content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-use-withstop/full_context_withstopwords_model_epoch_25_lstmbaseline0_nomask_gpu.h5',\n",
              "  'name': 'LSTM Baseline + Universal Sentence Encode',\n",
              "  'type': 'With stopwords'},\n",
              " {'id': 'bilstm-use-withstop',\n",
              "  'loc': '/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-use-withstop/full_context_withstopwords_model_epoch_25_bilstm_use_nomask_gpu.h5',\n",
              "  'name': 'Bi-LSTM + Universal Sentence Encode',\n",
              "  'type': 'With stopwords'},\n",
              " {'id': 'bilstm-q2c-attention-use-withstop',\n",
              "  'loc': '/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-use-withstop/full_context_withstopwords_model_epoch_25_bilstm_q2c-attention_use.h5',\n",
              "  'name': 'Bi-LSTM + Q2C Attention + Universal Sentence Encode',\n",
              "  'type': 'With stopwords'},\n",
              " {'id': 'bert-deeppavlov',\n",
              "  'loc': '/content/drive/My Drive/AIML-MRC-Capstone/models/bert/bert-results.csv',\n",
              "  'name': 'BERT + Cased_L-12_H-768_A-12 + DeepPavlov',\n",
              "  'type': 'BERT'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xfm8h4IBSGbh",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEtw1J-aPecV",
        "colab_type": "text"
      },
      "source": [
        "## bAbi \n",
        "\n",
        "Reference Paper = https://arxiv.org/pdf/1502.05698.pdf \n",
        "\n",
        "GitHub - https://github.com/facebookarchive/bAbI-tasks\n",
        "\n",
        "![alt text](https://storage.cloud.google.com/aiml-capstone/Screenshot%202020-06-20%20at%2011.11.50%20AM.png)\n",
        "![alt text](https://storage.cloud.google.com/aiml-capstone/Screenshot%202020-06-20%20at%2011.11.03%20AM.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ5nidQ8SVog",
        "colab_type": "text"
      },
      "source": [
        "## SQuAD test dataset and published dev set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACmlhKB1S1ZF",
        "colab_type": "text"
      },
      "source": [
        "Refer to this eval metrics - https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
        "\n",
        "Eval dataset from SQuAD - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
        "\n",
        "Test Dataset from training = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGFyqInm7ZD0",
        "colab_type": "text"
      },
      "source": [
        "## News Domain Specific Evaluations\n",
        "\n",
        "Test on Sample News Context, Question and Answer pairs ??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Yw8C9nY7uXT",
        "colab_type": "text"
      },
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDyvisumUImJ",
        "colab_type": "text"
      },
      "source": [
        "# Common Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XtBcy2Qwm2-c"
      },
      "source": [
        "## Custom function for preprocessing of context and question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LFr3-S_Gm9FX",
        "colab": {}
      },
      "source": [
        "# remove unwanted chars\n",
        "# convert to lowercase\n",
        "# remove unwanted spaces\n",
        "# remove stop words\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "## reference \n",
        "def decontracted(phrase):\n",
        "    \"\"\"\n",
        "    This function remooves punctuation from given sentence.\n",
        "    \"\"\"\n",
        "\n",
        "    if(phrase is np.nan):\n",
        "      return 'impossible'      \n",
        "\n",
        "    try:      \n",
        "      # specific\n",
        "      phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "      phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "      # general\n",
        "      phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "      phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "      phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "      phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "      phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "      phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "      phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "      phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "      \n",
        "      # string operation\n",
        "      phrase = phrase.replace('\\\\r', ' ')\n",
        "      phrase = phrase.replace('\\\\\"', ' ')\n",
        "      phrase = phrase.replace('\\\\n', ' ')\n",
        "\n",
        "      phrase = re.sub('[^A-Za-z0-9]+', ' ', phrase.lower())\n",
        "    except:\n",
        "      print(phrase)  \n",
        "    \n",
        "    return phrase\n",
        "\n",
        "def preprocess_text(corpus, text_lower_case=True, \n",
        "                      special_char_removal=True, stopword_removal=True, remove_digits=False):    \n",
        "    normalized_text = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # doc = decontracted(doc)\n",
        "        # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        # remove special characters and\\or digits    \n",
        "        if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = remove_special_characters(doc, remove_digits=remove_digits) \n",
        "\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc)\n",
        "\n",
        "        normalized_text.append(doc)\n",
        "        \n",
        "    return normalized_text\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    #Using regex\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text):  \n",
        "    word_tokens = word_tokenize(text) \n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]   \n",
        "    filtered_sentence = [] \n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(w)                 \n",
        "    return ' '.join(filtered_sentence)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "20-D8HBeOf_p"
      },
      "source": [
        "## Answer Span from Context and Answer, and reverse for predicted spans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vbM8z2AEjxKK",
        "colab": {}
      },
      "source": [
        "def tokenize(sentence):\n",
        "    \"\"\"\n",
        "    Returns tokenised words.\n",
        "    \"\"\"\n",
        "    return nltk.word_tokenize(sentence)\n",
        "\n",
        "def answer_span(context,ans):\n",
        "    \"\"\"\n",
        "    This funtion returns anwer span start index and end index.\n",
        "    \"\"\"\n",
        "    ans_token = tokenize(ans)\n",
        "    con_token = tokenize(context)\n",
        "    ans_len = len(ans_token)\n",
        "    \n",
        "    if ans_len!=0 and ans_token[0] in con_token:\n",
        "    \n",
        "        indices = [i for i, x in enumerate(con_token) if x == ans_token[0]]        \n",
        "        try:\n",
        "\n",
        "            if(len(indices)>1):\n",
        "                start = [i for i in indices if (con_token[i:i+ans_len] == ans_token) ]\n",
        "                end = start[0] + ans_len - 1\n",
        "                return start[0],end\n",
        "\n",
        "            else:\n",
        "                start = con_token.index(ans_token[0])\n",
        "                end = start + ans_len - 1\n",
        "                return start,end\n",
        "        except:\n",
        "            return -1,-1\n",
        "    else:\n",
        "        return -1,-1\n",
        "\n",
        "def span_to_answer(span, context):\n",
        "  con_token = tokenize(context)  \n",
        "  return ' '.join(con_token[span[0]:span[1]+1])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_sveyxKK5j8q"
      },
      "source": [
        "## Update and persist params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K170rfN15qGP",
        "colab": {}
      },
      "source": [
        "### SAVE PARAMS\n",
        "# Writing to sample.json \n",
        "\n",
        "def updateparams():\n",
        "  with open(model_path + \"params.json\", \"w\") as p: \n",
        "    p.write(json.dumps(params))\n",
        "  print(\"params.jsop updated and can be found in \", model_path + \"params.json\")  \n",
        "\n",
        "# updateparams()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yYMDmMJNU9mw",
        "colab": {}
      },
      "source": [
        "def showparams(params):\n",
        "  pprint.pprint(params)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SklWt_NbVzOU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "2c14d4cf-1d6a-4ecd-eb23-747218fe4dff"
      },
      "source": [
        "def loadparams(name='params_withoutstopwords.json'):\n",
        "  with open(model_path + name) as f:\n",
        "    params = json.load(f)\n",
        "  return params  \n",
        "\n",
        "params = loadparams(name='params_withoutstopwords.json')\n",
        "showparams(params)\n",
        "showparams(loadparams())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 300,\n",
            " 'prediction.accuracy.score': 0.3322461821809531,\n",
            " 'prediction.macrof1.score': 0.011907387665813255,\n",
            " 'prediction.microf1.score': 0.25672221926414995,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': [26062, 16],\n",
            " 'test_span_outofrange': 0,\n",
            " 'train_shape': [78183, 16],\n",
            " 'train_span_outofrange': 0,\n",
            " 'training.batch_size': 64,\n",
            " 'training.epochs': 25,\n",
            " 'training.train_length': 78183,\n",
            " 'training.train_steps': 1221,\n",
            " 'training.val_length': 26061,\n",
            " 'training.val_steps': 814,\n",
            " 'val_shape': [26061, 16],\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 100850}\n",
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 300,\n",
            " 'prediction.accuracy.score': 0.3322461821809531,\n",
            " 'prediction.macrof1.score': 0.011907387665813255,\n",
            " 'prediction.microf1.score': 0.25672221926414995,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': [26062, 16],\n",
            " 'test_span_outofrange': 0,\n",
            " 'train_shape': [78183, 16],\n",
            " 'train_span_outofrange': 0,\n",
            " 'training.batch_size': 64,\n",
            " 'training.epochs': 25,\n",
            " 'training.train_length': 78183,\n",
            " 'training.train_steps': 1221,\n",
            " 'training.val_length': 26061,\n",
            " 'training.val_steps': 814,\n",
            " 'val_shape': [26061, 16],\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSJXR52VTWsf",
        "colab_type": "text"
      },
      "source": [
        "## Load Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5FJHi14rHNTX",
        "colab": {}
      },
      "source": [
        "def load_tokenizer(params=params, name=\"glove\"):\n",
        "  if(name=='glove'):\n",
        "    print('Loading GloVe 300D')\n",
        "    with open(model_path + \"tokenizer.pkl\",\"rb\") as infile:\n",
        "        tokenizer = pickle.load(infile)\n",
        "    print('Vocab Loaded - ',len(tokenizer.word_index))\n",
        "    if(params):\n",
        "      params['vocab_size'] = len(tokenizer.word_index)\n",
        "    return tokenizer\n",
        "  elif (name=='use'):\n",
        "    print('Loading Universal Sentence Encoder')\n",
        "    with open(model_path + \"tokenizerwithstopwordspunct.pkl\",\"rb\") as infile:\n",
        "        tokenizer = pickle.load(infile)\n",
        "    print('Vocab Loaded - ',len(tokenizer.word_index))\n",
        "    if(params):\n",
        "      params['vocab_size'] = len(tokenizer.word_index)    \n",
        "    return tokenizer       "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0TSjNkdXzuR"
      },
      "source": [
        "## Create a common function to generate sequences (useful in prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sq97Vq4kXTST",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0d8278e0-6d1c-4cf1-c04d-731b6275f562"
      },
      "source": [
        "# function to generate sequences withg appropiate padding\n",
        "tokenizer = load_tokenizer()\n",
        "def generate_question_context_sequence(context,question,question_max_length,padding,context_max_length):\n",
        "  question_seq = tokenizer.texts_to_sequences(question)\n",
        "  context_seq = tokenizer.texts_to_sequences(context)\n",
        "  question_seq = preprocessing.sequence.pad_sequences(question_seq,\n",
        "                                                      maxlen=question_max_length,\n",
        "                                                      padding=padding)\n",
        "  context_seq = preprocessing.sequence.pad_sequences(context_seq,\n",
        "                                                     maxlen=context_max_length,\n",
        "                                                     padding=padding)\n",
        "  return context_seq, question_seq"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading GloVe 300D\n",
            "Vocab Loaded -  100850\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v78fhG3Tdhx5"
      },
      "source": [
        "## Create a common function to predict and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v6KDfM25dhZr",
        "colab": {}
      },
      "source": [
        "def predit_test(context, question, modeltoUse, params=params):\n",
        "  # get sequence for context and question\n",
        "  c_ = preprocess_text(context,stopword_removal=False)\n",
        "  q_ = preprocess_text(question,stopword_removal=False)\n",
        "  answer=[]\n",
        "  spans=[]\n",
        "\n",
        "\n",
        "  c,q = generate_question_context_sequence(context=c_,\n",
        "                                           question=q_,\n",
        "                                           question_max_length=params['question_max_length'],\n",
        "                                           padding=params['question_pad_seq'],\n",
        "                                           context_max_length=params['context_max_length'])\n",
        "\n",
        "\n",
        "\n",
        "  y_ = modeltoUse.predict([q,c])    \n",
        "  for i in range(len(context)):\n",
        "    s = np.argmax(y_[i,:params['context_max_length']])\n",
        "    e = np.argmax(y_[i,params['context_max_length']:])\n",
        "    answer.append(span_to_answer((s,e),c_[i]))\n",
        "    spans.append([s,e])\n",
        "  \n",
        "  # print(c.shape,q.shape,y_.shape,s,e,answer)  \n",
        "  # print(s, e)\n",
        "  return c_,q_,spans,y_,answer"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIjvBaQ4WFZQ",
        "colab_type": "text"
      },
      "source": [
        "## Load Test Data with Ground Truth Known"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYkWpLxxWFii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import resample,shuffle\n",
        "\n",
        "def load_data_withoutstopwords():\n",
        "  #### NOTE THE 2 data frames's\n",
        "  df_nostopwords = 'test_squad_data_final_context_withoutstopwords.csv'\n",
        "  # df_withstopwords = 'squad_data_final_withstopword_withpunctuation.csv'\n",
        "  test_squad_df = pd.read_csv(project_path+df_nostopwords)\n",
        "  test_squad_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  test_squad_df[\"answer_word_span\"] = test_squad_df[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "  print(test_squad_df.info())\n",
        "  return test_squad_df"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH025QzmyMAJ",
        "colab_type": "text"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDX397bcyMZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logits_loss(y_true,logits):\n",
        "    \"\"\"\n",
        "    Custom loss function which minimises log_loss.\n",
        "    Referance https://stackoverflow.com/questions/50063613/add-loss-function-in-keras\n",
        "    \"\"\"\n",
        "    \n",
        "    #y_true = tf.cast(y_true,dtype=tf.int32)\n",
        "    #logits = tf.cast(logits,dtype=tf.float32)\n",
        "    \n",
        "    # breaking the tensor into two half's to get start and end label.\n",
        "    start_label = y_true[:,:params['context_max_length']]\n",
        "    end_label = y_true[:,params['context_max_length']:]\n",
        "    \n",
        "    # braking the logits tensor into start and end part for loss calcultion.\n",
        "    start_logit = logits[:,:params['context_max_length']]\n",
        "    end_logit = logits[:,params['context_max_length']:]\n",
        "    \n",
        "    start_loss = tf.keras.backend.categorical_crossentropy(start_label,start_logit)\n",
        "    end_loss = tf.keras.backend.categorical_crossentropy(end_label,end_logit)\n",
        "    \n",
        "#     start_loss = tf.losses.sparse_softmax_cross_entropy(labels=start_label, logits=start_logit)\n",
        "#     end_loss = tf.losses.sparse_softmax_cross_entropy(labels=end_label, logits=end_logit)\n",
        "    \n",
        "    # as per paer\n",
        "    \n",
        "    loss = start_loss + end_loss\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP43qWYkewja",
        "colab_type": "text"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pSzIAlTe07P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "def load_mrc_model(model_path):\n",
        "  custom_objects = {\"logits_loss\": logits_loss}\n",
        "  new_model = keras.models.load_model(model_path,custom_objects=custom_objects)\n",
        "  ### Check its architecture\n",
        "  # new_model.summary()  \n",
        "  return new_model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2fUNm7uPWSm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# strategy = tf.distribute.MirroredStrategy()\n",
        "# with strategy.scope():\n",
        "#   model = load_mrc_model('/content/drive/My Drive/AIML-MRC-Capstone/models/bilstm/full_context_withoutstopwords_model_epoch_25_bilstm_glove_nomask_gpu.h5')\n",
        "  \n",
        "# model.summary()\n",
        "# # list_of_models['bilstm-glove-q2c-c2q-attention']['loc'].replace('https://storage.cloud.google.com/aiml-capstone/','gbucket/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DURR8zGyUpy6",
        "colab_type": "text"
      },
      "source": [
        "## Load Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWgeXhRLUrj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_test_data(name='test-withoutstopwords.csv'):\n",
        "  test = pd.read_csv(model_path +name)\n",
        "  test.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "  test[\"answer_word_span\"] = test[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "  test.loc[test['answer'].isna(), 'answer'] = ''\n",
        "  test.loc[test['plausible_answer'].isna(), 'plausible_answer'] = ''\n",
        "  print(test.shape)\n",
        "  return test"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoXsJRF_Vah4",
        "colab_type": "text"
      },
      "source": [
        "## Create Answer Sequence for test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rregOZyRS_Da",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_answer_sequence(test, context_length):\n",
        "  # for test data\n",
        "  y_test = []\n",
        "  for i in range(len(test)):    \n",
        "      s = np.zeros(context_length,dtype = \"int\")\n",
        "      e = np.zeros(context_length,dtype = \"int\")        \n",
        "      start,end = test[\"answer_word_span\"].iloc[i]    \n",
        "      s[start] = 1\n",
        "      e[end] = 1\n",
        "      y_test.append(np.concatenate((s,e)))\n",
        "  return y_test\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDeMi7IWeb6_",
        "colab_type": "text"
      },
      "source": [
        "## Create a combined y_test array having both start and end vectors in OHE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3S3SB2cfFR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def combine_y(y, test_sample_size):\n",
        "  # argmax is used to get the index where the max value in a list appears, and hence \n",
        "  # for every index i, we can get the place of start and end token of the max probab\n",
        "  start = []\n",
        "  end = []\n",
        "  for i in range(test_sample_size):\n",
        "      start.append(np.argmax(y[i,:params['context_max_length']]))\n",
        "      end.append(np.argmax(y[i,params['context_max_length']:]))\n",
        "      \n",
        "  y_new = np.zeros((test_sample_size,params['context_max_length']))\n",
        "  for i in range(test_sample_size):\n",
        "      y_new[i,start[i]:end[i]+1] = 1\n",
        "  return y_new,start,end"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUd9-eqtioBM",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0e-TuininW8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy_metrics(y_true,y_pred):\n",
        "  acc_score = accuracy_score(y_true,y_pred)\n",
        "  macro_f1_score = f1_score(y_true,y_pred,average=\"macro\")\n",
        "  micro_f1_score = f1_score(y_true,y_pred,average=\"micro\")\n",
        "  return acc_score,macro_f1_score,micro_f1_score\n",
        "  "
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wqs4OxyjszkY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ff5feaeb-acbe-4658-ae91-abc7650233f3"
      },
      "source": [
        "accuracy_metrics([1,1,1],[0,1,1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6666666666666666, 0.4, 0.6666666666666666)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnoYgva4_iEI",
        "colab_type": "text"
      },
      "source": [
        "## Exact Match Count per Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jf3h0HWD_iyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def checkEMMatch(test, start, end):\n",
        "  field_names = [\"True Answer\",\n",
        "                \"True AS and AE\",\n",
        "                \"Predict Answer\",\n",
        "                \"Predict AS and AE\"]\n",
        "  result_df = pd.DataFrame(columns=field_names)\n",
        "  print('Checking for equality match percentage')\n",
        "  for i in tqdm(range(test.shape[0])):  \n",
        "    values = [test['clean_answer'].iloc[i], \n",
        "              test['answer_word_span'].iloc[i],\n",
        "              span_to_answer([start_pred[i],end_pred[i]],test['clean_context'].iloc[i]),\n",
        "              (start_pred[i],end_pred[i])]\n",
        "    zipped = zip(field_names, values)\n",
        "    a_dictionary = dict(zipped)\n",
        "    result_df = result_df.append(a_dictionary,ignore_index=True)\n",
        "\n",
        "  ematch = result_df[result_df['Predict Answer'] == result_df['True Answer']].shape[0]  / test.shape[0]\n",
        "  del result_df\n",
        "\n",
        "  return ematch\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvVeDCNqX5pr",
        "colab_type": "text"
      },
      "source": [
        "## Generate y_preds in text from predicted start and end span"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eDXknqOX5Cq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_y_preds_text(test, start, end):\n",
        "  y_preds = []\n",
        "  for i in tqdm(range(test.shape[0])):\n",
        "    y_preds.append(span_to_answer([start_pred[i],end_pred[i]],test['context'].iloc[i]))\n",
        "  return y_preds"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvA32P7IjPSZ",
        "colab_type": "text"
      },
      "source": [
        "## Load all model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xDin7-igafk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_all_models():\n",
        "  loaded_models = {}\n",
        "  for model in list_of_models:\n",
        "    print('Loading model ',model['name'],' from ', model['loc'])  \n",
        "    if(model['type'] == 'BERT'):\n",
        "      print('BERT is WIP !!')\n",
        "      continue    \n",
        "    tfmodel = load_mrc_model(model['loc'])\n",
        "    loaded_models[model['name']] = tfmodel\n",
        "  return loaded_models"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p87BPm-enj0t",
        "colab_type": "text"
      },
      "source": [
        "## SQuAD Official Evaluation Script\n",
        "\n",
        "Refer to https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsd_6KDXtTOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import collections\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import sys"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AJyEUWDzIrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getpred(test, y_predict):\n",
        "  pred = {}\n",
        "  for i in range(test.shape[0]):\n",
        "    pred[test['id'].iloc[i]] = y_predict[i]\n",
        "\n",
        "  return pred  "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGn2mA7atJEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_answer(s):\n",
        "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "    return re.sub(regex, ' ', text)\n",
        "  def white_space_fix(text):\n",
        "    if(text != text):\n",
        "      return ''\n",
        "    return ' '.join(text.split())\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return ''.join(ch for ch in text if ch not in exclude)\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKsIlANKnvuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_qid_to_has_ans(test):\n",
        "  qid_to_has_ans = {}\n",
        "  for i in range(test.shape[0]):\n",
        "    qid_to_has_ans[test['id'].iloc[i]] = bool(test['answer'].iloc[i])\n",
        "  return qid_to_has_ans"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHL-JtnCx70o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_tokens(s):\n",
        "  if not s: return []\n",
        "  return normalize_answer(s).split()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWjFQTjLx51X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6e0c0388-ec32-4d7b-d652-d3630c85b0c3"
      },
      "source": [
        "def compute_exact(a_gold, a_pred, type='a'):\n",
        "  if(type=='a'):\n",
        "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "  else:\n",
        "    return int((normalize_answer(a_gold) != '') and (normalize_answer(a_gold) == normalize_answer(a_pred)))\n",
        "\n",
        "def compute_f1(a_gold, a_pred,  type='a'):\n",
        "  gold_toks = get_tokens(a_gold)\n",
        "  pred_toks = get_tokens(a_pred)\n",
        "  # find number of common tokens\n",
        "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "  # count the common tokens\n",
        "  num_same = sum(common.values())\n",
        "  if (len(gold_toks) == 0 or len(pred_toks) == 0) and type == 'a':\n",
        "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "    return int(gold_toks == pred_toks)\n",
        "  if num_same == 0:\n",
        "    return 0\n",
        "  # precision is always to y_pred - 1 * (ratio of common token / total tokens in y_pred)\n",
        "  precision = 1.0 * num_same / len(pred_toks)\n",
        "  # recal is always to y_true - 1 * (ratio of common token / total tokens in y_true)\n",
        "  recall = 1.0 * num_same / len(gold_toks)\n",
        "  # F1 score formulais as below - 2*((precision*recall)/(precision+recall))\n",
        "  f1 = (2 * precision * recall) / (precision + recall)\n",
        "  return f1\n",
        "\n",
        "# Test the sklearn f1 versus SQuAD F1 metrics func\n",
        "print('scklearn F1 =',f1_score(['by marriage through coburgs'],['by marriage through the Coburgs'],average=\"macro\"))\n",
        "print('custom F1 =',compute_f1('by marriage through coburgs','by marriage through the Coburgs'))  \n",
        "\n",
        "# equals\n",
        "print('custom exact =',compute_exact('by marriage through coburgs','by marriage through the Coburgs'))  \n",
        "print('exact exact =',int('by marriage through coburgs' == 'by marriage through the Coburgs'))  \n",
        "\n",
        "## Clearly the custom f1 and exact makes so much sense "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "scklearn F1 = 0.0\n",
            "custom F1 = 1.0\n",
            "custom exact = 1\n",
            "exact exact = 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHykDpVjUQz1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_eval_dict(exact_scores, f1_scores,exact_scores_pa,f1_scores_pa, qid_list=None):\n",
        "  total = len(exact_scores)\n",
        "  # return collections.OrderedDict([\n",
        "  #     ('exact', 100.0 * sum(exact_scores.values()) / total),\n",
        "  #     ('f1', 100.0 * sum(f1_scores.values()) / total),\n",
        "  #     ('total', total),\n",
        "  # ])\n",
        "  exact = 100.0 * sum(exact_scores.values()) / total\n",
        "  f1 = 100.0 * sum(f1_scores.values()) / total\n",
        "  exact_pa = 100.0 * sum(exact_scores_pa.values()) / total\n",
        "  f1_pa = 100.0 * sum(f1_scores_pa.values()) / total\n",
        "  return exact, f1, exact_pa, f1_pa, total"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpGEMJJasDap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  preds = {\n",
        "#     \"id\" : 'predicted answer'    \n",
        "#      }\n",
        "def get_raw_scores(test, preds):\n",
        "    exact_scores = {}\n",
        "    f1_scores = {}    \n",
        "    exact_scores_pa = {}\n",
        "    f1_scores_pa = {}        \n",
        "    for i in range(test.shape[0]):\n",
        "      if 'id' in test.columns:\n",
        "        qid = test['id'].iloc[i]\n",
        "      else:\n",
        "        qid = i\n",
        "      gold_a = normalize_answer(test['answer'].iloc[i])\n",
        "      gold_pa = normalize_answer(test['plausible_answer'].iloc[i])\n",
        "      if not gold_a:\n",
        "        # For unanswerable questions, only correct answer is empty string\n",
        "        gold_a = ''\n",
        "\n",
        "      exact_scores[qid] = compute_exact(gold_a, preds[i])\n",
        "      f1_scores[qid] = compute_f1(gold_a, preds[i])\n",
        "      exact_scores_pa[qid] = compute_exact(gold_pa, preds[i], type='pa')\n",
        "      f1_scores_pa[qid] = compute_f1(gold_pa, preds[i], type='pa')      \n",
        "\n",
        "    return exact_scores, f1_scores,exact_scores_pa,f1_scores_pa"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C42zkDJ-o4JF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ded849cd-a0da-40de-acb4-85f1980b5c72"
      },
      "source": [
        "test = load_test_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(26062, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_XZUPY400oC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "71fb82ab-345e-41b0-d735-895b65875c58"
      },
      "source": [
        "test = test.head(5)\n",
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer</th>\n",
              "      <th>plausible_answer_start</th>\n",
              "      <th>plausible_answer</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>clean_context</th>\n",
              "      <th>clean_question</th>\n",
              "      <th>clean_answer</th>\n",
              "      <th>answer_len</th>\n",
              "      <th>answer_end</th>\n",
              "      <th>answer_span</th>\n",
              "      <th>answer_word_span</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Queen_Victoria</td>\n",
              "      <td>Internationally, Victoria took a keen interest...</td>\n",
              "      <td>How was the House of Orleans and the British R...</td>\n",
              "      <td>5722d1770dadf01500fa1f04</td>\n",
              "      <td>218</td>\n",
              "      <td>by marriage through the Coburgs</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>internationally victoria took a keen interest ...</td>\n",
              "      <td>how was the house of orleans and the british r...</td>\n",
              "      <td>by marriage through the coburgs</td>\n",
              "      <td>31</td>\n",
              "      <td>249</td>\n",
              "      <td>(218, 249)</td>\n",
              "      <td>(34, 38)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Southampton</td>\n",
              "      <td>Southampton's largest retail centre, and 35th ...</td>\n",
              "      <td>What's the largest retail center in Southampton?</td>\n",
              "      <td>56f8afe39e9bad19000a031d</td>\n",
              "      <td>72</td>\n",
              "      <td>WestQuay Shopping Centre</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>southampton is largest retail centre and 35th ...</td>\n",
              "      <td>what is the largest retail center in southampton</td>\n",
              "      <td>westquay shopping centre</td>\n",
              "      <td>24</td>\n",
              "      <td>96</td>\n",
              "      <td>(72, 96)</td>\n",
              "      <td>(13, 15)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Immunology</td>\n",
              "      <td>Other immune system disorders include various ...</td>\n",
              "      <td>What characterizes a hypersensitivity?</td>\n",
              "      <td>5706aa0a75f01819005e7ce8</td>\n",
              "      <td>110</td>\n",
              "      <td>respond inappropriately to otherwise harmless ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>other immune system disorders include various ...</td>\n",
              "      <td>what characterizes a hypersensitivity</td>\n",
              "      <td>respond inappropriately to otherwise harmless ...</td>\n",
              "      <td>56</td>\n",
              "      <td>166</td>\n",
              "      <td>(110, 166)</td>\n",
              "      <td>(15, 20)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Steven_Spielberg</td>\n",
              "      <td>Spielberg won the Academy Award for Best Direc...</td>\n",
              "      <td>When was Jaws released?</td>\n",
              "      <td>57318afba5e9cc1400cdc01f</td>\n",
              "      <td>143</td>\n",
              "      <td>1975</td>\n",
              "      <td>NaN</td>\n",
              "      <td></td>\n",
              "      <td>False</td>\n",
              "      <td>spielberg won the academy award for best direc...</td>\n",
              "      <td>when was jaws released</td>\n",
              "      <td>1975</td>\n",
              "      <td>4</td>\n",
              "      <td>147</td>\n",
              "      <td>(143, 147)</td>\n",
              "      <td>(24, 24)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Electric_motor</td>\n",
              "      <td>Because the rotor is much lighter in weight (m...</td>\n",
              "      <td>What advantage doesn't a coreless rotor have o...</td>\n",
              "      <td>5ad168ad645df0001a2d1a12</td>\n",
              "      <td>-1</td>\n",
              "      <td></td>\n",
              "      <td>141.0</td>\n",
              "      <td>accelerate much more rapidly</td>\n",
              "      <td>True</td>\n",
              "      <td>because the rotor is much lighter in weight ma...</td>\n",
              "      <td>what advantage does not a coreless rotor have ...</td>\n",
              "      <td>impossible</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>(-1, -1)</td>\n",
              "      <td>(-1, -1)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              title  ... answer_word_span\n",
              "0    Queen_Victoria  ...         (34, 38)\n",
              "1       Southampton  ...         (13, 15)\n",
              "2        Immunology  ...         (15, 20)\n",
              "3  Steven_Spielberg  ...         (24, 24)\n",
              "4    Electric_motor  ...         (-1, -1)\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIYtmXgrQ9i_",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'>**DO NOT EXECUTE THIS**</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57EsT35XyxGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = ['',\n",
        "          'WestQuay Shopping Centre',\n",
        "          'inappropriately to otherwise harmless compounds.',\n",
        "          '1975',\n",
        "          'accelerate much more rapidly\t']\n",
        "# preds = getpred(test,y_pred)\n",
        "# preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6ahjxlWq03p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "051eec57-0841-489f-e1d8-87d63f4e62a4"
      },
      "source": [
        "test[test['id']=='5a56d7bd6349e2001acdcf92']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer</th>\n",
              "      <th>plausible_answer_start</th>\n",
              "      <th>plausible_answer</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>clean_context</th>\n",
              "      <th>clean_question</th>\n",
              "      <th>clean_answer</th>\n",
              "      <th>answer_len</th>\n",
              "      <th>answer_end</th>\n",
              "      <th>answer_span</th>\n",
              "      <th>answer_word_span</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>611</th>\n",
              "      <td>Ministry_of_Defence_(United_Kingdom)</td>\n",
              "      <td>Henry VIII's wine cellar at the Palace of Whit...</td>\n",
              "      <td>What was the Palace of Whitehall built with in...</td>\n",
              "      <td>5a56d7bd6349e2001acdcf92</td>\n",
              "      <td>-1</td>\n",
              "      <td></td>\n",
              "      <td>225.0</td>\n",
              "      <td>steel and concrete</td>\n",
              "      <td>True</td>\n",
              "      <td>henry viiis wine cellar palace whitehall built...</td>\n",
              "      <td>what was the palace of whitehall built with in...</td>\n",
              "      <td>IMPOSSIBLE</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>(-1, -1)</td>\n",
              "      <td>(-1, -1)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    title  ... answer_word_span\n",
              "611  Ministry_of_Defence_(United_Kingdom)  ...         (-1, -1)\n",
              "\n",
              "[1 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHK2TaOho-9j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "9ed3e8f5-8dcb-4d62-de42-53a58473fa3d"
      },
      "source": [
        "# preds = getpred(test,y_pred)\n",
        "# qid_to_has_ans = make_qid_to_has_ans(test)  # maps qid to True/False\n",
        "# has_ans_qids = [k for k, v in qid_to_has_ans.items() if v]\n",
        "# no_ans_qids = [k for k, v in qid_to_has_ans.items() if not v]\n",
        "exact_raw, f1_raw,exact_scores_pa,f1_scores_pa = get_raw_scores(test, y_pred)\n",
        "pprint.pprint(exact_raw)\n",
        "pprint.pprint(f1_raw)\n",
        "pprint.pprint(exact_scores_pa)\n",
        "pprint.pprint(f1_scores_pa)\n",
        "exact, f1,exact_pa, f1_pa, total = make_eval_dict(exact_raw,f1_raw,exact_scores_pa,f1_scores_pa)\n",
        "\n",
        "print(exact, f1,exact_pa, f1_pa, total)\n",
        "\n",
        "# print(test['id'].iloc[2] + ' = ', compute_f1(test['answer'].iloc[2],y_pred[2]))\n",
        "# print(test['id'].iloc[0] + ' = ', compute_f1(test['answer'].iloc[0],y_pred[2]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'56f8afe39e9bad19000a031d': 1,\n",
            " '5706aa0a75f01819005e7ce8': 0,\n",
            " '5722d1770dadf01500fa1f04': 0,\n",
            " '57318afba5e9cc1400cdc01f': 1,\n",
            " '5ad168ad645df0001a2d1a12': 0}\n",
            "{'56f8afe39e9bad19000a031d': 1.0,\n",
            " '5706aa0a75f01819005e7ce8': 0.9090909090909091,\n",
            " '5722d1770dadf01500fa1f04': 0,\n",
            " '57318afba5e9cc1400cdc01f': 1.0,\n",
            " '5ad168ad645df0001a2d1a12': 0}\n",
            "{'56f8afe39e9bad19000a031d': 0,\n",
            " '5706aa0a75f01819005e7ce8': 0,\n",
            " '5722d1770dadf01500fa1f04': 0,\n",
            " '57318afba5e9cc1400cdc01f': 0,\n",
            " '5ad168ad645df0001a2d1a12': 1}\n",
            "{'56f8afe39e9bad19000a031d': 0,\n",
            " '5706aa0a75f01819005e7ce8': 0,\n",
            " '5722d1770dadf01500fa1f04': 0,\n",
            " '57318afba5e9cc1400cdc01f': 0,\n",
            " '5ad168ad645df0001a2d1a12': 1.0}\n",
            "40.0 58.18181818181819 20.0 20.0 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVPxnPRWWJ1V",
        "colab_type": "text"
      },
      "source": [
        "# Evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP13NC14X47t",
        "colab_type": "text"
      },
      "source": [
        "## Eval on test data\n",
        "\n",
        "Metrics would be F1 score micro, EM, Accuracy Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAUtNL4tkGut",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "b3302580-c9d4-4ec9-b1f5-e05dd0e5aee1"
      },
      "source": [
        "# test = load_test_data(name='train-withstopwordspunct.csv')\n",
        "# params = loadparams(name='params_withoutstopwords.json')\n",
        "# tokenizer = load_tokenizer(name='use')\n",
        "# showparams(params)\n",
        "# print(test.shape[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(26062, 16)\n",
            "Loading Universal Sentence Encoder\n",
            "Vocab Loaded -  82505\n",
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 300,\n",
            " 'prediction.accuracy.score': 0.3322461821809531,\n",
            " 'prediction.macrof1.score': 0.011907387665813255,\n",
            " 'prediction.microf1.score': 0.25672221926414995,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': [26062, 16],\n",
            " 'test_span_outofrange': 0,\n",
            " 'train_shape': [78183, 16],\n",
            " 'train_span_outofrange': 0,\n",
            " 'training.batch_size': 64,\n",
            " 'training.epochs': 25,\n",
            " 'training.train_length': 78183,\n",
            " 'training.train_steps': 1221,\n",
            " 'training.val_length': 26061,\n",
            " 'training.val_steps': 814,\n",
            " 'val_shape': [26061, 16],\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 82505}\n",
            "26062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4xL4rRSV5JW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "120e2d92-2103-41e8-e9d5-eb577b7c04f8"
      },
      "source": [
        "strategy = tf.distribute.MirroredStrategy()\n",
        "model_results = pd.DataFrame(columns=['Model Name','F1(Ans)','EM(Ans)','F1(Plau Ans)','EM(Plau Ans)'])\n",
        "with strategy.scope():\n",
        "\n",
        "  # y_test = create_answer_sequence(test,params['context_max_length'])\n",
        "  # y_test was a list changing to numpy array\n",
        "  # y_test_fixed = np.array(y_test)\n",
        "  # compute for y_test though in this case it the max of 0 and 1 for \n",
        "  # the frist half od array size for start, and rest for end\n",
        "  # y_test_new,_,_ = combine_y(y_test_fixed,test.shape[0])\n",
        "\n",
        "  test_map = {\n",
        "      'With stopwords': load_test_data(name='test-withstopwordspunct.csv'),\n",
        "      'Without stopwords': load_test_data(name='test-withoutstopwords.csv'),\n",
        "  }\n",
        "\n",
        "  test_seq_map = {\n",
        "      'With stopwords': [],\n",
        "      'Without stopwords': []\n",
        "  }\n",
        "\n",
        "  for model in list_of_models:\n",
        "    print('Loading model ',model['name'],' from ', model['loc'])\n",
        "    if(model['type']=='BERT'):\n",
        "      # print('BERT eval will be handled separately. Skipping')\n",
        "      bert_results = pd.read_csv(model['loc'])\n",
        "      bert_results.loc[bert_results['answer'].isna(), 'answer'] = '' \n",
        "      bert_results.loc[bert_results['plausible_answer'].isna(), 'plausible_answer'] = ''      \n",
        "      exact_raw, f1_raw, exact_scores_pa,f1_scores_pa = get_raw_scores(bert_results, bert_results['prediction'].values.tolist())\n",
        "      exact, f1,exact_pa, f1_pa, total = make_eval_dict(exact_raw,f1_raw, exact_scores_pa,f1_scores_pa)\n",
        "      values = [model['name'],              \n",
        "                f1,\n",
        "                exact,\n",
        "                f1_pa,\n",
        "                exact_pa]\n",
        "      zipped = zip(model_results.columns, values)\n",
        "      a_dictionary = dict(zipped)\n",
        "      model_results = model_results.append(a_dictionary,ignore_index=True)   \n",
        "      continue\n",
        "    \n",
        "    if(model['type']=='With stopwords'):\n",
        "      test = test_map['With stopwords']\n",
        "      params = loadparams(name='params_withstopwords_withuse.json')\n",
        "      tokenizer = load_tokenizer(name='use')\n",
        "      if not test_seq_map['With stopwords']:\n",
        "        print('Generating question sequnce for with stopwords ...')\n",
        "        test_context_sequence, test_question_sequence = generate_question_context_sequence(context=test[\"clean_context\"].values,\n",
        "                                      question=test[\"clean_question\"].values,\n",
        "                                      question_max_length=params['question_max_length'],\n",
        "                                      padding=params['question_pad_seq'],\n",
        "                                      context_max_length=params['context_max_length']\n",
        "                                      )      \n",
        "        test_seq_map['With stopwords'] = [test_context_sequence,test_question_sequence]\n",
        "      else:\n",
        "        print('Using question sequnce for with stopwords ...')\n",
        "        test_context_sequence = test_seq_map['With stopwords'][0]\n",
        "        test_question_sequence = test_seq_map['With stopwords'][1]\n",
        "    elif(model['type']=='Without stopwords'):\n",
        "      test = test_map['Without stopwords']\n",
        "      params = loadparams(name='params_withoutstopwords.json')\n",
        "      tokenizer = load_tokenizer(name='glove')\n",
        "      if not test_seq_map['Without stopwords']:\n",
        "        print('Generating question sequnce for without stopwords ...')\n",
        "        test_context_sequence, test_question_sequence = generate_question_context_sequence(context=test[\"clean_context\"].values,\n",
        "                                      question=test[\"clean_question\"].values,\n",
        "                                      question_max_length=params['question_max_length'],\n",
        "                                      padding=params['question_pad_seq'],\n",
        "                                      context_max_length=params['context_max_length']\n",
        "                                      )      \n",
        "        test_seq_map['Without stopwords'] = [test_context_sequence,test_question_sequence]\n",
        "      else:\n",
        "        print('Using question sequnce for without stopwords ...')\n",
        "        test_context_sequence = test_seq_map['Without stopwords'][0]\n",
        "        test_question_sequence = test_seq_map['Without stopwords'][1]      \n",
        "\n",
        "    tfmodel = load_mrc_model(model['loc'])\n",
        "    print(model['name'],' model successfuly. Starting eval')\n",
        "    # tfmodel.summary()  \n",
        "    y_prediction = tfmodel.predict([test_question_sequence,test_context_sequence])\n",
        "    y_prediction_new,start_pred,end_pred = combine_y(y_prediction,test.shape[0])\n",
        "    # acc_score,macro_f1_score,micro_f1_score = accuracy_metrics(y_test_new,y_prediction_new)\n",
        "    y_pred_text = generate_y_preds_text(test,start_pred,end_pred)\n",
        "    exact_raw, f1_raw, exact_scores_pa,f1_scores_pa = get_raw_scores(test, y_pred_text)\n",
        "    exact, f1,exact_pa, f1_pa, total = make_eval_dict(exact_raw,f1_raw, exact_scores_pa,f1_scores_pa)\n",
        "    values = [model['name'],              \n",
        "              f1,\n",
        "              exact,\n",
        "              f1_pa,\n",
        "              exact_pa]\n",
        "    zipped = zip(model_results.columns, values)\n",
        "    a_dictionary = dict(zipped)\n",
        "    model_results = model_results.append(a_dictionary,ignore_index=True)   \n",
        "    del tokenizer\n",
        "    del params      \n",
        "    del y_prediction\n",
        "    del y_pred_text  \n",
        "    del start_pred\n",
        "    del end_pred\n",
        "    del tfmodel\n",
        "\n",
        "model_results.head(10)   \n",
        "model_results.to_csv(model_path + \"model_results_\" + datetime.now().strftime(\"%d-%m-%Y_%I-%M-%S_%p\") + \".csv\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "(26062, 16)\n",
            "(26062, 16)\n",
            "Loading model  LSTM Baseline  from  /content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-0/full_context_withoutstopwords_model_epoch_lstmbaseline0_nomask_gpu.h5\n",
            "Loading GloVe 300D\n",
            "Vocab Loaded -  100850\n",
            "Generating question sequnce for without stopwords ...\n",
            "LSTM Baseline  model successfuly. Starting eval\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 26062/26062 [00:29<00:00, 884.03it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model  Deep LSTM + GloVe  from  /content/drive/My Drive/AIML-MRC-Capstone/models/deeplstm/full_context_withoutstopwords_model_epoch_25_deeplstm_glove_nomask_gpu.h5\n",
            "Loading GloVe 300D\n",
            "Vocab Loaded -  100850\n",
            "Using question sequnce for without stopwords ...\n",
            "Deep LSTM + GloVe  model successfuly. Starting eval\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 26062/26062 [00:29<00:00, 893.08it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model  Bi-LSTM + GloVe  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm/full_context_withoutstopwords_model_epoch_25_bilstm_glove_nomask_gpu.h5\n",
            "Loading GloVe 300D\n",
            "Vocab Loaded -  100850\n",
            "Using question sequnce for without stopwords ...\n",
            "Bi-LSTM + GloVe  model successfuly. Starting eval\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 26062/26062 [00:29<00:00, 891.51it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model  Bi-LSTM + GloVe + Q2C Attention  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-glove/full_context_withoutstopwords_model_epoch_25_bilstm_q2c-attention_glove.h5\n",
            "Loading GloVe 300D\n",
            "Vocab Loaded -  100850\n",
            "Using question sequnce for without stopwords ...\n",
            "Bi-LSTM + GloVe + Q2C Attention  model successfuly. Starting eval\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 26062/26062 [00:29<00:00, 894.85it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model  Bi-LSTM + GloVe + Q2C-C2Q Attention  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-bidaf-glove/full_context_withoutstopwords_model_epoch_25_bilstm_bidaf_glove_nomask_gpu_after_fix.h5\n",
            "Loading GloVe 300D\n",
            "Vocab Loaded -  100850\n",
            "Using question sequnce for without stopwords ...\n",
            "Bi-LSTM + GloVe + Q2C-C2Q Attention  model successfuly. Starting eval\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 26062/26062 [00:29<00:00, 895.16it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model  LSTM Baseline + Universal Sentence Encode  from  /content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-use-withstop/full_context_withstopwords_model_epoch_25_lstmbaseline0_nomask_gpu.h5\n",
            "Loading Universal Sentence Encoder\n",
            "Vocab Loaded -  82505\n",
            "Generating question sequnce for with stopwords ...\n",
            "LSTM Baseline + Universal Sentence Encode  model successfuly. Starting eval\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 26062/26062 [00:29<00:00, 893.83it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model  Bi-LSTM + Universal Sentence Encode  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-use-withstop/full_context_withstopwords_model_epoch_25_bilstm_use_nomask_gpu.h5\n",
            "Loading Universal Sentence Encoder\n",
            "Vocab Loaded -  82505\n",
            "Using question sequnce for with stopwords ...\n",
            "Bi-LSTM + Universal Sentence Encode  model successfuly. Starting eval\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 26062/26062 [00:29<00:00, 893.31it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model  Bi-LSTM + Q2C Attention + Universal Sentence Encode  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-use-withstop/full_context_withstopwords_model_epoch_25_bilstm_q2c-attention_use.h5\n",
            "Loading Universal Sentence Encoder\n",
            "Vocab Loaded -  82505\n",
            "Using question sequnce for with stopwords ...\n",
            "Bi-LSTM + Q2C Attention + Universal Sentence Encode  model successfuly. Starting eval\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 26062/26062 [00:29<00:00, 894.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model  BERT + Cased_L-12_H-768_A-12 + DeepPavlov  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bert/bert-results.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHx9KbHZ8pNb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "797145af-d7b1-439b-aea7-b2055eaacea8"
      },
      "source": [
        "model_results.head(10)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model Name</th>\n",
              "      <th>F1(Ans)</th>\n",
              "      <th>EM(Ans)</th>\n",
              "      <th>F1(Plau Ans)</th>\n",
              "      <th>EM(Plau Ans)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM Baseline</td>\n",
              "      <td>29.001121</td>\n",
              "      <td>28.225002</td>\n",
              "      <td>0.214623</td>\n",
              "      <td>0.053718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Deep LSTM + GloVe</td>\n",
              "      <td>26.666439</td>\n",
              "      <td>26.030236</td>\n",
              "      <td>0.276301</td>\n",
              "      <td>0.069066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bi-LSTM + GloVe</td>\n",
              "      <td>28.021961</td>\n",
              "      <td>27.526667</td>\n",
              "      <td>0.281106</td>\n",
              "      <td>0.126621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bi-LSTM + GloVe + Q2C Attention</td>\n",
              "      <td>33.095119</td>\n",
              "      <td>33.094160</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bi-LSTM + GloVe + Q2C-C2Q Attention</td>\n",
              "      <td>29.625994</td>\n",
              "      <td>29.138209</td>\n",
              "      <td>0.199186</td>\n",
              "      <td>0.069066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM Baseline + Universal Sentence Encode</td>\n",
              "      <td>24.551010</td>\n",
              "      <td>22.956795</td>\n",
              "      <td>0.618575</td>\n",
              "      <td>0.099762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Bi-LSTM + Universal Sentence Encode</td>\n",
              "      <td>32.025226</td>\n",
              "      <td>31.544010</td>\n",
              "      <td>0.068411</td>\n",
              "      <td>0.023022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Bi-LSTM + Q2C Attention + Universal Sentence E...</td>\n",
              "      <td>30.695975</td>\n",
              "      <td>30.120482</td>\n",
              "      <td>0.146946</td>\n",
              "      <td>0.042207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>BERT + Cased_L-12_H-768_A-12 + DeepPavlov</td>\n",
              "      <td>59.096928</td>\n",
              "      <td>51.362136</td>\n",
              "      <td>22.539289</td>\n",
              "      <td>18.179725</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Model Name  ...  EM(Plau Ans)\n",
              "0                                      LSTM Baseline  ...      0.053718\n",
              "1                                  Deep LSTM + GloVe  ...      0.069066\n",
              "2                                    Bi-LSTM + GloVe  ...      0.126621\n",
              "3                    Bi-LSTM + GloVe + Q2C Attention  ...      0.000000\n",
              "4                Bi-LSTM + GloVe + Q2C-C2Q Attention  ...      0.069066\n",
              "5          LSTM Baseline + Universal Sentence Encode  ...      0.099762\n",
              "6                Bi-LSTM + Universal Sentence Encode  ...      0.023022\n",
              "7  Bi-LSTM + Q2C Attention + Universal Sentence E...  ...      0.042207\n",
              "8          BERT + Cased_L-12_H-768_A-12 + DeepPavlov  ...     18.179725\n",
              "\n",
              "[9 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCkMUNLEFYwG",
        "colab_type": "text"
      },
      "source": [
        "## Out of domain handing \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bppIF0J_Fews",
        "colab_type": "text"
      },
      "source": [
        "### Different language \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmEXz0ORFhIw",
        "colab_type": "text"
      },
      "source": [
        "### Totally irrelevant question given a known context from train domain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OOCtgDAFj-X",
        "colab_type": "text"
      },
      "source": [
        "### Totally different context say from Medicine and ask answer from it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwvNDiQRFZql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WulPny3Z00mm",
        "colab_type": "text"
      },
      "source": [
        "## Eval on manual input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v33J0DrkgSh6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "19740284-575a-4bac-a955-7b1750783af5"
      },
      "source": [
        "all_models = load_all_models()\n",
        "all_models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model  LSTM Baseline  from  /content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-0/full_context_withoutstopwords_model_epoch_lstmbaseline0_nomask_gpu.h5\n",
            "Loading model  Deep LSTM + GloVe  from  /content/drive/My Drive/AIML-MRC-Capstone/models/deeplstm/full_context_withoutstopwords_model_epoch_25_deeplstm_glove_nomask_gpu.h5\n",
            "Loading model  Bi-LSTM + GloVe  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm/full_context_withoutstopwords_model_epoch_25_bilstm_glove_nomask_gpu.h5\n",
            "Loading model  Bi-LSTM + GloVe + Q2C Attention  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-glove/full_context_withoutstopwords_model_epoch_25_bilstm_q2c-attention_glove.h5\n",
            "Loading model  Bi-LSTM + GloVe + Q2C-C2Q Attention  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-bidaf-glove/full_context_withoutstopwords_model_epoch_25_bilstm_bidaf_glove_nomask_gpu_after_fix.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Bi-LSTM + GloVe': <tensorflow.python.keras.engine.training.Model at 0x7fcbe0ff4da0>,\n",
              " 'Bi-LSTM + GloVe + Q2C Attention': <tensorflow.python.keras.engine.training.Model at 0x7fcbe0c5c550>,\n",
              " 'Bi-LSTM + GloVe + Q2C-C2Q Attention': <tensorflow.python.keras.engine.training.Model at 0x7fcbe0812ac8>,\n",
              " 'Deep LSTM + GloVe': <tensorflow.python.keras.engine.training.Model at 0x7fcc8058fcf8>,\n",
              " 'LSTM Baseline': <tensorflow.python.keras.engine.training.Model at 0x7fcc8070bb70>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OBJci8mhZmG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54fe8e45-b154-4449-8e44-a27029cacfcb"
      },
      "source": [
        "len(all_models)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcuhAMQNrgi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del all_models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R03OP9TxDc6P",
        "colab_type": "text"
      },
      "source": [
        "### TEST1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTNhlpswRbMW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "outputId": "59fee2b1-cbba-408c-bed1-767d6905efa1"
      },
      "source": [
        "\n",
        "c1=\"With a record 15,968 coronavirus cases reported in the past 24 hours, \" \\\n",
        ".join('the total count in India crossed the 4.5 lakh mark. The death toll has gone up to 14,476')\n",
        "\n",
        "# .append(' as Covid-19 claimed 465 lives in 24 hours. Of the 4,56,183 cases, over 2.5 lakh ')\n",
        "\n",
        "q1='what is to total count'\n",
        "\n",
        "cs = [c1]\n",
        "qs = [q1]\n",
        "# c_,q_,span,y_,answer = predit_test(test['context'].iloc[39],test['question'].iloc[39])\n",
        "df = pd.DataFrame(columns=[\"Model\",\"Context\", \"Question\",\"Predicted Ans\"])\n",
        "for m in all_models:\n",
        "  c_,q_,span,y_,answer = predit_test(cs,qs, all_models[m])\n",
        "  # print('new context')\n",
        "  # pprint.pprint(c_[0])\n",
        "  # print('new q',q_)\n",
        "  print('Prediction for model - ', m)\n",
        "  for i in range(len(cs)):    \n",
        "    values = [m,cs[i],qs[i],answer[i]]\n",
        "    zipped = zip(df.columns, values)\n",
        "    a_dictionary = dict(zipped)\n",
        "    df = df.append(a_dictionary,ignore_index=True)       \n",
        "\n",
        "df  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction for model -  LSTM Baseline\n",
            "Prediction for model -  Deep LSTM + GloVe\n",
            "Prediction for model -  Bi-LSTM + GloVe\n",
            "Prediction for model -  Bi-LSTM + GloVe + Q2C Attention\n",
            "Prediction for model -  Bi-LSTM + GloVe + Q2C-C2Q Attention\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 677) for input Tensor(\"CONTEXT_INPUT_5:0\", shape=(None, 677), dtype=float32), but it was called on an input with incompatible shape (None, 426).\n",
            "Prediction for model -  LSTM Baseline + Universal Sentence Encode\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 677) for input Tensor(\"CONTEXT_INPUT_6:0\", shape=(None, 677), dtype=float32), but it was called on an input with incompatible shape (None, 426).\n",
            "Prediction for model -  Bi-LSTM + Universal Sentence Encode\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Context</th>\n",
              "      <th>Question</th>\n",
              "      <th>Predicted Ans</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LSTM Baseline</td>\n",
              "      <td>tWith a record 15,968 coronavirus cases report...</td>\n",
              "      <td>what is to total count</td>\n",
              "      <td>the past</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Deep LSTM + GloVe</td>\n",
              "      <td>tWith a record 15,968 coronavirus cases report...</td>\n",
              "      <td>what is to total count</td>\n",
              "      <td>twith a record 15968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bi-LSTM + GloVe</td>\n",
              "      <td>tWith a record 15,968 coronavirus cases report...</td>\n",
              "      <td>what is to total count</td>\n",
              "      <td>cases</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bi-LSTM + GloVe + Q2C Attention</td>\n",
              "      <td>tWith a record 15,968 coronavirus cases report...</td>\n",
              "      <td>what is to total count</td>\n",
              "      <td>cases</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Bi-LSTM + GloVe + Q2C-C2Q Attention</td>\n",
              "      <td>tWith a record 15,968 coronavirus cases report...</td>\n",
              "      <td>what is to total count</td>\n",
              "      <td>cases</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LSTM Baseline + Universal Sentence Encode</td>\n",
              "      <td>tWith a record 15,968 coronavirus cases report...</td>\n",
              "      <td>what is to total count</td>\n",
              "      <td>twith a record 15968 coronavirus cases reporte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Bi-LSTM + Universal Sentence Encode</td>\n",
              "      <td>tWith a record 15,968 coronavirus cases report...</td>\n",
              "      <td>what is to total count</td>\n",
              "      <td>cases reported in the past 24 hours hwith a re...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       Model  ...                                      Predicted Ans\n",
              "0                              LSTM Baseline  ...                                           the past\n",
              "1                          Deep LSTM + GloVe  ...                               twith a record 15968\n",
              "2                            Bi-LSTM + GloVe  ...                                              cases\n",
              "3            Bi-LSTM + GloVe + Q2C Attention  ...                                              cases\n",
              "4        Bi-LSTM + GloVe + Q2C-C2Q Attention  ...                                              cases\n",
              "5  LSTM Baseline + Universal Sentence Encode  ...  twith a record 15968 coronavirus cases reporte...\n",
              "6        Bi-LSTM + Universal Sentence Encode  ...  cases reported in the past 24 hours hwith a re...\n",
              "\n",
              "[7 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkyTSbDp1Jfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncB__cjC6T88",
        "colab_type": "text"
      },
      "source": [
        "# Serving API "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li214BwR8PaI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "63da4fa3-2899-4ba5-93b5-5ac911e5c598"
      },
      "source": [
        "# A nice and nifty way done by this fellow - https://medium.com/@kshitijvijay271199/flask-on-google-colab-f6525986797b\n",
        "# and of course ngrok saves the day !!\n",
        "!pip install flask-ngrok"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMeXDG-y6WAC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "aa7dd537-dc5f-4d77-bad2-dd69111cee23"
      },
      "source": [
        "all_params = {\n",
        "    \"params_withoutstopwords\": loadparams(name='params_withoutstopwords.json'),\n",
        "    \"params_withstopwords_withuse\": loadparams(name='params_withstopwords_withuse.json')\n",
        "}\n",
        "all_tokenizers = {\n",
        "    \"glove\": load_tokenizer(all_params['params_withoutstopwords']),\n",
        "    \"use\": load_tokenizer(all_params['params_withstopwords_withuse'],'use')\n",
        "}\n",
        "all_params\n",
        "all_models = load_all_models()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading GloVe 300D\n",
            "Vocab Loaded -  100850\n",
            "Loading Universal Sentence Encoder\n",
            "Vocab Loaded -  82505\n",
            "Loading model  LSTM Baseline  from  /content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-0/full_context_withoutstopwords_model_epoch_lstmbaseline0_nomask_gpu.h5\n",
            "Loading model  Deep LSTM + GloVe  from  /content/drive/My Drive/AIML-MRC-Capstone/models/deeplstm/full_context_withoutstopwords_model_epoch_25_deeplstm_glove_nomask_gpu.h5\n",
            "Loading model  Bi-LSTM + GloVe  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm/full_context_withoutstopwords_model_epoch_25_bilstm_glove_nomask_gpu.h5\n",
            "Loading model  Bi-LSTM + GloVe + Q2C Attention  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-q2c-attention-glove/full_context_withoutstopwords_model_epoch_25_bilstm_q2c-attention_glove.h5\n",
            "Loading model  Bi-LSTM + GloVe + Q2C-C2Q Attention  from  /content/drive/My Drive/AIML-MRC-Capstone/models/bilstm-bidaf-glove/full_context_withoutstopwords_model_epoch_25_bilstm_bidaf_glove_nomask_gpu_after_fix.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cepdwTqD8Syx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "aeb6fb47-56ae-4e93-93d0-b4e7dde2e59e"
      },
      "source": [
        "from flask import Flask\n",
        "from flask import request\n",
        "from flask import jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)  # Start ngrok when app is run\n",
        "\n",
        "# for / root, return Hello Word\n",
        "@app.route(\"/\")\n",
        "def root():\n",
        "    url = request.method\n",
        "    return f\"Welcome to the AIML Batch 4 - MRC Capstone\"\n",
        "\n",
        "@app.route(\"/models\")\n",
        "def listmodels():\n",
        "    url = request.method\n",
        "    return {'models':list_of_models}\n",
        "\n",
        "# for / root, return Hello Word\n",
        "@app.route(\"/predict\", methods=['POST'])\n",
        "def predict():\n",
        "    if request.method == 'POST':\n",
        "        try:\n",
        "            data = request.get_json()\n",
        "            modelid = data[\"model\"] \n",
        "            context = data[\"c\"]\n",
        "            question = data[\"q\"]\n",
        "\n",
        "            if(modelid == 'BERT'):\n",
        "              return jsonify(\"BERT is not yet supported\")            \n",
        "\n",
        "            thatmodel = [m for m in list_of_models if m['id']==modelid][0]\n",
        "            print('Running with model ',thatmodel['name'])\n",
        "            if(thatmodel['type']=='With stopwords'):\n",
        "              params = all_params['params_withstopwords_withuse']\n",
        "              tokenizer = all_tokenizers['use']\n",
        "            elif(thatmodel['type']=='Without stopwords'):\n",
        "              params = all_params['params_withoutstopwords']\n",
        "              tokenizer = all_tokenizers['glove']\n",
        "            \n",
        "            tfmodel = all_models[thatmodel['name']]            \n",
        "            c_,q_,span,y_,answer = predit_test([context],[question],tfmodel, params=params)                \n",
        "            print(context, question, answer)         \n",
        "            # lin_reg = joblib.load(\"./linear_regression_model.pkl\")\n",
        "        except err:\n",
        "            print(err)\n",
        "            return jsonify(\"Some thing is not right !!\")\n",
        "        finally:\n",
        "          del tfmodel\n",
        "\n",
        "        return jsonify({'context': context, 'question':question, 'answer': answer})\n",
        "\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://c0a7ccc2cea1.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqy24lzf_1yt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}