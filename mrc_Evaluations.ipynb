{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mrc_Evaluations.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1OvpbmeuHylPXPBoWC9BdEAV2l87bDBh8",
      "authorship_tag": "ABX9TyNwsLUBAopnvK3KeBYgimC9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanbasu/aiml-capstone/blob/master/mrc_Evaluations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOGCXAgWaIsd",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaepomVyaJfm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "e156b311-da99-4528-bbfb-bde62ac53dff"
      },
      "source": [
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "tf.debugging.set_log_device_placement(False)\n",
        "import pickle\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pprint\n",
        "from tensorflow.keras.layers import Bidirectional,LSTM,Dense,Dropout,BatchNormalization,Flatten,Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from numpy import array\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "from tensorflow.keras.models import load_model\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuIsS7WFh76y",
        "colab_type": "text"
      },
      "source": [
        "# Load Google Bucket as drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCaxxOEXh8GK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a81d21e8-2b15-4126-b286-ef4ab3878234"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "project_id = 'ai-ml-capstone'\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil ls\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "gs://aiml-capstone/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3NaG4vBiPvw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "c93ab87e-4b14-4c26-f00d-01364d3e17f9"
      },
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   653  100   653    0     0  25115      0 --:--:-- --:--:-- --:--:-- 25115\n",
            "OK\n",
            "49 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,274 kB of archives.\n",
            "After this operation, 12.8 MB of additional disk space will be used.\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 144328 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_0.28.1_amd64.deb ...\n",
            "Unpacking gcsfuse (0.28.1) ...\n",
            "Setting up gcsfuse (0.28.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdNVzqJ_jfs2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b5df8b07-f587-47f7-be8b-5b3c30b205f6"
      },
      "source": [
        "!mkdir gbucket\n",
        "!gcsfuse --implicit-dirs aiml-capstone gbucket \n",
        "# !umount gbucket"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using mount point: /content/gbucket\n",
            "Opening GCS connection...\n",
            "Opening bucket...\n",
            "Mounting file system...\n",
            "File system has been successfully mounted.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in-L5Mi9kEir",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "4dad9b43-ad9c-4e53-9b9e-6416bd76c805"
      },
      "source": [
        "!ls gbucket/lstmbaseline-0/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " context_withoutstopwords_model_epoch_24.h5\n",
            " context_withoutstopwords_model_epoch_25_lstmbaseline0_glove_nomask_gpu.h5\n",
            " full_context_withoutstopwords_model_epoch.h5\n",
            " full_context_withoutstopwords_model_epoch_lstmbaseline0_glove_nomask_gpu.h5\n",
            "'NN Network.png'\n",
            " params.json\n",
            " results.csv\n",
            " tf-serve\n",
            " tokenizer.pkl\n",
            " whole_model_epoch_24.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUPKTSR-GWt5",
        "colab_type": "text"
      },
      "source": [
        "# List of Models\n",
        "\n",
        "As part of our capstone, we are in process of evaluating the following models\n",
        "\n",
        "Data | Model | On GPU | Masking | Padding | Epoch | Location | \n",
        "--- | --- | --- | --- | --- | --- | --- | \n",
        "Without stopwords | SVM | No | - | - | - | [here](https://storage.cloud.google.com/aiml-capstone/svm/)\n",
        "Without stopwords | LSTM Baseline | No | No | Pre | 25 | [here](https://storage.cloud.google.com/aiml-capstone/lstmbaseline-0/full_context_withoutstopwords_model_epoch_lstmbaseline0_nomask_gpu.h5)\n",
        "Without stopwords | Deep LSTM + GloVe | Yes | No | Pre | 25 | [here](https://storage.cloud.google.com/aiml-capstone/deeplstm/full_context_withoutstopwords_model_epoch_deeplstm_gpu.h5)\n",
        "Without stopwords | Bi-LSTM + GloVe | No | No | Pre | 10 | [here](https://storage.cloud.google.com/aiml-capstone/bilstm/full_context_withoutstopwords_model_epoch_10_bilstm_cpu.h5)\n",
        "Without stopwords | Bi-LSTM + GloVe + Q2C Attention | Yes | No | Pre | 25 | [here](https://storage.cloud.google.com/aiml-capstone/bilstm-q2c-attention-glove/full_context_withoutstopwords_nomask_epoch_25_bilstm_q2c-attention_glove_gpu.h5)\n",
        "Without stopwords | Bi-LSTM + GloVe + Q2C-C2Q Attention | Yes | No | Pre | 25 | [here](https://storage.cloud.google.com/aiml-capstone/bilstm-bidaf-glove/full_context_withoutstopwords_model_epoch_25_bilstm_bidaf_glove_nomask_gpu.h5)\n",
        "--- | --- | --- | --- | --- | --- | --- | \n",
        "With Stopwords | Bi-LSTM + Universal Sentence Encoder + Q2C Attention\n",
        "With Stopwords | Bi-LSTM + Universal Sentence Encoder + Q2C-C2Q Attention\n",
        "With Stopwords | BERT + Universal Sentence Encoder \n",
        "With Stopwords | GPT2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_GOOYs6cXfu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "729730c3-9344-415b-e0cc-117c684d9135"
      },
      "source": [
        "list_of_models = {\n",
        "    \"lstm-baseline\": {\n",
        "        \"name\":\"LSTM Baseline\",\n",
        "        \"loc\":\"https://storage.cloud.google.com/aiml-capstone/lstmbaseline-0/context_withoutstopwords_model_epoch_24.h5\",\n",
        "    },\n",
        "    \"lstm-glove\": {\n",
        "        \"name\":\"LSTM + GloVe\",\n",
        "        \"loc\":\"https://storage.cloud.google.com/aiml-capstone/lstmbaseline-0/full_context_withoutstopwords_model_epoch_lstmbaseline0_glove_nomask_gpu.h5\"\n",
        "    },\n",
        "    \"deeplstm-glove\":{\n",
        "        \"name\":\"Deep LSTM + GloVe\",\n",
        "        \"loc\":\"https://storage.cloud.google.com/aiml-capstone/deeplstm/full_context_withoutstopwords_model_epoch_deeplstm_gpu.h5\"\n",
        "    },\n",
        "    \"bilstm-glove\":{\n",
        "        \"name\":\"Bi-LSTM + GloVe\",\n",
        "        \"loc\":\"https://storage.cloud.google.com/aiml-capstone/bilstm/full_context_withoutstopwords_model_epoch_10_bilstm_cpu.h5\"\n",
        "    },\n",
        "    \"bilstm-glove-q2c-attention\": {\n",
        "        \"name\":\"Bi-LSTM + GloVe + Q2C Attention\",\n",
        "        \"loc\":\"https://storage.cloud.google.com/aiml-capstone/bilstm-q2c-attention-glove/full_context_withoutstopwords_nomask_epoch_25_bilstm_q2c-attention_glove_gpu.h5\"        \n",
        "    },\n",
        "    \"bilstm-glove-q2c-c2q-attention\":{\n",
        "        \"name\":\"Bi-LSTM + GloVe + Q2C-C2Q Attention\",\n",
        "        \"loc\":\"https://storage.cloud.google.com/aiml-capstone/bilstm-bidaf-glove/full_context_withoutstopwords_model_epoch_25_bilstm_bidaf_glove_nomask_gpu.h5\"\n",
        "    },\n",
        "    \"bilstm-use-q2c-attention\":{\n",
        "        \"name\":\"Bi-LSTM + Universal Sentence Encoder + Q2C Attention\",\n",
        "        \"loc\":\"\"\n",
        "    },\n",
        "    \"bilstm-use-q2c-c2q-attention\":{\n",
        "        \"name\":\"Bi-LSTM + Universal Sentence Encoder + Q2C-C2Q Attention\",\n",
        "        \"loc\":\"\"\n",
        "    },\n",
        "    \"bert-use\":{\n",
        "        \"name\":\"BERT + Universal Sentence Encoder\",\n",
        "        \"loc\":\"\"\n",
        "    },\n",
        "    \"gpt\":{\n",
        "        \"name\":\"GPT\",\n",
        "        \"loc\":\"\"\n",
        "    }    \n",
        "}\n",
        "\n",
        "list_of_models"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bert-use': {'loc': '', 'name': 'BERT + Universal Sentence Encoder'},\n",
              " 'bilstm-glove': {'loc': 'https://storage.cloud.google.com/aiml-capstone/bilstm/full_context_withoutstopwords_model_epoch_10_bilstm_cpu.h5',\n",
              "  'name': 'Bi-LSTM + GloVe'},\n",
              " 'bilstm-glove-q2c-attention': {'loc': 'https://storage.cloud.google.com/aiml-capstone/bilstm-q2c-attention-glove/full_context_withoutstopwords_nomask_epoch_25_bilstm_q2c-attention_glove_gpu.h5',\n",
              "  'name': 'Bi-LSTM + GloVe + Q2C Attention'},\n",
              " 'bilstm-glove-q2c-c2q-attention': {'loc': 'https://storage.cloud.google.com/aiml-capstone/bilstm-bidaf-glove/full_context_withoutstopwords_model_epoch_25_bilstm_bidaf_glove_nomask_gpu.h5',\n",
              "  'name': 'Bi-LSTM + GloVe + Q2C-C2Q Attention'},\n",
              " 'bilstm-use-q2c-attention': {'loc': '',\n",
              "  'name': 'Bi-LSTM + Universal Sentence Encoder + Q2C Attention'},\n",
              " 'bilstm-use-q2c-c2q-attention': {'loc': '',\n",
              "  'name': 'Bi-LSTM + Universal Sentence Encoder + Q2C-C2Q Attention'},\n",
              " 'deeplstm-glove': {'loc': 'https://storage.cloud.google.com/aiml-capstone/deeplstm/full_context_withoutstopwords_model_epoch_deeplstm_gpu.h5',\n",
              "  'name': 'Deep LSTM + GloVe'},\n",
              " 'gpt': {'loc': '', 'name': 'GPT'},\n",
              " 'lstm-baseline': {'loc': 'https://storage.cloud.google.com/aiml-capstone/lstmbaseline-0/context_withoutstopwords_model_epoch_24.h5',\n",
              "  'name': 'LSTM Baseline'},\n",
              " 'lstm-glove': {'loc': 'https://storage.cloud.google.com/aiml-capstone/lstmbaseline-0/full_context_withoutstopwords_model_epoch_lstmbaseline0_glove_nomask_gpu.h5',\n",
              "  'name': 'LSTM + GloVe'}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xfm8h4IBSGbh",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEtw1J-aPecV",
        "colab_type": "text"
      },
      "source": [
        "## bAbi \n",
        "\n",
        "Reference Paper = https://arxiv.org/pdf/1502.05698.pdf \n",
        "\n",
        "GitHub - https://github.com/facebookarchive/bAbI-tasks\n",
        "\n",
        "![alt text](https://storage.cloud.google.com/aiml-capstone/Screenshot%202020-06-20%20at%2011.11.50%20AM.png)\n",
        "![alt text](https://storage.cloud.google.com/aiml-capstone/Screenshot%202020-06-20%20at%2011.11.03%20AM.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ5nidQ8SVog",
        "colab_type": "text"
      },
      "source": [
        "## SQuAD test dataset and published dev set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACmlhKB1S1ZF",
        "colab_type": "text"
      },
      "source": [
        "Refer to this eval metrics - https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
        "\n",
        "Eval dataset from SQuAD - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
        "\n",
        "Test Dataset from training = "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGFyqInm7ZD0",
        "colab_type": "text"
      },
      "source": [
        "## News Domain Specific Evaluations\n",
        "\n",
        "Test on Sample News Context, Question and Answer pairs ??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Yw8C9nY7uXT",
        "colab_type": "text"
      },
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDyvisumUImJ",
        "colab_type": "text"
      },
      "source": [
        "# Common Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XtBcy2Qwm2-c"
      },
      "source": [
        "## Custom function for preprocessing of context and question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LFr3-S_Gm9FX",
        "colab": {}
      },
      "source": [
        "# remove unwanted chars\n",
        "# convert to lowercase\n",
        "# remove unwanted spaces\n",
        "# remove stop words\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "## reference \n",
        "def decontracted(phrase):\n",
        "    \"\"\"\n",
        "    This function remooves punctuation from given sentence.\n",
        "    \"\"\"\n",
        "\n",
        "    if(phrase is np.nan):\n",
        "      return 'impossible'      \n",
        "\n",
        "    try:      \n",
        "      # specific\n",
        "      phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "      phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "      # general\n",
        "      phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "      phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "      phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "      phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "      phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "      phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "      phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "      phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "      \n",
        "      # string operation\n",
        "      phrase = phrase.replace('\\\\r', ' ')\n",
        "      phrase = phrase.replace('\\\\\"', ' ')\n",
        "      phrase = phrase.replace('\\\\n', ' ')\n",
        "\n",
        "      phrase = re.sub('[^A-Za-z0-9]+', ' ', phrase.lower())\n",
        "    except:\n",
        "      print(phrase)  \n",
        "    \n",
        "    return phrase\n",
        "\n",
        "def preprocess_text(corpus, text_lower_case=True, \n",
        "                      special_char_removal=True, stopword_removal=True, remove_digits=False):    \n",
        "    normalized_text = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # doc = decontracted(doc)\n",
        "        # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        # remove special characters and\\or digits    \n",
        "        if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = remove_special_characters(doc, remove_digits=remove_digits) \n",
        "\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc)\n",
        "\n",
        "        normalized_text.append(doc)\n",
        "        \n",
        "    return normalized_text\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    #Using regex\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text):  \n",
        "    word_tokens = word_tokenize(text) \n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]   \n",
        "    filtered_sentence = [] \n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(w)                 \n",
        "    return ' '.join(filtered_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "20-D8HBeOf_p"
      },
      "source": [
        "## Answer Span from Context and Answer, and reverse for predicted spans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vbM8z2AEjxKK",
        "colab": {}
      },
      "source": [
        "def tokenize(sentence):\n",
        "    \"\"\"\n",
        "    Returns tokenised words.\n",
        "    \"\"\"\n",
        "    return nltk.word_tokenize(sentence)\n",
        "\n",
        "def answer_span(context,ans):\n",
        "    \"\"\"\n",
        "    This funtion returns anwer span start index and end index.\n",
        "    \"\"\"\n",
        "    ans_token = tokenize(ans)\n",
        "    con_token = tokenize(context)\n",
        "    ans_len = len(ans_token)\n",
        "    \n",
        "    if ans_len!=0 and ans_token[0] in con_token:\n",
        "    \n",
        "        indices = [i for i, x in enumerate(con_token) if x == ans_token[0]]        \n",
        "        try:\n",
        "\n",
        "            if(len(indices)>1):\n",
        "                start = [i for i in indices if (con_token[i:i+ans_len] == ans_token) ]\n",
        "                end = start[0] + ans_len - 1\n",
        "                return start[0],end\n",
        "\n",
        "            else:\n",
        "                start = con_token.index(ans_token[0])\n",
        "                end = start + ans_len - 1\n",
        "                return start,end\n",
        "        except:\n",
        "            return -1,-1\n",
        "    else:\n",
        "        return -1,-1\n",
        "\n",
        "def span_to_answer(span, context):\n",
        "  con_token = tokenize(context)  \n",
        "  return ' '.join(con_token[span[0]:span[1]+1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_sveyxKK5j8q"
      },
      "source": [
        "## Update and persist params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K170rfN15qGP",
        "colab": {}
      },
      "source": [
        "### SAVE PARAMS\n",
        "# Writing to sample.json \n",
        "\n",
        "def updateparams():\n",
        "  with open(model_path + \"params.json\", \"w\") as p: \n",
        "    p.write(json.dumps(params))\n",
        "  print(\"params.jsop updated and can be found in \", model_path + \"params.json\")  \n",
        "\n",
        "# updateparams()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yYMDmMJNU9mw",
        "colab": {}
      },
      "source": [
        "def showparams():\n",
        "  pprint.pprint(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SklWt_NbVzOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadparams():\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V0TSjNkdXzuR"
      },
      "source": [
        "## Create a common function to generate sequences (useful in prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sq97Vq4kXTST",
        "colab": {}
      },
      "source": [
        "# function to generate sequences withg appropiate padding\n",
        "def generate_question_context_sequence(context, question):\n",
        "  question_seq = tokenizer.texts_to_sequences(question)\n",
        "  context_seq = tokenizer.texts_to_sequences(context)\n",
        "  question_seq = preprocessing.sequence.pad_sequences(question_seq,\n",
        "                                                      maxlen=params['question_max_length'],\n",
        "                                                      padding=params['question_pad_seq'])\n",
        "  context_seq = preprocessing.sequence.pad_sequences(context_seq,\n",
        "                                                     maxlen=params['context_max_length'],\n",
        "                                                     padding=params['question_pad_seq'])\n",
        "  return context_seq, question_seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v78fhG3Tdhx5"
      },
      "source": [
        "## Create a common function to predict and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v6KDfM25dhZr",
        "colab": {}
      },
      "source": [
        "def predit_test(context, question):\n",
        "  # get sequence for context and question\n",
        "  c_ = preprocess_text(context)\n",
        "  q_ = preprocess_text(question,stopword_removal=False)\n",
        "  c,q = generate_question_context_sequence(c_, q_)  \n",
        "  y_ = model.predict([q,c])    \n",
        "  # # for i in range(26062):\n",
        "  s = np.argmax(y_[0,:params['context_max_length']])\n",
        "  e = np.argmax(y_[0,params['context_max_length']:])\n",
        "  answer = span_to_answer((s,e),c_[0])\n",
        "  \n",
        "  # print(c.shape,q.shape,y_.shape,s,e,answer)  \n",
        "  # print(s, e)\n",
        "  return c_,q_,[s,e],y_,answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSJXR52VTWsf",
        "colab_type": "text"
      },
      "source": [
        "## Load Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5FJHi14rHNTX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "efef1243-7615-4bc8-c0b9-cd76b38ec69c"
      },
      "source": [
        "def load_tokenizer():\n",
        "  with open(model_path + \"tokenizer.pkl\",\"rb\") as infile:\n",
        "      tokenizer = pickle.load(infile)\n",
        "\n",
        "len(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100850"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIjvBaQ4WFZQ",
        "colab_type": "text"
      },
      "source": [
        "## Load Test Data with Ground Truth Known"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYkWpLxxWFii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import resample,shuffle\n",
        "\n",
        "def load_data_withoutstopwords():\n",
        "  #### NOTE THE 2 data frames's\n",
        "  df_nostopwords = 'test_squad_data_final_context_withoutstopwords.csv'\n",
        "  # df_withstopwords = 'squad_data_final_withstopword_withpunctuation.csv'\n",
        "  test_squad_df = pd.read_csv(project_path+df_nostopwords)\n",
        "  test_squad_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "\n",
        "\n",
        "  test_squad_df[\"answer_word_span\"] = test_squad_df[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "  print(test_squad_df.info())\n",
        "  return test_squad_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH025QzmyMAJ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDX397bcyMZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logits_loss(y_true,logits):\n",
        "    \"\"\"\n",
        "    Custom loss function which minimises log_loss.\n",
        "    Referance https://stackoverflow.com/questions/50063613/add-loss-function-in-keras\n",
        "    \"\"\"\n",
        "    \n",
        "    #y_true = tf.cast(y_true,dtype=tf.int32)\n",
        "    #logits = tf.cast(logits,dtype=tf.float32)\n",
        "    \n",
        "    # breaking the tensor into two half's to get start and end label.\n",
        "    start_label = y_true[:,:params['context_max_length']]\n",
        "    end_label = y_true[:,params['context_max_length']:]\n",
        "    \n",
        "    # braking the logits tensor into start and end part for loss calcultion.\n",
        "    start_logit = logits[:,:params['context_max_length']]\n",
        "    end_logit = logits[:,params['context_max_length']:]\n",
        "    \n",
        "    start_loss = tf.keras.backend.categorical_crossentropy(start_label,start_logit)\n",
        "    end_loss = tf.keras.backend.categorical_crossentropy(end_label,end_logit)\n",
        "    \n",
        "#     start_loss = tf.losses.sparse_softmax_cross_entropy(labels=start_label, logits=start_logit)\n",
        "#     end_loss = tf.losses.sparse_softmax_cross_entropy(labels=end_label, logits=end_logit)\n",
        "    \n",
        "    # as per paer\n",
        "    \n",
        "    loss = start_loss + end_loss\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP43qWYkewja",
        "colab_type": "text"
      },
      "source": [
        "## Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv8x2nxpSTwR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52d1cc36-7b6b-4f31-e6fd-560e08423bd6"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pSzIAlTe07P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow import keras\n",
        "def load_mrc_model(model_name):\n",
        "  model_path = list_of_models[model_name]['loc'].replace('https://storage.cloud.google.com/aiml-capstone/','gbucket/')\n",
        "  print(model_path)\n",
        "  # tf.keras.utils.get_file\n",
        "  # json_file = open('/content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-0/lstmbaseline-model.json', 'r')\n",
        "  # loaded_model_json = json_file.read()\n",
        "  # json_file.close()\n",
        "  # new_model= keras.models.model_from_json(loaded_model_json)\n",
        "  custom_objects = {\"logits_loss\": logits_loss}\n",
        "  new_model = keras.models.load_model('/content/drive/My Drive/AIML-MRC-Capstone/models/lstmbaseline-0/tf-serve',\n",
        "                                      custom_objects=custom_objects)\n",
        "  # # # Check its architecture\n",
        "  new_model.summary()  "
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2fUNm7uPWSm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "03bc608b-5df5-462f-ab3d-a055b7a78165"
      },
      "source": [
        "load_mrc_model('lstm-baseline')\n",
        "\n",
        "# list_of_models['bilstm-glove-q2c-c2q-attention']['loc'].replace('https://storage.cloud.google.com/aiml-capstone/','gbucket/')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gbucket/lstmbaseline-0/context_withoutstopwords_model_epoch_24.h5\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "QUESTION_INPUT (InputLayer)     [(None, 40)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_EMBEDDING (Embedding)  (None, 40, 100)      10085100    QUESTION_INPUT[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_INPUT (InputLayer)      [(None, 426)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_LSTM (LSTM)            (None, 256)          365568      QUESTION_EMBEDDING[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_EMBEDDING (Embedding)   (None, 426, 100)     10085100    CONTEXT_INPUT[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "BILINEAR_AS_SPAN (Dense)        (None, 256)          65792       QUESTION_LSTM[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "BILINEAR_AE_SPAN (Dense)        (None, 256)          65792       QUESTION_LSTM[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_LSTM (LSTM)             (None, 426, 256)     365568      CONTEXT_EMBEDDING[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BILINEAR_AS_ADD_DIM (None, 256, 1)       0           BILINEAR_AS_SPAN[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BILINEAR_AE_ADD_DIM (None, 256, 1)       0           BILINEAR_AE_SPAN[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2 (Tens (None, 426, 1)       0           CONTEXT_LSTM[0][0]               \n",
            "                                                                 tf_op_layer_BILINEAR_AS_ADD_DIM[0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_1 (Te (None, 426, 1)       0           CONTEXT_LSTM[0][0]               \n",
            "                                                                 tf_op_layer_BILINEAR_AE_ADD_DIM[0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BILINEAR_AS_DEL_DIM (None, 426)          0           tf_op_layer_BatchMatMulV2[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BILINEAR_AE_DEL_DIM (None, 426)          0           tf_op_layer_BatchMatMulV2_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BILINEAR_AS_SOFTMAX (None, 426)          0           tf_op_layer_BILINEAR_AS_DEL_DIM[0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BILINEAR_AE_SOFTMAX (None, 426)          0           tf_op_layer_BILINEAR_AE_DEL_DIM[0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BILINEAR_AS_AE_CONC (None, 852)          0           tf_op_layer_BILINEAR_AS_SOFTMAX[0\n",
            "                                                                 tf_op_layer_BILINEAR_AE_SOFTMAX[0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_16 (T (None, 426)          0           tf_op_layer_BILINEAR_AS_AE_CONCAT\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_17 (T (None, 426)          0           tf_op_layer_BILINEAR_AS_AE_CONCAT\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_START_PROBAB_8 (Ten (None, 426)          0           tf_op_layer_strided_slice_16[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_END_PROBAB_8 (Tenso (None, 426)          0           tf_op_layer_strided_slice_17[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_PREDICT_AS_PROBAB_1 (None, 426, 1)       0           tf_op_layer_START_PROBAB_8[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_PREDICT_AS_PROBAB_1 (None, 1, 426)       0           tf_op_layer_END_PROBAB_8[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_10 (T (None, 426, 426)     0           tf_op_layer_PREDICT_AS_PROBAB_16[\n",
            "                                                                 tf_op_layer_PREDICT_AS_PROBAB_17[\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_PREDICT_AS_AE_TOPTR (None, 426, 426)     0           tf_op_layer_BatchMatMulV2_10[0][0\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_PREDICT_AS_MAX_7 (T (None, 426)          0           tf_op_layer_PREDICT_AS_AE_TOPTRIA\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_PREDICT_AE_MAX_7 (T (None, 426)          0           tf_op_layer_PREDICT_AS_AE_TOPTRIA\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_PREDICT_AS_AE_7 (Te (None, 852)          0           tf_op_layer_PREDICT_AS_MAX_7[0][0\n",
            "                                                                 tf_op_layer_PREDICT_AE_MAX_7[0][0\n",
            "==================================================================================================\n",
            "Total params: 21,032,920\n",
            "Trainable params: 21,032,920\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DURR8zGyUpy6",
        "colab_type": "text"
      },
      "source": [
        "# Load Existing Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWgeXhRLUrj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rregOZyRS_Da",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}