{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mrc-SVM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanbasu/aiml-capstone/blob/master/mrc_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CszkC2N6tQ6I",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries and setting up Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keG2-7eI4i0z",
        "colab_type": "code",
        "outputId": "d1bbffc7-2222-464e-e218-871d0afad53e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "import unicodedata\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english')) \n",
        "import pickle\n",
        "# we will store the params as we go along in this object\n",
        "params_svm = {}\n",
        "project_path = \"/content/drive/My Drive/AIML-MRC-Capstone/datasets/Squad2.0/TrainingDataset/\"\n",
        "model_path = \"/content/drive/My Drive/AIML-MRC-Capstone/models/\"\n",
        "tensorboard_logpath  = \"/content/drive/My Drive/AIML-MRC-Capstone/models/tensorboard-logs/\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYm1jjuX4shJ",
        "colab_type": "code",
        "outputId": "52cb67d2-eaea-400a-9979-40b6c71703bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI91De3Etmxe",
        "colab_type": "text"
      },
      "source": [
        "# Objective - SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANaQH_dUt_HO",
        "colab_type": "text"
      },
      "source": [
        "> Model 1\n",
        "  1.   Inputs: a context paragraph p = {p1, ..., pP } of length P\n",
        "  2.   Output: A question q = {q1, ..., qQ} of length Q \n",
        "\n",
        "> Model 2\n",
        "  1.   Inputs: A question q = {q1, ..., qQ} of length Q \n",
        "  2.   Output: A answer\n",
        "\n",
        "**Model1.predict(new context) --> new question<br>\n",
        "Model2.predict(new question) --> new answer**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBcMLgqjtv7K",
        "colab_type": "text"
      },
      "source": [
        "## 1 Common Functions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glzsn-Eql2l9",
        "colab_type": "text"
      },
      "source": [
        "#### 1.1 Custom function for preprocessing of context and question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZr1Phqp375Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove unwanted chars\n",
        "# convert to lowercase\n",
        "# remove unwanted spaces\n",
        "# remove stop words\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "## reference \n",
        "def decontracted(phrase):\n",
        "    \"\"\"\n",
        "    This function remooves punctuation from given sentence.\n",
        "    \"\"\"\n",
        "\n",
        "    if(phrase is np.nan):\n",
        "      return 'impossible'      \n",
        "\n",
        "    try:      \n",
        "      # specific\n",
        "      phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "      phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "      # general\n",
        "      phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "      phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "      phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "      phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "      phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "      phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "      phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "      phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "      \n",
        "      # string operation\n",
        "      phrase = phrase.replace('\\\\r', ' ')\n",
        "      phrase = phrase.replace('\\\\\"', ' ')\n",
        "      phrase = phrase.replace('\\\\n', ' ')\n",
        "\n",
        "      phrase = re.sub('[^A-Za-z0-9]+', ' ', phrase.lower())\n",
        "    except:\n",
        "      print(phrase)  \n",
        "    \n",
        "    return phrase\n",
        "\n",
        "def preprocess_text(corpus, text_lower_case=True, \n",
        "                      special_char_removal=True, stopword_removal=True, remove_digits=False):    \n",
        "    normalized_text = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # doc = decontracted(doc)\n",
        "        # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        # remove special characters and\\or digits    \n",
        "        if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = remove_special_characters(doc, remove_digits=remove_digits) \n",
        "\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc)\n",
        "\n",
        "        normalized_text.append(doc)\n",
        "        \n",
        "    return normalized_text\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    #Using regex\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text):  \n",
        "    word_tokens = word_tokenize(text) \n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]   \n",
        "    filtered_sentence = [] \n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(w)                 \n",
        "    return ' '.join(filtered_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7xgKEyZNioh",
        "colab_type": "text"
      },
      "source": [
        "## 2 Load Squad Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWoAQvv2N2re",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Load "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSSJZdwC4EpI",
        "colab_type": "code",
        "outputId": "34d27a38-e5eb-42d6-86ba-611103c83d52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "squad_df = pd.read_csv(project_path+'squad_data_final.csv')\n",
        "squad_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "squad_df.tail(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer</th>\n",
              "      <th>plausible_answer_start</th>\n",
              "      <th>plausible_answer</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>clean_context</th>\n",
              "      <th>clean_question</th>\n",
              "      <th>clean_answer</th>\n",
              "      <th>answer_len</th>\n",
              "      <th>answer_end</th>\n",
              "      <th>answer_span</th>\n",
              "      <th>answer_word_span</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>130301</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>Physics has broadly agreed on the definition o...</td>\n",
              "      <td>5a7e070b70df9f001a875439</td>\n",
              "      <td>-1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>485.0</td>\n",
              "      <td>matter</td>\n",
              "      <td>True</td>\n",
              "      <td>the term matter is used throughout physics in ...</td>\n",
              "      <td>physics has broadly agreed on the definition o...</td>\n",
              "      <td>impossible</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>(-1, -1)</td>\n",
              "      <td>(-1, -1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130302</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>Who coined the term partonic matter?</td>\n",
              "      <td>5a7e070b70df9f001a87543a</td>\n",
              "      <td>-1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>327.0</td>\n",
              "      <td>Alfvén</td>\n",
              "      <td>True</td>\n",
              "      <td>the term matter is used throughout physics in ...</td>\n",
              "      <td>who coined the term partonic matter</td>\n",
              "      <td>impossible</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>(-1, -1)</td>\n",
              "      <td>(-1, -1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130303</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>What is another name for anti-matter?</td>\n",
              "      <td>5a7e070b70df9f001a87543b</td>\n",
              "      <td>-1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>350.0</td>\n",
              "      <td>Gk. common matter</td>\n",
              "      <td>True</td>\n",
              "      <td>the term matter is used throughout physics in ...</td>\n",
              "      <td>what is another name for anti matter</td>\n",
              "      <td>impossible</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>(-1, -1)</td>\n",
              "      <td>(-1, -1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130304</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>Matter usually does not need to be used in con...</td>\n",
              "      <td>5a7e070b70df9f001a87543c</td>\n",
              "      <td>-1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>529.0</td>\n",
              "      <td>a specifying modifier</td>\n",
              "      <td>True</td>\n",
              "      <td>the term matter is used throughout physics in ...</td>\n",
              "      <td>matter usually does not need to be used in con...</td>\n",
              "      <td>impossible</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>(-1, -1)</td>\n",
              "      <td>(-1, -1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130305</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>What field of study has a variety of unusual c...</td>\n",
              "      <td>5a7e070b70df9f001a87543d</td>\n",
              "      <td>-1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>37.0</td>\n",
              "      <td>physics</td>\n",
              "      <td>True</td>\n",
              "      <td>the term matter is used throughout physics in ...</td>\n",
              "      <td>what field of study has a variety of unusual c...</td>\n",
              "      <td>impossible</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>(-1, -1)</td>\n",
              "      <td>(-1, -1)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         title  ... answer_word_span\n",
              "130301  Matter  ...         (-1, -1)\n",
              "130302  Matter  ...         (-1, -1)\n",
              "130303  Matter  ...         (-1, -1)\n",
              "130304  Matter  ...         (-1, -1)\n",
              "130305  Matter  ...         (-1, -1)\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QaNyZQKN_nJ",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Create Train, Validation and Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9loHnO_EY0g6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "squad_df_cleaned = squad_df.head(13000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6c9GoDw4h7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xcontext_train, Xcontext_test, yquestion_train, yquestion_test = train_test_split(squad_df_cleaned['clean_context'], squad_df_cleaned['clean_question'], test_size=0.33, random_state=42)\n",
        "Xquestion_train, Xquestion_test, yanswer_train, yanswer_test = train_test_split(squad_df_cleaned['clean_question'], squad_df_cleaned['clean_answer'], test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBqhbVCnVFsm",
        "colab_type": "code",
        "outputId": "d8a4a9c7-9312-4d74-eae6-1b006b9fb420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(Xcontext_train.shape)\n",
        "print(Xcontext_test.shape)\n",
        "print(yquestion_train.shape)\n",
        "print(yquestion_test.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8710,)\n",
            "(4290,)\n",
            "(8710,)\n",
            "(4290,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFQGozh2OJTm",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Build Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsIgoMwmVHiN",
        "colab_type": "code",
        "outputId": "41d278c1-2d45-43e3-8f46-0a380911f1ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Define TF-ID Venctorize and Label encoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "xvectorizer = TfidfVectorizer(max_features=5000)\n",
        "xvectorizer.fit(squad_df_cleaned['clean_context'] + \" \" + squad_df_cleaned['clean_question'])\n",
        "ylblencoder = LabelEncoder()\n",
        "# # generate label encoder for both question and clean answer\n",
        "ylblencoder.fit(squad_df_cleaned['clean_question'].append(squad_df_cleaned['clean_answer']).reset_index(drop=True).astype(str))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b-Il8mIVNzt",
        "colab_type": "code",
        "outputId": "6618d644-6a5e-40a9-9fb1-5346d1bf2073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(ylblencoder.classes_))\n",
        "len(xvectorizer.vocabulary_)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xba7IrlsOe7Q",
        "colab_type": "text"
      },
      "source": [
        "## 3 Vectorization / Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zjim_mduPT9T",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Context, Question, Answer vectorized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm_kiq2eW8tF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xcontext_vectorized = xvectorizer.transform(Xcontext_train)\n",
        "yquestion_vectorized = ylblencoder.transform(yquestion_train)\n",
        "\n",
        "Xquestion_vectorized = xvectorizer.transform(Xquestion_train)\n",
        "yanswer_vectorized = ylblencoder.transform(yanswer_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQDCJXueNuyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xcontext_test_vectorized= xvectorizer.transform(Xcontext_test)\n",
        "yquestion_test_vectorized= xvectorizer.transform(yquestion_test)\n",
        "\n",
        "Xquestion_test_vectorized = xvectorizer.transform(Xquestion_test)\n",
        "yanswer_test_vectorized = ylblencoder.transform(yanswer_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4VRh9OuPfHd",
        "colab_type": "text"
      },
      "source": [
        "### 3.2 Check 1 Value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIBQBERdPk-s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7881a38a-a71b-4be3-947f-6934adf5df6c"
      },
      "source": [
        "# should be a sparse matrix \n",
        "Xcontext_vectorized[0]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 23 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9H01nnNPv4K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "24ffff59-76b9-49df-ecbf-db616a49dee5"
      },
      "source": [
        "yquestion_vectorized[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42016"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpK9h6F1TnGy",
        "colab_type": "text"
      },
      "source": [
        "### 3.3 Persist Models, Tokenizers and Encoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8Q6v6nSTmj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(model_path + \"svm_xvectorizer.pkl\",\"wb\") as f:\n",
        "    pickle.dump(xvectorizer,f)\n",
        "\n",
        "with open(model_path + \"svm_ylblencoder.pkl\",\"wb\") as f:\n",
        "    pickle.dump(ylblencoder,f)    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBZkowPXPjjq",
        "colab_type": "text"
      },
      "source": [
        "## 4 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kf64oZTP5Ky",
        "colab_type": "text"
      },
      "source": [
        "### 4.1 Building Context - Question Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OX39CrYQE5_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6255f205-16b0-4b20-d22e-2be7e58fdc45"
      },
      "source": [
        "from sklearn import svm\n",
        "\n",
        "# SVM model 1\n",
        "try:\n",
        "  context2question = svm.SVC()    \n",
        "  context2question.fit(Xcontext_vectorized,yquestion_vectorized)\n",
        "except RuntimeError as e:\n",
        "  print(e)\n",
        "\n",
        "print('Model context2question', context2question)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model context2question SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7r3-nXdVV6q",
        "colab_type": "text"
      },
      "source": [
        "### 4.2 Persis Context - Question Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J5-uWrTVaqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(model_path + \"svm_context2question.pkl\",\"wb\") as f:\n",
        "    pickle.dump(context2question,f) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WirmE-j6PRra",
        "colab_type": "text"
      },
      "source": [
        "### 4.3 Building Question - Answer Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGiLwrxPNUYO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "fa1928ae-f30b-4b47-ea82-4f94ecb0ecd5"
      },
      "source": [
        "try: \n",
        "  question2answer = svm.SVC(gamma=0.025, C=10)    \n",
        "  question2answer.fit(Xquestion_vectorized,yanswer_vectorized)\n",
        "except RuntimeError as e:\n",
        "  print(e)\n",
        "\n",
        "print('Model question2answer', question2answer)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model question2answer SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma=0.025, kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da97OxftVkp9",
        "colab_type": "text"
      },
      "source": [
        "### 4.4 Persist Question - Answer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uFWEuMTVk6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(model_path + \"svm_question2answer.pkl\",\"wb\") as f:\n",
        "    pickle.dump(question2answer,f) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQpFsdBcUVGg",
        "colab_type": "text"
      },
      "source": [
        "### 4.4 Load Existing Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czNivcKoUYLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context2question = pickle.load(open(model_path + \"svm_context2question.pkl\", 'rb'))\n",
        "question2answer = pickle.load(open(model_path + \"svm_question2answer.pkl\", 'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5MdbbWGUqDM",
        "colab_type": "text"
      },
      "source": [
        "### 4.5 Accuracy Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6tz65GVXt_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_acc = context2question.score(Xcontext_vectorized,yquestion_vectorized)\n",
        "# test_acc = context2question.score(Xcontext_vectorized,yquestion_vectorized)\n",
        "\n",
        "print('Train Accuracy', train_acc)\n",
        "# print('Test Accuracy', test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE4StxNSUXIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}