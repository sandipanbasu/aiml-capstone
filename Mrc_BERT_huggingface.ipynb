{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mrc_BERT_huggingface",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanbasu/aiml-capstone/blob/master/Mrc_BERT_huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z3MR1Wml5Rn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "e099b9c8-9098-4d3a-f5f8-67814202a9da"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n",
            "\u001b[K     |████████████████████████████████| 757kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting tokenizers==0.8.0-rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 15.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 36.3MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 27.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=a02619cf3a529da7e433d475c54e4e57f0f0254d4fdb0a8663382419431f3266\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.0rc4 transformers-3.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4AUsThdmC7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertTokenizer, TFBertForQuestionAnswering\n",
        "\n",
        "berthugtokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "berthugmodel = TFBertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_ddY9geXLDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "berthugmodel.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7n7NUmdqPYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predBERTHuggingface(question, context):  \n",
        "  input_dict = berthugtokenizer(question, context, return_tensors='tf')\n",
        "  start_scores, end_scores = berthugmodel(input_dict)\n",
        "  all_tokens = berthugtokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"].numpy()[0])\n",
        "  # print(all_tokens)\n",
        "  answer = ' '.join(all_tokens[tf.math.argmax(start_scores, 1)[0] : tf.math.argmax(end_scores, 1)[0]+1])\n",
        "  # print(answer)\n",
        "  # Find the tokens with the highest `start` and `end` scores.\n",
        "  answer_start = np.argmax(start_scores)\n",
        "  answer_end = np.argmax(end_scores)\n",
        "  # Combine the tokens in the answer and print it out.\n",
        "  answer = ' '.join(all_tokens[answer_start:answer_end+1])\n",
        "  # Start with the first token.\n",
        "  answer = all_tokens[answer_start]\n",
        "\n",
        "  # Select the remaining answer tokens and join them with whitespace.\n",
        "  for i in range(answer_start + 1, answer_end + 1):      \n",
        "      # If it's a subword token, then recombine it with the previous token.\n",
        "      if all_tokens[i][0:2] == '##':\n",
        "          answer += all_tokens[i][2:]    \n",
        "      # Otherwise, add a space then the token.\n",
        "      else:\n",
        "          answer += ' ' + all_tokens[i]\n",
        "\n",
        "  return answer "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDeu1986pAYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=80) \n",
        "# print(wrapper.fill(cnndf['context'].iloc[0]))\n",
        "q1, c1  = \"what was the death toll\", \"With a record 15968 coronavirus cases reported in the past 24 hours,the total count in India crossed the 4.5 lakh mark. The death toll has gone up to 14476\"\n",
        "q2, c2 = 'how many cases in mumbai', 'Delhi, which already is the second worst hit state in terms of coronavirus caseload and fatalities, overtook Mumbai after the number of cases soared by 3788 to touch 70390. Mumbai has so far recorded 69625 cases, according to official figures.'\n",
        "\n",
        "# input_dict = berthugtokenizer(text=[q1,q2],text_pair =[c1,c2],is_pretokenized=True, return_tensors='tf')\n",
        "# print(input_dict)\n",
        "\n",
        "# input_dict_noar = berthugtokenizer(q2,c2, return_tensors='tf')\n",
        "# print(input_dict_noar)\n",
        "\n",
        "\n",
        "# all_tokens = berthugtokenizer.convert_ids_to_tokens(input_dict[\"input_ids\"][0])\n",
        "# print(all_tokens)\n",
        "\n",
        "# all_tokens = berthugtokenizer.convert_ids_to_tokens(input_dict_noar[\"input_ids\"][0])\n",
        "# print(all_tokens)\n",
        "\n",
        "print(predBERTHuggingface(q1,c1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOcNU8iwXWv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bert_pred=pd.DataFrame(columns=['c','q','true','pred'])\n",
        "test = load_test_data(name='test-withstopwordspunct.csv')\n",
        "bert_h_pred = test[['id','context','question','answer','plausible_answer']].copy()\n",
        "preds = []\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "# a better way is to batch its and tokenize \n",
        "# for a batch a record at a time and do inference \n",
        "# I was able to do for deeppavlov, for hugginface I felt lazy\n",
        "with strategy.scope():\n",
        "  for i in tqdm(range(test.shape[0])):\n",
        "    try:\n",
        "      p = ''\n",
        "      p = predBERTHuggingface(test['question'].iloc[i],test['context'].iloc[i])\n",
        "    except:\n",
        "      print('error in prediction')\n",
        "    preds = preds + [p]\n",
        "\n",
        "bert_h_pred['prediction'] = preds\n",
        "\n",
        "bert_h_pred.head()\n",
        "# remove na pls\n",
        "bert_h_pred.loc[bert_h_pred['prediction'].isna(), 'prediction'] = ''\n",
        "bert_h_pred.to_csv(model_path + 'bert/bert-huggingface-results.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}