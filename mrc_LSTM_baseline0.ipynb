{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mrc-LSTM-baseline0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanbasu/aiml-capstone/blob/master/mrc_LSTM_baseline0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "879mz4G6-lRH",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import Libraries, setting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA8sHlEbfYnn",
        "colab_type": "code",
        "outputId": "9278c8d7-62c8-4488-8da3-e837d737624b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_po1L9T-V5d",
        "colab_type": "code",
        "outputId": "c9a4e652-9900-4d30-f488-f7ee9c507915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTDFtsEtV7dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pprint\n",
        "from tensorflow.keras.layers import Bidirectional,LSTM,Dense,Dropout,BatchNormalization,Flatten,Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from numpy import array\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPD8_fgd8PhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will store the params as we go along in this object\n",
        "params = {}\n",
        "project_path = \"/content/drive/My Drive/AIML-MRC-Capstone/datasets/Squad2.0/TrainingDataset/\"\n",
        "model_path = \"/content/drive/My Drive/AIML-MRC-Capstone/models/\"\n",
        "tensorboard_logpath  = \"/content/drive/My Drive/AIML-MRC-Capstone/models/tensorboard-logs/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09eGk3oWWCaP",
        "colab_type": "text"
      },
      "source": [
        "# Objective - LSTM Baseline 0 \n",
        "\n",
        "*   **Inputs: A question q = {q1, ..., qQ} of length Q and a context paragraph p = {p1, ..., pP } of length P.**\n",
        "*   **Output: An answer span {as, ae} where as is the index of the first answer token in p, ae is the index of the last answer token in p, 0 <= as, ae >= m, and ae >= as.** \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGZp4iLEHE8n",
        "colab_type": "text"
      },
      "source": [
        "## 2 Load Squad Data - Cleaned and curated (output of preprocessing step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFPpoxl7Afls",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_u1n8G3fge_",
        "colab_type": "code",
        "outputId": "86b8f487-1965-4f26-8be6-92a90539b707",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "squad_df = pd.read_csv(project_path+'squad_data_final.csv')\n",
        "squad_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "squad_df.head(2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer</th>\n",
              "      <th>plausible_answer_start</th>\n",
              "      <th>plausible_answer</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>clean_context</th>\n",
              "      <th>clean_question</th>\n",
              "      <th>clean_answer</th>\n",
              "      <th>answer_len</th>\n",
              "      <th>answer_end</th>\n",
              "      <th>answer_span</th>\n",
              "      <th>answer_word_span</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>269</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>beyonc giselle knowlescarter bijnse beeyonsay ...</td>\n",
              "      <td>when did beyonce start becoming popular</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>17</td>\n",
              "      <td>286</td>\n",
              "      <td>(269, 286)</td>\n",
              "      <td>(-1, -1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>207</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>beyonc giselle knowlescarter bijnse beeyonsay ...</td>\n",
              "      <td>what areas did beyonce compete in when she was...</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>19</td>\n",
              "      <td>226</td>\n",
              "      <td>(207, 226)</td>\n",
              "      <td>(21, 23)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     title  ... answer_word_span\n",
              "0  Beyoncé  ...         (-1, -1)\n",
              "1  Beyoncé  ...         (21, 23)\n",
              "\n",
              "[2 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U35P3ZvGAQQD",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Create Train, Validation and Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M9hW4xy-Suj",
        "colab_type": "code",
        "outputId": "6b4212a1-13de-486c-eca2-8ba87417736b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample,shuffle\n",
        "\n",
        "# train = resample(train)\n",
        "# train = shuffle(train,n_samples =50000)\n",
        "\n",
        "train,test = train_test_split(squad_df,test_size = 0.2)\n",
        "train,val = train_test_split(train,test_size=0.25)\n",
        "\n",
        "print(train.shape)\n",
        "print(val.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(78183, 16)\n",
            "(26061, 16)\n",
            "(26062, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sAKjoZ7CIJ_",
        "colab_type": "code",
        "outputId": "35e2cb5b-010a-4250-9173-a9d677c99360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "train[\"answer_word_span\"] = train[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "test[\"answer_word_span\"] = test[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "val[\"answer_word_span\"] = val[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "train.info()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 78183 entries, 65753 to 63112\n",
            "Data columns (total 16 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   title                   78183 non-null  object \n",
            " 1   context                 78183 non-null  object \n",
            " 2   question                78183 non-null  object \n",
            " 3   id                      78183 non-null  object \n",
            " 4   answer_start            78183 non-null  int64  \n",
            " 5   answer                  51966 non-null  object \n",
            " 6   plausible_answer_start  26217 non-null  float64\n",
            " 7   plausible_answer        26217 non-null  object \n",
            " 8   is_impossible           78183 non-null  bool   \n",
            " 9   clean_context           78183 non-null  object \n",
            " 10  clean_question          78183 non-null  object \n",
            " 11  clean_answer            78183 non-null  object \n",
            " 12  answer_len              78183 non-null  int64  \n",
            " 13  answer_end              78183 non-null  int64  \n",
            " 14  answer_span             78183 non-null  object \n",
            " 15  answer_word_span        78183 non-null  object \n",
            "dtypes: bool(1), float64(1), int64(3), object(11)\n",
            "memory usage: 9.6+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6dcPCGiJrz1",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Build Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxoMIrC6EIgS",
        "colab_type": "code",
        "outputId": "e6ef3944-cf5c-4047-84a5-3bc6c9c152f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "params['tokenizer_num_words'] = 80000\n",
        "tokenizer = preprocessing.text.Tokenizer(num_words=params['tokenizer_num_words'])\n",
        "\n",
        "# NOTE: tokenizer is been made out of original dataset\n",
        "for text in tqdm([squad_df['clean_context'], squad_df['clean_question']]):  \n",
        "  tokenizer.fit_on_texts(text.values)\n",
        "\n",
        "# total tokenizer words\n",
        "params['vocab_size'] = len(tokenizer.word_index)\n",
        "\n",
        "### SAVE TOKENIZERS\n",
        "with open(model_path + \"tokenizer.pkl\",\"wb\") as f:\n",
        "    pickle.dump(tokenizer,f)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:08<00:00,  4.39s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7fIbj3tatHH",
        "colab_type": "code",
        "outputId": "7f4d96e7-15bf-4ca3-8af7-98e8782d0de2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.word_index['how']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gXSy_y36IFb",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 Update parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2whUHcmX5PwS",
        "colab_type": "code",
        "outputId": "38d5540b-658a-4a3b-b24b-2692833a4b8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# From the EDA and historgrams we can conclude that - \n",
        "# 99% percentile of context word length = 285\n",
        "# 99% percentile or question word lengt = 20\n",
        "context_length = 285\n",
        "question_length = 20\n",
        "params['train_shape'] = train.shape\n",
        "params['val_shape'] = val.shape\n",
        "params['test_shape'] = test.shape\n",
        "params['context_length_99'] = context_length # initialize with a high percentile\n",
        "params['question_length_99'] = question_length # initialize with a high percentile\n",
        "params['embedding_size'] = 100\n",
        "params['rnn_units'] = 256\n",
        "params['context_pad_seq'] = 'pre'\n",
        "params['question_pad_seq'] = 'pre'\n",
        "\n",
        "pprint.pprint(params)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'val_shape': (26061, 16),\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lRuVtp_7y51",
        "colab_type": "text"
      },
      "source": [
        "## 3 Vectorization / Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEJqzOfW9ARO",
        "colab_type": "text"
      },
      "source": [
        "#### 3.1 Integer Sequence of Context and Question "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhuvjYmZ7m6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_clean_context_sequence = tokenizer.texts_to_sequences(train[\"clean_context\"].values)\n",
        "test_clean_context_sequence = tokenizer.texts_to_sequences(test[\"clean_context\"].values)\n",
        "val_clean_context_sequence = tokenizer.texts_to_sequences(val[\"clean_context\"].values)\n",
        "\n",
        "\n",
        "train_clean_question_sequence = tokenizer.texts_to_sequences(train[\"clean_question\"].values)\n",
        "test_clean_question_sequence = tokenizer.texts_to_sequences(test[\"clean_question\"].values)\n",
        "val_clean_question_sequence = tokenizer.texts_to_sequences(val[\"clean_question\"].values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXQNaAxehrDa",
        "colab_type": "code",
        "outputId": "83034390-1ce8-478d-8b21-e6fe86c368fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "train_clean_question_sequence[5:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[71, 11, 33390, 6, 2134],\n",
              " [2, 1743, 1445, 263, 6388, 3343, 288],\n",
              " [2, 29, 16, 2728, 766, 147, 336, 59, 2436],\n",
              " [2, 14, 933, 41, 71, 1, 2870, 98, 1202, 98, 1875, 5123],\n",
              " [39, 9, 2617, 140, 26, 989, 563, 288]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kmRjkqRlExD",
        "colab_type": "code",
        "outputId": "b1a1bb85-a2b0-425d-b95e-04e4c408f262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "train['clean_question'][5:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72672                         when was mdrtb first observed\n",
              "129071    what electronic charge do cellular molecules have\n",
              "106489    what year did cd players become available for ...\n",
              "76709     what is it called when the variations are text...\n",
              "90219       how many courses does a mandolin commonly have \n",
              "Name: clean_question, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQpM6dGtoGkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u41PKZTFcwm",
        "colab_type": "text"
      },
      "source": [
        "#### 3.2 Find Max Sequence length of Context and Question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwh8HGs0Fbp2",
        "colab_type": "code",
        "outputId": "aa277d5a-e633-4961-c9d7-a8ef43029e5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# max length of context\n",
        "params['context_max_length'] = max(max(len(txt) for txt in train_clean_context_sequence),\n",
        "                                  max(len(txt) for txt in test_clean_context_sequence),\n",
        "                                  max(len(txt) for txt in val_clean_context_sequence))\n",
        "\n",
        "params['question_max_length'] = max(max(len(txt) for txt in train_clean_question_sequence),\n",
        "                                  max(len(txt) for txt in test_clean_question_sequence),\n",
        "                                  max(len(txt) for txt in val_clean_question_sequence))\n",
        "\n",
        "\n",
        "pprint.pprint(params)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'val_shape': (26061, 16),\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhnBwThFIA0H",
        "colab_type": "text"
      },
      "source": [
        "#### 3.3 Padding of the sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGvIf7xU9rIJ",
        "colab_type": "code",
        "outputId": "7460f40e-6731-46d4-ef94-db206567386b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_context_sequence = preprocessing.sequence.pad_sequences(train_clean_context_sequence,maxlen=params['context_max_length'])\n",
        "test_context_sequence = preprocessing.sequence.pad_sequences(test_clean_context_sequence,maxlen=params['context_max_length'])\n",
        "val_context_sequence = preprocessing.sequence.pad_sequences(val_clean_context_sequence,maxlen=params['context_max_length'])\n",
        "\n",
        "print(train_context_sequence.shape)\n",
        "print(test_context_sequence.shape)\n",
        "print(val_context_sequence.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(78183, 426)\n",
            "(26062, 426)\n",
            "(26061, 426)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NxpIGIb9p5T",
        "colab_type": "code",
        "outputId": "33c2f151-5981-4355-d6a8-245647f10e12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_question_sequence = preprocessing.sequence.pad_sequences(train_clean_question_sequence,maxlen=params['question_max_length'])\n",
        "test_question_sequence = preprocessing.sequence.pad_sequences(test_clean_question_sequence,maxlen=params['question_max_length'])\n",
        "val_question_sequence = preprocessing.sequence.pad_sequences(val_clean_question_sequence,maxlen=params['question_max_length'])\n",
        "\n",
        "print(train_question_sequence.shape)\n",
        "print(test_question_sequence.shape)\n",
        "print(val_question_sequence.shape)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(78183, 40)\n",
            "(26062, 40)\n",
            "(26061, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgCKNbH6_-yd",
        "colab_type": "text"
      },
      "source": [
        "#### 3.4 Create Answer Sequence "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv0yC_w8AF4x",
        "colab_type": "text"
      },
      "source": [
        "Encode y_trues as big array consisting of ans_start + ans_end. This has to be used in loss function as well. We will use the answer_word_span feature\n",
        "\n",
        "**y_true = answer_start + answer_end**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHPxRRHuAnSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for train data\n",
        "y_train = []\n",
        "span_ofr = 0;\n",
        "params['train_span_outofrange'] = 0\n",
        "params['test_span_outofrange'] = 0\n",
        "params['val_span_outofrange'] = 0\n",
        "\n",
        "for i in range(len(train)):    \n",
        "    s = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    e = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    start, end = train[\"answer_word_span\"].iloc[i]    \n",
        "    s[start] = 1\n",
        "    e[end] = 1\n",
        "    y_train.append(np.concatenate((s,e)))    \n",
        "\n",
        "params['train_span_outofrange'] = span_ofr\n",
        "span_ofr = 0;\n",
        "\n",
        "# for test data\n",
        "y_test = []\n",
        "for i in range(len(test)):    \n",
        "    s = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    e = np.zeros(params['context_max_length'],dtype = \"int\")        \n",
        "    start,end = test[\"answer_word_span\"].iloc[i]    \n",
        "    s[start] = 1\n",
        "    e[end] = 1\n",
        "    y_test.append(np.concatenate((s,e)))\n",
        "\n",
        "params['test_span_outofrange'] = span_ofr\n",
        "span_ofr = 0;\n",
        "                \n",
        "# for val data\n",
        "y_val = []\n",
        "for i in range(len(val)):\n",
        "    s = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    e = np.zeros(params['context_max_length'],dtype = \"int\")        \n",
        "    start,end = val[\"answer_word_span\"].iloc[i]    \n",
        "    s[start] = 1\n",
        "    e[end] = 1      \n",
        "    y_val.append(np.concatenate((s,e)))\n",
        "\n",
        "params['val_span_outofrange'] = span_ofr    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgPgnMl0VZCd",
        "colab_type": "code",
        "outputId": "b54a8e7b-de34-42db-9f2b-872af9d3e586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(len(y_train),len(y_train[0]))\n",
        "print(len(y_test),len(y_test[0]))\n",
        "print(len(y_val),len(y_val[0]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78183 852\n",
            "26062 852\n",
            "26061 852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbM8z2AEjxKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(sentence):\n",
        "    \"\"\"\n",
        "    Returns tokenised words.\n",
        "    \"\"\"\n",
        "    return nltk.word_tokenize(sentence)\n",
        "\n",
        "def answer_span(context,ans):\n",
        "    \"\"\"\n",
        "    This funtion returns anwer span start index and end index.\n",
        "    \"\"\"\n",
        "    ans_token = tokenize(ans)\n",
        "    print(ans_token)\n",
        "    con_token = tokenize(context)\n",
        "    print(con_token)\n",
        "    print('21 ',con_token[21])\n",
        "    print('22 ',con_token[22])\n",
        "    print('23 ',con_token[23])\n",
        "    print('24 ',con_token[24])\n",
        "    ans_len = len(ans_token)\n",
        "    \n",
        "    if ans_len!=0 and ans_token[0] in con_token:\n",
        "    \n",
        "        indices = [i for i, x in enumerate(con_token) if x == ans_token[0]]\n",
        "        print(indices)\n",
        "        try:\n",
        "\n",
        "            if(len(indices)>1):\n",
        "                start = [i for i in indices if (con_token[i:i+ans_len] == ans_token) ]\n",
        "                end = start[0] + ans_len - 1\n",
        "                return start[0],end\n",
        "\n",
        "            else:\n",
        "                start = con_token.index(ans_token[0])\n",
        "                end = start + ans_len - 1\n",
        "                return start,end\n",
        "        except:\n",
        "            return -1,-1\n",
        "    else:\n",
        "        return -1,-1\n",
        "\n",
        "def span_to_answer():\n",
        "  return 0;        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFuJMid-iT7a",
        "colab_type": "text"
      },
      "source": [
        "### 3.5 Check 1 value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXXhftC5kE8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "a988cd1d-da2d-4e6c-d2dc-4a44bd09429e"
      },
      "source": [
        "index = 2\n",
        "answer_span(train['clean_context'].iloc[index],train['clean_answer'].iloc[index])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['eastern', 'mediterranean', 'sea']\n",
            "['cyprus', 'isaprs', 'greek', 'ipa', '[', 'cipros', ']', 'turkish', 'kbrs', 'ipa', '[', 'kbs', ']', 'officially', 'republic', 'cyprus', 'greek', 'turkish', 'kbrs', 'cumhuriyeti', 'island', 'country', 'eastern', 'mediterranean', 'sea', 'coasts', 'syria', 'turkey', '[', 'e', ']', 'cyprus', 'third', 'largest', 'third', 'populous', 'island', 'mediterranean', 'member', 'state', 'european', 'union', 'located', 'south', 'turkey', 'west', 'syria', 'lebanon', 'northwest', 'israel', 'palestine', 'north', 'egypt', 'east', 'greece']\n",
            "21  country\n",
            "22  eastern\n",
            "23  mediterranean\n",
            "24  sea\n",
            "[22]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22, 24)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAXS9YCpiVkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "4b1f24fe-928a-4b44-abe7-45442585c59a"
      },
      "source": [
        "print(\"Ori Cont = \")\n",
        "pprint.pprint(train['context'].iloc[index])\n",
        "print(\"CLean Cont = \")\n",
        "pprint.pprint(train['clean_context'].iloc[index])\n",
        "print('Question = ',train['question'].iloc[index])\n",
        "print('Clean Question = ',train['clean_question'].iloc[index])\n",
        "print('Answer = ',train['answer'].iloc[index])\n",
        "print('Clean Answer = ',train['clean_answer'].iloc[index])\n",
        "print('AS,AE = ',train['answer_word_span'].iloc[index])\n",
        "print(\"encoded \", y_train[index])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ori Cont = \n",
            "('Cyprus (i/ˈsaɪprəs/; Greek: Κύπρος IPA: [ˈcipros]; Turkish: Kıbrıs IPA: '\n",
            " '[ˈkɯbɾɯs]), officially the Republic of Cyprus (Greek: Κυπριακή Δημοκρατία; '\n",
            " 'Turkish: Kıbrıs Cumhuriyeti), is an island country in the Eastern '\n",
            " 'Mediterranean Sea, off the coasts of Syria and Turkey.[e] Cyprus is the '\n",
            " 'third largest and third most populous island in the Mediterranean, and a '\n",
            " 'member state of the European Union. It is located south of Turkey, west of '\n",
            " 'Syria and Lebanon, northwest of Israel and Palestine, north of Egypt and '\n",
            " 'east of Greece.')\n",
            "CLean Cont = \n",
            "('cyprus isaprs greek ipa [ cipros ] turkish kbrs ipa [ kbs ] officially '\n",
            " 'republic cyprus greek turkish kbrs cumhuriyeti island country eastern '\n",
            " 'mediterranean sea coasts syria turkey [ e ] cyprus third largest third '\n",
            " 'populous island mediterranean member state european union located south '\n",
            " 'turkey west syria lebanon northwest israel palestine north egypt east greece')\n",
            "Question =  Where is Cyprus located?\n",
            "Clean Question =  where is cyprus located\n",
            "Answer =  Eastern Mediterranean Sea\n",
            "Clean Answer =  eastern mediterranean sea\n",
            "AS,AE =  (22, 24)\n",
            "encoded  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rZCyBydBZW1",
        "colab_type": "code",
        "outputId": "00c96fb0-f2bb-4de2-f20e-ccd5320d871d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "pprint.pprint(params)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'test_span_outofrange': 0,\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'train_span_outofrange': 0,\n",
            " 'training.batch_size': 64,\n",
            " 'training.epochs': 25,\n",
            " 'val_shape': (26061, 16),\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDzXjXa3MpP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(squad_df['clean_context'][10])\n",
        "print(train_context_sequence[110])\n",
        "print(squad_df['clean_question'][10])\n",
        "print(train_question_sequence[10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sveyxKK5j8q",
        "colab_type": "text"
      },
      "source": [
        "### 3.5 Update and persist params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K170rfN15qGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### SAVE PARAMS\n",
        "# Writing to sample.json \n",
        "with open(model_path + \"params.json\", \"w\") as p: \n",
        "    p.write(json.dumps(params))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6FSl5UDL_qD",
        "colab_type": "text"
      },
      "source": [
        "## 4 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS6fKEvieWrX",
        "colab_type": "text"
      },
      "source": [
        "**Implements a baseline 0 in Deep Learning based approach per our project synopsis. This baseline model uses the following layers **\n",
        "0.   Input layer\n",
        "1.   Embedding Layer\n",
        "2.   List LSTM\n",
        "3.   a custom Bilinear Similarity layer \n",
        "4.   Prediction Layer\n",
        "5.   Output layer \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9JFn3oWiU4y",
        "colab_type": "text"
      },
      "source": [
        "### 4.2 Building Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLGurD9eijTh",
        "colab_type": "text"
      },
      "source": [
        "**For Questions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLqUQHSjeVwq",
        "colab_type": "code",
        "outputId": "cb9c3dbe-53d2-4d82-93f1-fd1922017884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# question embedding\n",
        "q_input = layers.Input(shape=(params['question_max_length'],),name=\"QUESTION_INPUT\")\n",
        "q_emb = layers.Embedding(input_dim=params['vocab_size']+1,\n",
        "                  output_dim=params['embedding_size'],\n",
        "                  name=\"QUESTION_EMBEDDING\")(q_input)\n",
        "\n",
        "# encoder \n",
        "q_output = layers.LSTM(units=params['rnn_units'], \n",
        "                     name='QUESTION_LSTM')(q_emb)\n",
        "print(q_output.shape)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eJ2ykC1megw",
        "colab_type": "text"
      },
      "source": [
        "**For Context**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-glhG509P5Yh",
        "colab_type": "code",
        "outputId": "c89fbe7d-0181-4501-f84f-0a39075dc393",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c_input = layers.Input(shape=(params['context_max_length'],),name=\"CONTEXT_INPUT\")\n",
        "\n",
        "# context embedding\n",
        "c_emb = layers.Embedding(input_dim=params['vocab_size']+1,\n",
        "                  output_dim=params['embedding_size'],\n",
        "                  name=\"CONTEXT_EMBEDDING\")(c_input)\n",
        "\n",
        "\n",
        "\n",
        "# exact_match\n",
        "# ex_ = Input(shape=(CON_LEN,2))\n",
        "\n",
        "# # pos tags\n",
        "# pos_ = Input(shape=(CON_LEN,len(tag_to_num)+1))\n",
        "\n",
        "# # term frequency\n",
        "# term_ = Input(shape=(CON_LEN,1)) \n",
        "\n",
        "# concatenate input\n",
        "# concat = concatenate([c_emb,ex_,pos_,term_])\n",
        "\n",
        "c_output = layers.LSTM(params['rnn_units'],\n",
        "                 name='CONTEXT_LSTM',return_sequences=True)(c_emb)\n",
        "\n",
        "print(\"final output to bilinear \",c_output.shape)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final output to bilinear  (None, 426, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g93054zrp9yp",
        "colab_type": "text"
      },
      "source": [
        "**Bilinear Term**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNsrWO_tpppa",
        "colab_type": "code",
        "outputId": "56c967cc-a222-473f-83fa-ba2a5f65edc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Reference -- https://github.com/kellywzhang/reading-comprehension/blob/master/attention.py\n",
        "# bilinear term ####\n",
        "print(\"Question context shape \",q_output.shape)\n",
        "print(\"final o/p of context \",c_output.shape)\n",
        "\n",
        "################ start prediction ######################\n",
        "start = layers.Dense(params['rnn_units'])(q_output)\n",
        "\n",
        "# ading time_slice to question (batch_size,1,hidden)\n",
        "# shape (64,128) --> (64,1,128)\n",
        "hidden_start_time_axis = tf.expand_dims(start, -1)\n",
        "\n",
        "# squeeze remooves time slice we added before\n",
        "# final shape = (batch_size,decoder_timesteps)\n",
        "start_ = tf.squeeze(tf.matmul(c_output,hidden_start_time_axis),2)\n",
        "    \n",
        "start_ = tf.nn.softmax(start_,axis = 1)\n",
        "    \n",
        "################ end prediction ######################\n",
        "end = layers.Dense(params['rnn_units'])(q_output)\n",
        "\n",
        "hidden_end_time_axis = tf.expand_dims(end, -1)\n",
        "\n",
        "# squeeze remooves time slice we added before\n",
        "# final shape = (batch_size,decoder_timesteps)\n",
        "end_ = tf.squeeze(tf.matmul(c_output,hidden_end_time_axis),2)\n",
        "end_ = tf.nn.softmax(end_,axis=1)\n",
        "\n",
        "prob_token_span = tf.concat((start_,end_),axis = 1)\n",
        "print(\"Logits shape \",prob_token_span.shape)\n",
        "\n",
        "\n",
        "# logits = BilinearSimilarity(UNITS)(q_cont,c_)\n",
        "# Y_prob = Prediction()(logits)\n",
        "# print(\"Logits shape \",logits.shape)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question context shape  (None, 256)\n",
            "final o/p of context  (None, 426, 256)\n",
            "Logits shape  (None, 852)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibcGeRqUxzKN",
        "colab_type": "text"
      },
      "source": [
        "**Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFbuhviAyX7l",
        "colab_type": "code",
        "outputId": "511f8849-906a-435e-f4ea-b0df33590792",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "####### Prediction ### \n",
        "token_span = 20\n",
        "start_prob = prob_token_span[:,:params['context_max_length']]\n",
        "end_prob = prob_token_span[:,params['context_max_length']:]\n",
        "\n",
        "# do the outer product\n",
        "outer = tf.matmul(tf.expand_dims(start_prob, axis=2),tf.expand_dims(end_prob, axis=1))\n",
        "\n",
        "outer = tf.linalg.band_part(outer, 0, token_span)\n",
        "\n",
        "#print(outer.shape)\n",
        "\n",
        "# start_position will have shape of (batch_size,)\n",
        "start_position = tf.reduce_max(outer, axis=2)\n",
        "#end position will have shape of (batch_size,)\n",
        "end_position = tf.reduce_max(outer, axis=1)\n",
        "\n",
        "y_probab = tf.concat([start_position,end_position],axis=1)\n",
        "\n",
        "print(y_probab.shape)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 852)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLtlPSfLxyzB",
        "colab_type": "text"
      },
      "source": [
        "### 4.3 Custom Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlOkeMveyCWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logits_loss(y_true,logits):\n",
        "    \"\"\"\n",
        "    Custom loss function which minimises log_loss.\n",
        "    Referance https://stackoverflow.com/questions/50063613/add-loss-function-in-keras\n",
        "    \"\"\"\n",
        "    \n",
        "    #y_true = tf.cast(y_true,dtype=tf.int32)\n",
        "    #logits = tf.cast(logits,dtype=tf.float32)\n",
        "    \n",
        "    # breaking the tensor into two half's to get start and end label.\n",
        "    start_label = y_true[:,:params['context_max_length']]\n",
        "    end_label = y_true[:,params['context_max_length']:]\n",
        "    \n",
        "    # braking the logits tensor into start and end part for loss calcultion.\n",
        "    start_logit = logits[:,:params['context_max_length']]\n",
        "    end_logit = logits[:,params['context_max_length']:]\n",
        "    \n",
        "    start_loss = tf.keras.backend.categorical_crossentropy(start_label,start_logit)\n",
        "    end_loss = tf.keras.backend.categorical_crossentropy(end_label,end_logit)\n",
        "    \n",
        "#     start_loss = tf.losses.sparse_softmax_cross_entropy(labels=start_label, logits=start_logit)\n",
        "#     end_loss = tf.losses.sparse_softmax_cross_entropy(labels=end_label, logits=end_logit)\n",
        "    \n",
        "    # as per paer\n",
        "    \n",
        "    loss = start_loss + end_loss\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNiC79-tyLmq",
        "colab_type": "text"
      },
      "source": [
        "### 4.4 Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv9aw2EewJbr",
        "colab_type": "code",
        "outputId": "ccb93cb5-5e13-4a35-9b61-ac4838b83b3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Model(inputs = [q_input,c_input],outputs =y_probab)\n",
        "model.summary()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "QUESTION_INPUT (InputLayer)     [(None, 40)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_EMBEDDING (Embedding)  (None, 40, 100)      10085100    QUESTION_INPUT[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_INPUT (InputLayer)      [(None, 426)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_LSTM (LSTM)            (None, 256)          365568      QUESTION_EMBEDDING[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_EMBEDDING (Embedding)   (None, 426, 100)     10085100    CONTEXT_INPUT[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 256)          65792       QUESTION_LSTM[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 256)          65792       QUESTION_LSTM[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_LSTM (LSTM)             (None, 426, 256)     365568      CONTEXT_EMBEDDING[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_2 (Tenso [(None, 256, 1)]     0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_3 (Tenso [(None, 256, 1)]     0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_2 (Te [(None, 426, 1)]     0           CONTEXT_LSTM[0][0]               \n",
            "                                                                 tf_op_layer_ExpandDims_2[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_3 (Te [(None, 426, 1)]     0           CONTEXT_LSTM[0][0]               \n",
            "                                                                 tf_op_layer_ExpandDims_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze_2 (TensorFl [(None, 426)]        0           tf_op_layer_BatchMatMulV2_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze_3 (TensorFl [(None, 426)]        0           tf_op_layer_BatchMatMulV2_3[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softmax_2 (TensorFl [(None, 426)]        0           tf_op_layer_Squeeze_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softmax_3 (TensorFl [(None, 426)]        0           tf_op_layer_Squeeze_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1 (TensorFlo [(None, 852)]        0           tf_op_layer_Softmax_2[0][0]      \n",
            "                                                                 tf_op_layer_Softmax_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, 426)]        0           tf_op_layer_concat_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_1 (Te [(None, 426)]        0           tf_op_layer_concat_1[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_4 (Tenso [(None, 426, 1)]     0           tf_op_layer_strided_slice[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_5 (Tenso [(None, 1, 426)]     0           tf_op_layer_strided_slice_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_4 (Te [(None, 426, 426)]   0           tf_op_layer_ExpandDims_4[0][0]   \n",
            "                                                                 tf_op_layer_ExpandDims_5[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_MatrixBandPart (Ten [(None, 426, 426)]   0           tf_op_layer_BatchMatMulV2_4[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Max (TensorFlowOpLa [(None, 426)]        0           tf_op_layer_MatrixBandPart[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Max_1 (TensorFlowOp [(None, 426)]        0           tf_op_layer_MatrixBandPart[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_2 (TensorFlo [(None, 852)]        0           tf_op_layer_Max[0][0]            \n",
            "                                                                 tf_op_layer_Max_1[0][0]          \n",
            "==================================================================================================\n",
            "Total params: 21,032,920\n",
            "Trainable params: 21,032,920\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Biepd_j518BQ",
        "colab_type": "text"
      },
      "source": [
        "### 4.5 Model Compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAqT6LWbXxWE",
        "colab_type": "text"
      },
      "source": [
        "**Tensorboard Logs and Model compilation** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTB5bE8I2Al-",
        "colab_type": "code",
        "outputId": "e59c5503-872d-4c69-fd74-8e8a214c8117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# using tensorboard instance for callbacks\n",
        "from time import time\n",
        "from datetime import datetime\n",
        "from tensorflow.python.keras.callbacks import TensorBoard\n",
        "\n",
        "log_dir = tensorboard_logpath +\"lstm-baseline0\"\n",
        "print('Tensprflow logs ',log_dir)\n",
        "tensorboard = TensorBoard(log_dir=log_dir,histogram_freq=1)\n",
        "\n",
        "# model compilation\n",
        "model.compile(optimizer=\"adamax\",loss=logits_loss,metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensprflow logs  /content/drive/My Drive/AIML-MRC-Capstone/models/tensorboard-logs/lstm-baseline0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WuaPHlU8hnp",
        "colab_type": "text"
      },
      "source": [
        "### 4.6 Generator Function for use in Model.fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYIgojpb8g82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Reference \n",
        "def generator_function(length,batch_size = 64,data_type = 'Train'):\n",
        "    \"\"\"\n",
        "    This function is generates batches of data to avoid strain on memory.\n",
        "    \"\"\"\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    flag = True\n",
        "    if data_type == 'Val':\n",
        "        flag = False\n",
        "    n = 0\n",
        "    # loop forever over datapoints.\n",
        "    while 1:\n",
        "        for i in range(length):\n",
        "            n += 1\n",
        "            if flag:\n",
        "                X1.append(train_question_sequence[i])\n",
        "                X2.append(train_context_sequence[i])                \n",
        "                y.append(y_train[i])\n",
        "            else:\n",
        "                X1.append(val_question_sequence[i])\n",
        "                X2.append(val_context_sequence[i])                \n",
        "                y.append(y_val[i])\n",
        "            if n == batch_size:\n",
        "                yield ((array(X1),array(X2)),array(y))\n",
        "                X1,X2, y = list(), list(), list()\n",
        "                n=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlGf9CAe-ZW-",
        "colab_type": "text"
      },
      "source": [
        "### 4.7 Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO-l9AhD-fPM",
        "colab_type": "code",
        "outputId": "d4225b5d-005a-4223-fb9c-72b12fb5727e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "params['training.epochs']=25\n",
        "params['training.batch_size']=64\n",
        "params['training.train_length']=len(y_train)\n",
        "params['training.val_length']=len(y_val)\n",
        "params['training.train_steps']=params['training.train_length']//params['training.batch_size']\n",
        "params['training.val_steps']=params['training.val_length']//32\n",
        "\n",
        "pprint.pprint(params)\n",
        "\n",
        "### SAVE PARAMS\n",
        "# Writing to sample.json \n",
        "with open(model_path + \"params.json\", \"w\") as p: \n",
        "    p.write(json.dumps(params))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'test_span_outofrange': 0,\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'train_span_outofrange': 0,\n",
            " 'training.batch_size': 64,\n",
            " 'training.epochs': 25,\n",
            " 'training.train_length': 78183,\n",
            " 'training.train_steps': 1221,\n",
            " 'training.val_length': 26061,\n",
            " 'training.val_steps': 814,\n",
            " 'val_shape': (26061, 16),\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfbn-RnO_EOc",
        "colab_type": "code",
        "outputId": "fef1bf16-45a1-4e23-d188-f63c28d155f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "for i in range(params['training.epochs']):\n",
        "    print(\"Epoch {} start at time \".format(i),datetime.now())\n",
        "    \n",
        "    train_generator = generator_function(params['training.train_length'],\n",
        "                                         params['training.batch_size'])\n",
        "    \n",
        "    val_generator = generator_function(params['training.val_length'],\n",
        "                                       32,\n",
        "                                       \"Val\")\n",
        "    model.fit(x=train_generator, epochs=1, \n",
        "                        steps_per_epoch=params['training.train_steps'],\n",
        "                        verbose=1,\n",
        "                        callbacks=[tensorboard],\n",
        "                        validation_data=val_generator,\n",
        "                        validation_steps=params['training.val_steps'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 start at time  2020-05-31 02:06:29.202658\n",
            "1221/1221 [==============================] - 136s 111ms/step - loss: 8.3468 - accuracy: 0.2365 - val_loss: 8.1262 - val_accuracy: 0.2791\n",
            "Epoch 1 start at time  2020-05-31 02:08:48.412338\n",
            "1113/1221 [==========================>...] - ETA: 10s - loss: 7.3605 - accuracy: 0.3453"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRoOUajk492o",
        "colab_type": "text"
      },
      "source": [
        "### 4.6 Serialize and Persist Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnnjf-hX2FD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(model_path + \"model_epoch_{}.h5\".format(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl0YsArymDdF",
        "colab_type": "text"
      },
      "source": [
        "### 4.7 Load existing models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBwGJN07mEGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(model_path + \"model_epoch_24.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tVohk7bORpH",
        "colab_type": "text"
      },
      "source": [
        "### 4.8 Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7pxr5yHOV6m",
        "colab_type": "text"
      },
      "source": [
        "**Eval on Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VEUBrVHN6c_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "bf5dcf3a-5205-4d6d-8d2d-723f86c2cb5c"
      },
      "source": [
        "y_prediction = model.predict([test_question_sequence,test_context_sequence])"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-e786f5bb1bb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_question_sequence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_context_sequence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1266\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnVyS4ONOiNO",
        "colab_type": "code",
        "outputId": "a9b8ea33-1061-42e7-e4f5-194e6a50ab20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_prediction.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26062"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs9NJGGPP60m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test_fixed = np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xTLoptoRwZ0",
        "colab_type": "code",
        "outputId": "4da49e73-851b-4217-d99b-1af641694db6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "y_test_fixed[0,:params['context_max_length']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_XCfB-gOpuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_pred = []\n",
        "end_pred = []\n",
        "for i in range(26062):\n",
        "    start_pred.append(np.argmax(y_prediction[i,:params['context_max_length']]))\n",
        "    end_pred.append(np.argmax(y_prediction[i,params['context_max_length']:]))\n",
        "    \n",
        "start = []\n",
        "end = []\n",
        "for i in range(26062):\n",
        "    start.append(np.argmax(y_test_fixed[i,:params['context_max_length']]))\n",
        "    end.append(np.argmax(y_test_fixed[i,params['context_max_length']:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwqdJBsmR2rt",
        "colab_type": "code",
        "outputId": "0209f673-9f34-4eef-ca8f-42aac8a70bb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(start_pred[100:120])\n",
        "print(end_pred[100:120])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[425, 10, 425, 425, 425, 425, 423, 425, 49, 425, 425, 425, 425, 425, 425, 0, 29, 425, 9, 425]\n",
            "[425, 10, 425, 425, 425, 425, 423, 425, 49, 425, 425, 425, 425, 425, 425, 0, 29, 425, 9, 425]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "138CmIXnR7LS",
        "colab_type": "code",
        "outputId": "cd8241f5-1a45-4e06-e6b4-109a9b14c1bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(start[100:120])\n",
        "print(end[100:120])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[425, 425, 425, 20, 36, 3, 84, 425, 425, 425, 425, 70, 425, 19, 24, 8, 25, 74, 1, 49]\n",
            "[425, 425, 425, 22, 40, 3, 85, 425, 425, 425, 425, 70, 425, 20, 26, 9, 28, 75, 1, 54]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPx7_OKMR8fX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_predicted_new = np.zeros((26062,params['context_max_length']))\n",
        "for i in range(26062):\n",
        "    y_predicted_new[i,start_pred[i]:end_pred[i]+1] = 1\n",
        "    \n",
        "y_test_new = np.zeros((26062,params['context_max_length']))\n",
        "for i in range(26062):\n",
        "    y_test_new[i,start[i]:end[i]+1] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmwzLM3WSriL",
        "colab_type": "code",
        "outputId": "1293cbe3-a53d-455d-ce3c-f1556d3ac554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.metrics import f1_score,accuracy_score,precision_score\n",
        "print(\"Micro f1-score on test data is \",f1_score(y_test_new,y_predicted_new,average=\"micro\"))\n",
        "print(\"Macro f1-score on test data is \",f1_score(y_test_new,y_predicted_new,average=\"macro\"))\n",
        "print(\"Accuracy on test data is \",accuracy_score(y_test_new,y_predicted_new))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Micro f1-score on test data is  0.24381819178683042\n",
            "Macro f1-score on test data is  0.0029310912484228793\n",
            "Accuracy on test data is  0.3332054331977592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5T9BSNgS-uG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}