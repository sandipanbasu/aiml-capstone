{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mrc-LSTM-baseline0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanbasu/aiml-capstone/blob/master/mrc_LSTM_baseline0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA8sHlEbfYnn",
        "colab_type": "code",
        "outputId": "b8dc32a0-5639-4a1e-a0ac-c43412476d28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGZp4iLEHE8n",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries and Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_u1n8G3fge_",
        "colab_type": "code",
        "outputId": "13f0b929-9b1d-4f66-bef2-3f2eda93efee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "project_path = \"/content/drive/My Drive/AIML-MRC-Capstone/datasets/Squad2.0/TrainingDataset/\"\n",
        "\n",
        "squad_df = pd.read_csv(project_path+'squad_data_final.csv')\n",
        "squad_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "squad_df.tail(5)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer</th>\n",
              "      <th>plausible_answer_start</th>\n",
              "      <th>plausible_answer</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>clean_context</th>\n",
              "      <th>clean_question</th>\n",
              "      <th>clean_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>130301</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>Physics has broadly agreed on the definition o...</td>\n",
              "      <td>5a7e070b70df9f001a875439</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>485.0</td>\n",
              "      <td>matter</td>\n",
              "      <td>True</td>\n",
              "      <td>term matter used throughout physics bewilderin...</td>\n",
              "      <td>physics has broadly agreed on the definition o...</td>\n",
              "      <td>IMPOSSIBLE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130302</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>Who coined the term partonic matter?</td>\n",
              "      <td>5a7e070b70df9f001a87543a</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>327.0</td>\n",
              "      <td>Alfvén</td>\n",
              "      <td>True</td>\n",
              "      <td>term matter used throughout physics bewilderin...</td>\n",
              "      <td>who coined the term partonic matter</td>\n",
              "      <td>IMPOSSIBLE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130303</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>What is another name for anti-matter?</td>\n",
              "      <td>5a7e070b70df9f001a87543b</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>350.0</td>\n",
              "      <td>Gk. common matter</td>\n",
              "      <td>True</td>\n",
              "      <td>term matter used throughout physics bewilderin...</td>\n",
              "      <td>what is another name for antimatter</td>\n",
              "      <td>IMPOSSIBLE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130304</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>Matter usually does not need to be used in con...</td>\n",
              "      <td>5a7e070b70df9f001a87543c</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>529.0</td>\n",
              "      <td>a specifying modifier</td>\n",
              "      <td>True</td>\n",
              "      <td>term matter used throughout physics bewilderin...</td>\n",
              "      <td>matter usually does not need to be used in con...</td>\n",
              "      <td>IMPOSSIBLE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130305</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>What field of study has a variety of unusual c...</td>\n",
              "      <td>5a7e070b70df9f001a87543d</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>37.0</td>\n",
              "      <td>physics</td>\n",
              "      <td>True</td>\n",
              "      <td>term matter used throughout physics bewilderin...</td>\n",
              "      <td>what field of study has a variety of unusual c...</td>\n",
              "      <td>IMPOSSIBLE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         title  ... clean_answer\n",
              "130301  Matter  ...   IMPOSSIBLE\n",
              "130302  Matter  ...   IMPOSSIBLE\n",
              "130303  Matter  ...   IMPOSSIBLE\n",
              "130304  Matter  ...   IMPOSSIBLE\n",
              "130305  Matter  ...   IMPOSSIBLE\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "855WpIGFHfMr",
        "colab_type": "text"
      },
      "source": [
        "# Lets use 2 LSTM network for context and question and use a concat layer to merge. y is the answer "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6dcPCGiJrz1",
        "colab_type": "text"
      },
      "source": [
        "# Step 1 Tokenize and Vectorize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTFqLp3iAqjq",
        "colab_type": "text"
      },
      "source": [
        "### Find Max Sequence Length for both the Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di181GmIOSq3",
        "colab_type": "code",
        "outputId": "5647c204-b0b3-48c8-bde1-c4c5780b5447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# max length of context\n",
        "max_context_seq_length= max(len(txt) for txt in context_sequence)\n",
        "print('max_context_seq_length=',max_context_seq_length)\n",
        "\n",
        "# vocab size of context\n",
        "context_vocab_size=len(context_tokenize.word_index)\n",
        "print('context_vocab_size=',context_vocab_size)\n",
        "\n",
        "max_question_seq_length=max(len(txt) for txt in questions_sequence)\n",
        "print('max_question_seq_length=',max_question_seq_length)\n",
        "\n",
        "# vocab size of questions\n",
        "questions_vocab_size=len(questions_tokenize.word_index)\n",
        "print('questions_vocab_size=',questions_vocab_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_context_seq_length= 426\n",
            "context_vocab_size= 93529\n",
            "max_question_seq_length= 40\n",
            "questions_vocab_size= 47289\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkTaGKrnLLFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From the EDA and historgrams we can conclude that - \n",
        "# 99% percentile of context word length = 285\n",
        "# 99% percentile or question word lengt = 20\n",
        "max_context_seq_length = 285\n",
        "max_question_seq_length = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcNCdQgK_sE9",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizing context and question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cCWphXNKoHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context_tokenize = preprocessing.text.Tokenizer()\n",
        "context_tokenize.fit_on_texts(squad_df['clean_context']) #Fit it on clean_context\n",
        "\n",
        "questions_tokenize= preprocessing.text.Tokenizer()\n",
        "questions_tokenize.fit_on_texts(squad_df['clean_question'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2iQ-9ZZNWhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### SAVE TOKENIZERS\n",
        "pickle.dump(context_tokenize, open(project_path + \"context_tokenize.pickle\", \"wb\"))\n",
        "pickle.dump(questions_tokenize, open(project_path + \"questions_tokenize.pickle\", \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn5paTsLONKE",
        "colab_type": "code",
        "outputId": "030c780d-8492-4219-f7a2-603885d7bbda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(squad_df['clean_context'][2000])\n",
        "print(context_sequence[3000])\n",
        "print(squad_df['clean_question'][2000])\n",
        "print(questions_sequence[2000])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "october 21 2008 apple reported 14 21 total revenue fiscal quarter 4 year 2008 came ipods september 9 2009 keynote presentation apple event phil schiller announced total cumulative sales ipods exceeded 220 million continual decline ipod sales since 2009 surprising trend apple corporation apple cfo peter oppenheimer explained june 2009 expect traditional mp3 players decline time cannibalize ipod touch iphone since 2009 companys ipod sales continually decreased every financial quarter 2013 new model introduced onto market\n",
            "[360, 464, 56909, 483, 454, 21, 7582, 7198, 1401, 2632, 2957, 4, 14, 1401, 5514, 56910, 1051, 2806, 4752, 249, 1260, 9282, 2806, 5514, 7725, 2752, 1600, 3891, 692, 1892, 5514, 204, 2129, 4752, 26680, 78, 578, 5194, 118, 1401, 618, 5590, 118, 5514, 2, 56911, 1482, 11598, 4122, 16977, 229, 56912, 962, 3628, 4968, 23304, 1030, 2632, 2957, 207, 1339, 92, 6491, 162, 1512, 337, 4752, 26680, 578, 8296, 29, 3198, 10, 397, 2728, 525, 170, 578, 65, 8, 10870, 51, 3279, 584]\n",
            "who was chief financial officer of apple in july of 2009\n",
            "[10, 6, 886, 620, 2250, 3, 762, 4, 1224, 3, 320]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lRuVtp_7y51",
        "colab_type": "text"
      },
      "source": [
        "### Vectorization "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhuvjYmZ7m6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert the context text to indexes\n",
        "context_sequence= context_tokenize.texts_to_sequences(squad_df['clean_context'])\n",
        "# convert the questions to indexes\n",
        "questions_sequence= questions_tokenize.texts_to_sequences(squad_df['clean_question'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHh0g7SDLOjL",
        "colab_type": "text"
      },
      "source": [
        "### Padding the sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVRGBRsXLU2U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9e41d644-c662-43c5-a621-203817824bdf"
      },
      "source": [
        "# padding context\n",
        "context_input_data= tf.keras.preprocessing.sequence.pad_sequences(context_sequence, maxlen=max_context_seq_length, padding='pre')\n",
        "# padding question\n",
        "question_input_data=tf.keras.preprocessing.sequence.pad_sequences(questions_sequence, maxlen=max_question_seq_length,padding='pre')\n",
        "\n",
        "print(context_input_data.shape)\n",
        "print(question_input_data.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(130306, 285)\n",
            "(130306, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6FSl5UDL_qD",
        "colab_type": "text"
      },
      "source": [
        "# Build the LSTM Model for both Sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-glhG509P5Yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size = 50\n",
        "rnn_units=256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOYjXu-vCZyB",
        "colab_type": "text"
      },
      "source": [
        "### Embedding Layer for Context and Question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_LGPo_6QKtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CONTEXT LSTM\n",
        "# input layer\n",
        "context_input=layers.Input(shape=(max_context_seq_length,),name=\"CONTEXT_INPUT\")\n",
        "# Build Embedding layer and Get Embedding Layer output\n",
        "context_embedding_output=layers.Embedding(context_vocab_size+1, embedding_size, name=\"CONTEXT_EMBEDDING\")(context_input)\n",
        "\n",
        "\n",
        "# QUESTION LSTM\n",
        "#input layer\n",
        "question_input=layers.Input(shape=(max_question_seq_length,),name=\"QUESTION_INPUT\")\n",
        "#Embedding layer and #Embedding layer output\n",
        "question_embedding_output=layers.Embedding(input_dim=questions_vocab_size+1, output_dim=embedding_size, name=\"QUESTION_EMBEDDING\")(question_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rliqd3nmR_3V",
        "colab_type": "text"
      },
      "source": [
        "### Encoder Layer for Context and Question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOC36paDR9W7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "25d250b5-0f51-4f06-f504-cf8fefca0619"
      },
      "source": [
        "# RNN Encoder with LSTM for context\n",
        "c_output,c_h, c_s = layers.LSTM(rnn_units,name='CONTEXT_LSTM', return_state=True)(context_embedding_output)\n",
        "context_states= [c_h, c_s]\n",
        "\n",
        "# RNN Encoder with LSTM for question\n",
        "q_output,q_h, q_s= tf.keras.layers.LSTM(rnn_units,name='QUESTION_LSTM',return_state=True)(question_embedding_output)\n",
        "questions_states = [q_h, q_s]\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1d4f3495ce52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# RNN Encoder with LSTM for context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mc_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CONTEXT_LSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_embedding_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mcontext_states\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_s\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# RNN Encoder with LSTM for question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wE36OJYsgHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d2206674-d390-48d0-da3e-e9fd03b72846"
      },
      "source": [
        "context_states,questions_states"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<tf.Tensor 'CONTEXT_LSTM/Identity_1:0' shape=(None, 256) dtype=float32>,\n",
              "  <tf.Tensor 'CONTEXT_LSTM/Identity_2:0' shape=(None, 256) dtype=float32>],\n",
              " [<tf.Tensor 'QUESTION_LSTM/Identity_1:0' shape=(None, 256) dtype=float32>,\n",
              "  <tf.Tensor 'QUESTION_LSTM/Identity_2:0' shape=(None, 256) dtype=float32>])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRcPtZqFXabR",
        "colab_type": "text"
      },
      "source": [
        "### Concat the both RNN LSTM Encoder layers to get merged cell state and hidden state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbyWtSg8XeZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MERGED_cell_state =layers.concatenate([context_states[0],questions_states[0]],name=\"CONCAT_CELL_STATE\")\n",
        "MERGED_hidden_state =layers.concatenate([context_states[1],questions_states[1]],name=\"HIDDEN_CELL_STATE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGqKHNq3rEPL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f27be10d-038e-4d6d-fa49-b7e14d9a3df9"
      },
      "source": [
        "decoder_initial_state = [MERGED_cell_state,MERGED_hidden_state]\n",
        "decoder_initial_state"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'CONCAT_CELL_STATE/Identity:0' shape=(None, 512) dtype=float32>,\n",
              " <tf.Tensor 'HIDDEN_CELL_STATE/Identity:0' shape=(None, 512) dtype=float32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkqzHf00dSfI",
        "colab_type": "text"
      },
      "source": [
        "# Create Decoder for Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVjDhKLUd9jM",
        "colab_type": "text"
      },
      "source": [
        "# Add  Start and  End tokens to Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEf_85IGsYIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "squad_df['answer_start_end']= '<start>' + squad_df['clean_answer'] + '<end>'\n",
        "squad_df['answer_start_end']=squad_df['answer_start_end'].astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S25hREZeNIW",
        "colab_type": "text"
      },
      "source": [
        "#Tokenize the Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-5MBMrOyTsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answers_tokenize=tf.keras.preprocessing.text.Tokenizer()\n",
        "answers_tokenize.fit_on_texts(squad_df['answer_start_end'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1by-Iw8yteW",
        "colab_type": "code",
        "outputId": "8c3c3991-047c-41fc-899e-319ffffb9422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Vocab\n",
        "print(len(answers_tokenize.word_index))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sbN9pYI_GqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert sentences to numbers \n",
        "answers_seq = answers_tokenize.texts_to_sequences(squad_df['answer_start_end']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEmnUNaw_jyH",
        "colab_type": "code",
        "outputId": "3ac55ec5-777d-4f52-eca2-b9346471efd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(squad_df['answer_start_end'][2000])\n",
        "print(answers_seq[2000])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start>peter oppenheimer<end>\n",
            "[2, 924, 7781, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24mDpru-e624",
        "colab_type": "text"
      },
      "source": [
        "# Get maximum length and pad the sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL5y5fqqzPav",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4905d04b-a41b-4b1f-b3c6-bb17db05a341"
      },
      "source": [
        "squad_df[squad_df['clean_answer'].str.len() > 200]['answer_start_end']"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3201    <start>that the sudden shift of a huge quantit...\n",
              "Name: answer_start_end, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4R8QCYI_ywa",
        "colab_type": "code",
        "outputId": "1948a90a-e8e3-4e2b-db7b-fabb0a798ea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "max_answers_seq_length=max(len(txt) for txt in squad_df['answer_start_end'])\n",
        "print('max_answers_seq_length=',max_answers_seq_length)\n",
        "\n",
        "answers_vocab_size=len(answers_tokenize.word_index)\n",
        "print('answers_vocab_size=',answers_vocab_size)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_answers_seq_length= 248\n",
            "answers_vocab_size= 41475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IImu_CPiN8gi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From the EDA and historgrams we can conclude that - \n",
        "# 99% percentile of answer word length = 17\n",
        "max_answers_seq_length=17"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqJpe8rTAUHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pad pre\n",
        "answers_input_data= tf.keras.preprocessing.sequence.pad_sequences(answers_seq,maxlen=max_answers_seq_length,padding='pre')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC5nH0YKizaj",
        "colab_type": "text"
      },
      "source": [
        "# Building Decoder Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byD2aHWjBfng",
        "colab_type": "code",
        "outputId": "7b9c1d89-ff38-443e-918c-683894a0f10b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "answers_input_data.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(130306, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRMx8dPPB-jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize array\n",
        "answers_target_data = np.zeros((answers_input_data.shape[0], #number of sentences 130306\n",
        "                                answers_input_data.shape[1])) #number of words in each sentence 248\n",
        "\n",
        "#Shift Target output by one word\n",
        "for i in range(answers_input_data.shape[0]):\n",
        "    for j in range(1,answers_input_data.shape[1]):\n",
        "        answers_target_data[i][j-1] = answers_input_data[i][j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbFQKKXkDGaP",
        "colab_type": "code",
        "outputId": "6e0cac8e-4a54-4e98-ac20-90d91653a239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(squad_df['answer_start_end'][2000])\n",
        "print(answers_input_data[2000])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start>peter oppenheimer<end>\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    2\n",
            "  924 7781    1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtEzDD_Ai2Rx",
        "colab_type": "text"
      },
      "source": [
        "# Convert Answers to one-hot vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN4VGg0ECo3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Crashing !!\n",
        "answers_target_data_one_hot= np.zeros((answers_input_data.shape[0], #number of sentences\n",
        "                                       answers_input_data.shape[1], #Number of words in each sentence\n",
        "                                       len(answers_tokenize.word_index)+1)) #Vocab size + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkuJpMPXpwS2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "02064c15-5230-4081-8688-eed5b037a1d4"
      },
      "source": [
        "print(answers_input_data.shape)\n",
        "print(len(answers_tokenize.word_index)+1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(130306, 17)\n",
            "41476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhCFV20ZLOmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answers_embedding_size = 50\n",
        "decoder_rnn_units = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xfx0f0mii58",
        "colab_type": "text"
      },
      "source": [
        "# Build Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k38FCLDrJTlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input layer\n",
        "answers_inputs=tf.keras.layers.Input(shape=(max_answers_seq_length,),name=\"ANSWER_INPUT\")\n",
        "\n",
        "#Embedding\n",
        "answers_embedding_output=tf.keras.layers.Embedding(answers_vocab_size+1, answers_embedding_size, name=\"ANSWER_EMBEDDING\")(answers_inputs)\n",
        "\n",
        "#lstm layer\n",
        "answers_lstm= tf.keras.layers.LSTM(decoder_rnn_units,return_sequences=True,name=\"ANSWER_LSTM\", return_state=True)\n",
        "\n",
        "#LSTM Output, State initialization from Encoder states(concat of question and answer)\n",
        "#Output will be all hidden sequences, last 'h' state and last 'c' state\n",
        "\n",
        "output,_,_=answers_lstm(answers_embedding_output,initial_state=decoder_initial_state)\n",
        "\n",
        "#dense layer\n",
        "lstm3_dense= tf.keras.layers.Dense(answers_vocab_size+1,activation='softmax',name=\"FINAL_OUTPUT\")\n",
        "\n",
        "#answer output\n",
        "answer_outputs=lstm3_dense(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14J7GXH2jBcO",
        "colab_type": "text"
      },
      "source": [
        "# Build Model using Encoder ( output of concat) and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFhlppB6qsgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Model([context_input,question_input, answers_inputs],answer_outputs) #Output of the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCTlgD7tu2yn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "234e2ca0-51e4-468d-8af4-b47c2d3af04e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "CONTEXT_INPUT (InputLayer)      [(None, 285)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_INPUT (InputLayer)     [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_EMBEDDING (Embedding)   (None, 285, 50)      4676500     CONTEXT_INPUT[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_EMBEDDING (Embedding)  (None, 20, 50)       2364500     QUESTION_INPUT[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "ANSWER_INPUT (InputLayer)       [(None, 17)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_LSTM (LSTM)             [(None, 256), (None, 314368      CONTEXT_EMBEDDING[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_LSTM (LSTM)            [(None, 256), (None, 314368      QUESTION_EMBEDDING[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "ANSWER_EMBEDDING (Embedding)    (None, 17, 50)       2073800     ANSWER_INPUT[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_CELL_STATE (Concatenate) (None, 512)          0           CONTEXT_LSTM[0][1]               \n",
            "                                                                 QUESTION_LSTM[0][1]              \n",
            "__________________________________________________________________________________________________\n",
            "HIDDEN_CELL_STATE (Concatenate) (None, 512)          0           CONTEXT_LSTM[0][2]               \n",
            "                                                                 QUESTION_LSTM[0][2]              \n",
            "__________________________________________________________________________________________________\n",
            "ANSWER_LSTM (LSTM)              [(None, 17, 512), (N 1153024     ANSWER_EMBEDDING[0][0]           \n",
            "                                                                 CONCAT_CELL_STATE[0][0]          \n",
            "                                                                 HIDDEN_CELL_STATE[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "FINAL_OUTPUT (Dense)            (None, 17, 41476)    21277188    ANSWER_LSTM[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 32,173,748\n",
            "Trainable params: 32,173,748\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFboskisjumN",
        "colab_type": "text"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDB4_KhCiqDp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xw8T5_pdOIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcF8YXJ4ZocR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}