{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mrc-LSTM-baseline0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanbasu/aiml-capstone/blob/master/mrc_LSTM_baseline0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "879mz4G6-lRH",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import Libraries, setting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA8sHlEbfYnn",
        "colab_type": "code",
        "outputId": "082bb441-46bd-4100-9a3c-be0dde4d0d19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_po1L9T-V5d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "996f3af4-9d55-44a1-a558-cb164bbd9784"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTDFtsEtV7dY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pprint\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPD8_fgd8PhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will store the params as we go along in this object\n",
        "params = {}\n",
        "project_path = \"/content/drive/My Drive/AIML-MRC-Capstone/datasets/Squad2.0/TrainingDataset/\"\n",
        "model_path = \"/content/drive/My Drive/AIML-MRC-Capstone/models/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09eGk3oWWCaP",
        "colab_type": "text"
      },
      "source": [
        "# Objective - LSTM Baseline 0 \n",
        "\n",
        "*   **Inputs: A question q = {q1, ..., qQ} of length Q and a context paragraph p = {p1, ..., pP } of length P.**\n",
        "*   **Output: An answer span {as, ae} where as is the index of the first answer token in p, ae is the index of the last answer token in p, 0 <= as, ae >= m, and as >= ae.** \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGZp4iLEHE8n",
        "colab_type": "text"
      },
      "source": [
        "## 2. Load Squad Data - Cleaned and curated (output of preprocessing step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFPpoxl7Afls",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_u1n8G3fge_",
        "colab_type": "code",
        "outputId": "74cbf4c6-db31-4448-cdb3-82614e8a689b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "squad_df = pd.read_csv(project_path+'squad_data_final.csv')\n",
        "squad_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "squad_df.head(2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer</th>\n",
              "      <th>plausible_answer_start</th>\n",
              "      <th>plausible_answer</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>clean_context</th>\n",
              "      <th>clean_question</th>\n",
              "      <th>clean_answer</th>\n",
              "      <th>answer_len</th>\n",
              "      <th>answer_end</th>\n",
              "      <th>answer_span</th>\n",
              "      <th>answer_word_span</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>269</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>beyonc giselle knowlescarter bijnse beeyonsay ...</td>\n",
              "      <td>when did beyonce start becoming popular</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>17</td>\n",
              "      <td>286</td>\n",
              "      <td>(269, 286)</td>\n",
              "      <td>(-1, -1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>207</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>beyonc giselle knowlescarter bijnse beeyonsay ...</td>\n",
              "      <td>what areas did beyonce compete in when she was...</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>19</td>\n",
              "      <td>226</td>\n",
              "      <td>(207, 226)</td>\n",
              "      <td>(21, 23)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     title  ... answer_word_span\n",
              "0  Beyoncé  ...         (-1, -1)\n",
              "1  Beyoncé  ...         (21, 23)\n",
              "\n",
              "[2 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U35P3ZvGAQQD",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Create Train, Validation and Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M9hW4xy-Suj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "21815fcd-578f-4825-e354-fe276fa1ac35"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample,shuffle\n",
        "\n",
        "# train = resample(train)\n",
        "# train = shuffle(train,n_samples =50000)\n",
        "\n",
        "train,test = train_test_split(squad_df,test_size = 0.2)\n",
        "train,val = train_test_split(train,test_size=0.25)\n",
        "\n",
        "print(train.shape)\n",
        "print(val.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(78183, 16)\n",
            "(26061, 16)\n",
            "(26062, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sAKjoZ7CIJ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "8db5c3de-baf4-4c9b-f0b2-1532dff2b9f4"
      },
      "source": [
        "train[\"answer_word_span\"] = train[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "test[\"answer_word_span\"] = test[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "val[\"answer_word_span\"] = val[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "train.info()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 78183 entries, 115157 to 106639\n",
            "Data columns (total 16 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   title                   78183 non-null  object \n",
            " 1   context                 78183 non-null  object \n",
            " 2   question                78183 non-null  object \n",
            " 3   id                      78183 non-null  object \n",
            " 4   answer_start            78183 non-null  int64  \n",
            " 5   answer                  51967 non-null  object \n",
            " 6   plausible_answer_start  26215 non-null  float64\n",
            " 7   plausible_answer        26215 non-null  object \n",
            " 8   is_impossible           78183 non-null  bool   \n",
            " 9   clean_context           78183 non-null  object \n",
            " 10  clean_question          78183 non-null  object \n",
            " 11  clean_answer            78183 non-null  object \n",
            " 12  answer_len              78183 non-null  int64  \n",
            " 13  answer_end              78183 non-null  int64  \n",
            " 14  answer_span             78183 non-null  object \n",
            " 15  answer_word_span        78183 non-null  object \n",
            "dtypes: bool(1), float64(1), int64(3), object(11)\n",
            "memory usage: 9.6+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6dcPCGiJrz1",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Build Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxoMIrC6EIgS",
        "colab_type": "code",
        "outputId": "7ccc7f78-2418-4353-dbe4-3f8d05b37ed5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "params['tokenizer_num_words'] = 80000\n",
        "tokenizer = preprocessing.text.Tokenizer(num_words=params['tokenizer_num_words'])\n",
        "\n",
        "# NOTE: tokenizer is been made out of original dataset\n",
        "for text in tqdm([squad_df['clean_context'], squad_df['clean_question']]):  \n",
        "  tokenizer.fit_on_texts(text.values)\n",
        "\n",
        "# total tokenizer words\n",
        "params['vocab_size'] = len(tokenizer.word_index)\n",
        "\n",
        "### SAVE TOKENIZERS\n",
        "with open(model_path + \"tokenizer.pkl\",\"wb\") as f:\n",
        "    pickle.dump(tokenizer,f)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:08<00:00,  4.29s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gXSy_y36IFb",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 Update parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2whUHcmX5PwS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1cd5650a-fba8-4857-cbda-749780e07d75"
      },
      "source": [
        "# From the EDA and historgrams we can conclude that - \n",
        "# 99% percentile of context word length = 285\n",
        "# 99% percentile or question word lengt = 20\n",
        "context_length = 285\n",
        "question_length = 20\n",
        "params['train_shape'] = train.shape\n",
        "params['val_shape'] = val.shape\n",
        "params['test_shape'] = test.shape\n",
        "params['context_length_99'] = context_length # initialize with a high percentile\n",
        "params['question_length_99'] = question_length # initialize with a high percentile\n",
        "params['embedding_size'] = 100\n",
        "params['rnn_units'] = 256\n",
        "params['context_pad_seq'] = 'pre'\n",
        "params['question_pad_seq'] = 'pre'\n",
        "\n",
        "pprint.pprint(params)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'val_shape': (26061, 16),\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lRuVtp_7y51",
        "colab_type": "text"
      },
      "source": [
        "### 2.5 Vectorization / Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEJqzOfW9ARO",
        "colab_type": "text"
      },
      "source": [
        "#### 2.5.1 Integer Sequence of Context and Question "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhuvjYmZ7m6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_clean_context_sequence = tokenizer.texts_to_sequences(train[\"clean_context\"].values)\n",
        "test_clean_context_sequence = tokenizer.texts_to_sequences(test[\"clean_context\"].values)\n",
        "val_clean_context_sequence = tokenizer.texts_to_sequences(val[\"clean_context\"].values)\n",
        "\n",
        "\n",
        "train_clean_question_sequence = tokenizer.texts_to_sequences(train[\"clean_question\"].values)\n",
        "test_clean_question_sequence = tokenizer.texts_to_sequences(test[\"clean_question\"].values)\n",
        "val_clean_question_sequence = tokenizer.texts_to_sequences(val[\"clean_question\"].values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjiPxU_wFbuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u41PKZTFcwm",
        "colab_type": "text"
      },
      "source": [
        "#### 2.5.2 Find Max Sequence length of Context and Question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwh8HGs0Fbp2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "39fd13e7-53f8-4977-ebec-8a1c0b339ffe"
      },
      "source": [
        "# max length of context\n",
        "params['context_max_length'] = max(max(len(txt) for txt in train_clean_context_sequence),\n",
        "                                  max(len(txt) for txt in test_clean_context_sequence),\n",
        "                                  max(len(txt) for txt in val_clean_context_sequence))\n",
        "\n",
        "params['question_max_length'] = max(max(len(txt) for txt in train_clean_question_sequence),\n",
        "                                  max(len(txt) for txt in test_clean_question_sequence),\n",
        "                                  max(len(txt) for txt in val_clean_question_sequence))\n",
        "\n",
        "\n",
        "pprint.pprint(params)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'val_shape': (26061, 16),\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhnBwThFIA0H",
        "colab_type": "text"
      },
      "source": [
        "#### 2.5.3 Padding of the sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGvIf7xU9rIJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c975adcc-97af-4a53-8240-3ca4486ef288"
      },
      "source": [
        "train_context_sequence = preprocessing.sequence.pad_sequences(train_clean_context_sequence,maxlen=params['context_max_length'])\n",
        "\n",
        "print(\"Max context Sequence length is {}\".format(train_context_sequence.shape[1]))\n",
        "\n",
        "test_context_sequence = preprocessing.sequence.pad_sequences(test_clean_context_sequence,maxlen=params['context_max_length'])\n",
        "val_context_sequence = preprocessing.sequence.pad_sequences(val_clean_context_sequence,maxlen=params['context_max_length'])\n",
        "\n",
        "print(train_context_sequence.shape)\n",
        "print(test_context_sequence.shape)\n",
        "print(val_context_sequence.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max context Sequence length is 426\n",
            "(78183, 426)\n",
            "(26062, 426)\n",
            "(26061, 426)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NxpIGIb9p5T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "733ac2a4-7005-4fe1-c585-6f8a9706285a"
      },
      "source": [
        "train_question_sequence = preprocessing.sequence.pad_sequences(train_clean_question_sequence,maxlen=params['question_max_length'])\n",
        "print(\"Max Question Sequence length is {}\".format(train_question_sequence.shape[1]))\n",
        "test_question_sequence = preprocessing.sequence.pad_sequences(test_clean_question_sequence,maxlen=params['question_max_length'])\n",
        "val_question_sequence = preprocessing.sequence.pad_sequences(val_clean_question_sequence,maxlen=params['question_max_length'])\n",
        "\n",
        "print(train_question_sequence.shape)\n",
        "print(test_question_sequence.shape)\n",
        "print(val_question_sequence.shape)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Question Sequence length is 40\n",
            "(78183, 40)\n",
            "(26062, 40)\n",
            "(26061, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgCKNbH6_-yd",
        "colab_type": "text"
      },
      "source": [
        "### 2.5.3 Create Answer Sequence "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv0yC_w8AF4x",
        "colab_type": "text"
      },
      "source": [
        "Encode y_trues as big array consisting of ans_start + ans_end. This has to be used in loss function as well.\n",
        "\n",
        "**y_true = answer_start + answer_end**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHPxRRHuAnSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for train data\n",
        "y_train = []\n",
        "span_ofr = 0;\n",
        "params['train_span_outofrange'] = 0\n",
        "params['test_span_outofrange'] = 0\n",
        "params['val_span_outofrange'] = 0\n",
        "\n",
        "for i in range(len(train)):    \n",
        "    s = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    e = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    start, end = train[\"answer_word_span\"].iloc[i]\n",
        "    # if(start < params['context_length'] and end < params['context_length']):\n",
        "    s[start] = 1\n",
        "    e[end] = 1\n",
        "    # else:\n",
        "    #   span_ofr = span_ofr + 1\n",
        "    #   print(start,end)\n",
        "    y_train.append(np.concatenate((s,e)))    \n",
        "\n",
        "params['train_span_outofrange'] = span_ofr\n",
        "span_ofr = 0;\n",
        "\n",
        "# for test data\n",
        "y_test = []\n",
        "for i in range(len(test)):    \n",
        "    s = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    e = np.zeros(params['context_max_length'],dtype = \"int\")        \n",
        "    start,end = test[\"answer_word_span\"].iloc[i]    \n",
        "    s[start] = 1\n",
        "    e[end] = 1\n",
        "    y_test.append(np.concatenate((s,e)))\n",
        "\n",
        "params['test_span_outofrange'] = span_ofr\n",
        "span_ofr = 0;\n",
        "                \n",
        "# for val data\n",
        "y_val = []\n",
        "for i in range(len(val)):\n",
        "    s = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    e = np.zeros(params['context_max_length'],dtype = \"int\")        \n",
        "    start,end = val[\"answer_word_span\"].iloc[i]    \n",
        "    s[start] = 1\n",
        "    e[end] = 1      \n",
        "    y_val.append(np.concatenate((s,e)))\n",
        "\n",
        "params['val_span_outofrange'] = span_ofr    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgPgnMl0VZCd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "524a9029-ba19-45c3-b732-ee29d22f51c8"
      },
      "source": [
        "print(len(y_train),len(y_train[0]))\n",
        "print(len(y_test),len(y_test[0]))\n",
        "print(len(y_val),len(y_val[0]))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78183 852\n",
            "26062 852\n",
            "26061 852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rZCyBydBZW1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "5b30c826-2ec9-4d2c-f541-f740aad026d0"
      },
      "source": [
        "pprint.pprint(params)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'test_span_outofrange': 0,\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'train_span_outofrange': 0,\n",
            " 'val_shape': (26061, 16),\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTFqLp3iAqjq",
        "colab_type": "text"
      },
      "source": [
        "### Find Max Sequence Length for both the Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di181GmIOSq3",
        "colab_type": "code",
        "outputId": "7c04a141-cfa5-46a5-e61f-70c64ce56248",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# # max length of context\n",
        "# max_context_seq_length= max(len(txt) for txt in context_sequence)\n",
        "# print('max_context_seq_length=',max_context_seq_length)\n",
        "\n",
        "# vocab size of context\n",
        "vocab_size=len(tokenizer.word_index)\n",
        "print('vocab_size=',vocab_size)\n",
        "\n",
        "# max_question_seq_length=max(len(txt) for txt in questions_sequence)\n",
        "# print('max_question_seq_length=',max_question_seq_length)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size= 100850\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkTaGKrnLLFo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From the EDA and historgrams we can conclude that - \n",
        "# 99% percentile of context word length = 285\n",
        "# 99% percentile or question word lengt = 20\n",
        "max_context_seq_length = 285\n",
        "max_question_seq_length = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDzXjXa3MpP0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "309c4f8e-3eba-4dc5-852c-ffebd142cceb"
      },
      "source": [
        "print(squad_df['clean_context'][2000])\n",
        "print(context_sequence[3000])\n",
        "print(squad_df['clean_question'][2000])\n",
        "print(questions_sequence[2000])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "october 21 2008 apple reported 14 21 total revenue fiscal quarter 4 year 2008 came ipods september 9 2009 keynote presentation apple event phil schiller announced total cumulative sales ipods exceeded 220 million continual decline ipod sales since 2009 surprising trend apple corporation apple cfo peter oppenheimer explained june 2009 expect traditional mp3 players decline time cannibalize ipod touch iphone since 2009 companys ipod sales continually decreased every financial quarter 2013 new model introduced onto market\n",
            "[384, 511, 58838, 520, 504, 34, 7655, 7507, 1434, 2740, 3028, 8, 22, 1434, 5566, 48288, 1070, 2828, 4793, 266, 1211, 9403, 2828, 5566, 7837, 3016, 1636, 4027, 697, 1922, 5566, 252, 2271, 4793, 27639, 99, 578, 5016, 142, 1434, 680, 5683, 142, 5566, 7, 58839, 1562, 11769, 4284, 17681, 255, 58840, 1050, 3760, 5209, 24179, 1084, 2740, 3028, 235, 1376, 115, 6740, 179, 1578, 374, 4793, 27639, 578, 8588, 44, 3439, 19, 455, 2820, 574, 193, 578, 87, 13, 11280, 60, 3366, 636]\n",
            "who was chief financial officer of apple in july of 2009\n",
            "[37, 11, 767, 520, 2272, 3, 859, 5, 335, 3, 281]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHh0g7SDLOjL",
        "colab_type": "text"
      },
      "source": [
        "### Padding the sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVRGBRsXLU2U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "c8fa72b4-bdb1-430e-b5dd-3df3ea9f654d"
      },
      "source": [
        "# padding context\n",
        "context_input_data= tf.keras.preprocessing.sequence.pad_sequences(context_sequence, maxlen=max_context_seq_length, padding='pre')\n",
        "# padding question\n",
        "question_input_data=tf.keras.preprocessing.sequence.pad_sequences(questions_sequence, maxlen=max_question_seq_length,padding='pre')\n",
        "\n",
        "print(context_input_data.shape)\n",
        "print(question_input_data.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(130306, 285)\n",
            "(130306, 20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6FSl5UDL_qD",
        "colab_type": "text"
      },
      "source": [
        "# Build the LSTM Model for both Sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-glhG509P5Yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size = 50\n",
        "rnn_units=256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOYjXu-vCZyB",
        "colab_type": "text"
      },
      "source": [
        "### Embedding Layer for Context and Question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_LGPo_6QKtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CONTEXT LSTM\n",
        "# input layer\n",
        "context_input=layers.Input(shape=(max_context_seq_length,),name=\"CONTEXT_INPUT\")\n",
        "# Build Embedding layer and Get Embedding Layer output\n",
        "\n",
        "context_embedding_output=layers.Embedding(input_dim=tokenizer.num_words+1, \n",
        "                                          output_dim=embedding_size, \n",
        "                                          input_length=max_context_seq_length,\n",
        "                                          name=\"CONTEXT_EMBEDDING\")(context_input)\n",
        "\n",
        "\n",
        "# QUESTION LSTM\n",
        "#input layer\n",
        "question_input=layers.Input(shape=(max_question_seq_length,),name=\"QUESTION_INPUT\")\n",
        "#Embedding layer and #Embedding layer output\n",
        "question_embedding_output=layers.Embedding(input_dim=tokenizer.num_words+1, \n",
        "                                           output_dim=embedding_size, \n",
        "                                           input_length=max_question_seq_length,\n",
        "                                           name=\"QUESTION_EMBEDDING\")(question_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rliqd3nmR_3V",
        "colab_type": "text"
      },
      "source": [
        "### Encoder Layer for Context and Question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOC36paDR9W7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN Encoder with LSTM for context\n",
        "c_output,c_h, c_s = layers.LSTM(rnn_units,name='CONTEXT_LSTM', return_state=True)(context_embedding_output)\n",
        "context_states= [c_h, c_s]\n",
        "\n",
        "# RNN Encoder with LSTM for question\n",
        "q_output,q_h, q_s= tf.keras.layers.LSTM(rnn_units,name='QUESTION_LSTM',return_state=True)(question_embedding_output)\n",
        "questions_states = [q_h, q_s]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wE36OJYsgHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d9287eb3-7e08-4e50-85cb-f047c2bb5511"
      },
      "source": [
        "context_states,questions_states"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<tf.Tensor 'CONTEXT_LSTM_1/Identity_1:0' shape=(None, 256) dtype=float32>,\n",
              "  <tf.Tensor 'CONTEXT_LSTM_1/Identity_2:0' shape=(None, 256) dtype=float32>],\n",
              " [<tf.Tensor 'QUESTION_LSTM_1/Identity_1:0' shape=(None, 256) dtype=float32>,\n",
              "  <tf.Tensor 'QUESTION_LSTM_1/Identity_2:0' shape=(None, 256) dtype=float32>])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRcPtZqFXabR",
        "colab_type": "text"
      },
      "source": [
        "### Concat the both RNN LSTM Encoder layers to get merged cell state and hidden state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbyWtSg8XeZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MERGED_cell_state =layers.concatenate([context_states[0],questions_states[0]],name=\"CONCAT_CELL_STATE\")\n",
        "MERGED_hidden_state =layers.concatenate([context_states[1],questions_states[1]],name=\"HIDDEN_CELL_STATE\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGqKHNq3rEPL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f27be10d-038e-4d6d-fa49-b7e14d9a3df9"
      },
      "source": [
        "decoder_initial_state = [MERGED_cell_state,MERGED_hidden_state]\n",
        "decoder_initial_state"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Tensor 'CONCAT_CELL_STATE/Identity:0' shape=(None, 512) dtype=float32>,\n",
              " <tf.Tensor 'HIDDEN_CELL_STATE/Identity:0' shape=(None, 512) dtype=float32>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkqzHf00dSfI",
        "colab_type": "text"
      },
      "source": [
        "# Create Decoder for Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVjDhKLUd9jM",
        "colab_type": "text"
      },
      "source": [
        "# Add  Start and  End tokens to Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEf_85IGsYIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "squad_df['answer_start_end']= '<start>' + squad_df['clean_answer'] + '<end>'\n",
        "squad_df['answer_start_end']=squad_df['answer_start_end'].astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S25hREZeNIW",
        "colab_type": "text"
      },
      "source": [
        "#Tokenize the Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-5MBMrOyTsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answers_tokenize=tf.keras.preprocessing.text.Tokenizer()\n",
        "answers_tokenize.fit_on_texts(squad_df['answer_start_end'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1by-Iw8yteW",
        "colab_type": "code",
        "outputId": "8c3c3991-047c-41fc-899e-319ffffb9422",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Vocab\n",
        "print(len(answers_tokenize.word_index))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sbN9pYI_GqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert sentences to numbers \n",
        "answers_seq = answers_tokenize.texts_to_sequences(squad_df['answer_start_end']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEmnUNaw_jyH",
        "colab_type": "code",
        "outputId": "3ac55ec5-777d-4f52-eca2-b9346471efd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(squad_df['answer_start_end'][2000])\n",
        "print(answers_seq[2000])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start>peter oppenheimer<end>\n",
            "[2, 924, 7781, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24mDpru-e624",
        "colab_type": "text"
      },
      "source": [
        "# Get maximum length and pad the sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL5y5fqqzPav",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4905d04b-a41b-4b1f-b3c6-bb17db05a341"
      },
      "source": [
        "squad_df[squad_df['clean_answer'].str.len() > 200]['answer_start_end']"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3201    <start>that the sudden shift of a huge quantit...\n",
              "Name: answer_start_end, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4R8QCYI_ywa",
        "colab_type": "code",
        "outputId": "1948a90a-e8e3-4e2b-db7b-fabb0a798ea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "max_answers_seq_length=max(len(txt) for txt in squad_df['answer_start_end'])\n",
        "print('max_answers_seq_length=',max_answers_seq_length)\n",
        "\n",
        "answers_vocab_size=len(answers_tokenize.word_index)\n",
        "print('answers_vocab_size=',answers_vocab_size)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_answers_seq_length= 248\n",
            "answers_vocab_size= 41475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IImu_CPiN8gi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From the EDA and historgrams we can conclude that - \n",
        "# 99% percentile of answer word length = 17\n",
        "max_answers_seq_length=17"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqJpe8rTAUHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pad pre\n",
        "answers_input_data= tf.keras.preprocessing.sequence.pad_sequences(answers_seq,maxlen=max_answers_seq_length,padding='pre')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC5nH0YKizaj",
        "colab_type": "text"
      },
      "source": [
        "# Building Decoder Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byD2aHWjBfng",
        "colab_type": "code",
        "outputId": "7b9c1d89-ff38-443e-918c-683894a0f10b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "answers_input_data.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(130306, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRMx8dPPB-jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize array\n",
        "answers_target_data = np.zeros((answers_input_data.shape[0], #number of sentences 130306\n",
        "                                answers_input_data.shape[1])) #number of words in each sentence 248\n",
        "\n",
        "#Shift Target output by one word\n",
        "for i in range(answers_input_data.shape[0]):\n",
        "    for j in range(1,answers_input_data.shape[1]):\n",
        "        answers_target_data[i][j-1] = answers_input_data[i][j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbFQKKXkDGaP",
        "colab_type": "code",
        "outputId": "6e0cac8e-4a54-4e98-ac20-90d91653a239",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(squad_df['answer_start_end'][2000])\n",
        "print(answers_input_data[2000])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start>peter oppenheimer<end>\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    2\n",
            "  924 7781    1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtEzDD_Ai2Rx",
        "colab_type": "text"
      },
      "source": [
        "# Convert Answers to one-hot vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN4VGg0ECo3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Crashing !!\n",
        "answers_target_data_one_hot= np.zeros((answers_input_data.shape[0], #number of sentences\n",
        "                                       answers_input_data.shape[1], #Number of words in each sentence\n",
        "                                       len(answers_tokenize.word_index)+1)) #Vocab size + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkuJpMPXpwS2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "02064c15-5230-4081-8688-eed5b037a1d4"
      },
      "source": [
        "print(answers_input_data.shape)\n",
        "print(len(answers_tokenize.word_index)+1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(130306, 17)\n",
            "41476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhCFV20ZLOmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answers_embedding_size = 50\n",
        "decoder_rnn_units = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xfx0f0mii58",
        "colab_type": "text"
      },
      "source": [
        "# Build Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k38FCLDrJTlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input layer\n",
        "answers_inputs=tf.keras.layers.Input(shape=(max_answers_seq_length,),name=\"ANSWER_INPUT\")\n",
        "\n",
        "#Embedding\n",
        "answers_embedding_output=tf.keras.layers.Embedding(answers_vocab_size+1, answers_embedding_size, name=\"ANSWER_EMBEDDING\")(answers_inputs)\n",
        "\n",
        "#lstm layer\n",
        "answers_lstm= tf.keras.layers.LSTM(decoder_rnn_units,return_sequences=True,name=\"ANSWER_LSTM\", return_state=True)\n",
        "\n",
        "#LSTM Output, State initialization from Encoder states(concat of question and answer)\n",
        "#Output will be all hidden sequences, last 'h' state and last 'c' state\n",
        "\n",
        "output,_,_=answers_lstm(answers_embedding_output,initial_state=decoder_initial_state)\n",
        "\n",
        "#dense layer\n",
        "lstm3_dense= tf.keras.layers.Dense(answers_vocab_size+1,activation='softmax',name=\"FINAL_OUTPUT\")\n",
        "\n",
        "#answer output\n",
        "answer_outputs=lstm3_dense(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14J7GXH2jBcO",
        "colab_type": "text"
      },
      "source": [
        "# Build Model using Encoder ( output of concat) and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFhlppB6qsgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Model([context_input,question_input, answers_inputs],answer_outputs) #Output of the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCTlgD7tu2yn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "234e2ca0-51e4-468d-8af4-b47c2d3af04e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "CONTEXT_INPUT (InputLayer)      [(None, 285)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_INPUT (InputLayer)     [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_EMBEDDING (Embedding)   (None, 285, 50)      4676500     CONTEXT_INPUT[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_EMBEDDING (Embedding)  (None, 20, 50)       2364500     QUESTION_INPUT[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "ANSWER_INPUT (InputLayer)       [(None, 17)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_LSTM (LSTM)             [(None, 256), (None, 314368      CONTEXT_EMBEDDING[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_LSTM (LSTM)            [(None, 256), (None, 314368      QUESTION_EMBEDDING[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "ANSWER_EMBEDDING (Embedding)    (None, 17, 50)       2073800     ANSWER_INPUT[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "CONCAT_CELL_STATE (Concatenate) (None, 512)          0           CONTEXT_LSTM[0][1]               \n",
            "                                                                 QUESTION_LSTM[0][1]              \n",
            "__________________________________________________________________________________________________\n",
            "HIDDEN_CELL_STATE (Concatenate) (None, 512)          0           CONTEXT_LSTM[0][2]               \n",
            "                                                                 QUESTION_LSTM[0][2]              \n",
            "__________________________________________________________________________________________________\n",
            "ANSWER_LSTM (LSTM)              [(None, 17, 512), (N 1153024     ANSWER_EMBEDDING[0][0]           \n",
            "                                                                 CONCAT_CELL_STATE[0][0]          \n",
            "                                                                 HIDDEN_CELL_STATE[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "FINAL_OUTPUT (Dense)            (None, 17, 41476)    21277188    ANSWER_LSTM[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 32,173,748\n",
            "Trainable params: 32,173,748\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFboskisjumN",
        "colab_type": "text"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDB4_KhCiqDp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xw8T5_pdOIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcF8YXJ4ZocR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}