{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mrc-LSTM-baseline0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanbasu/aiml-capstone/blob/master/mrc_LSTM_baseline0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "879mz4G6-lRH",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import Libraries, setting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA8sHlEbfYnn",
        "colab_type": "code",
        "outputId": "87eabca0-ca12-4445-aca1-9c812ac0c17f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_po1L9T-V5d",
        "colab_type": "code",
        "outputId": "c9a4e652-9900-4d30-f488-f7ee9c507915",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTDFtsEtV7dY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "31629991-8d4c-4f69-df0c-2a40603eb9da"
      },
      "source": [
        "import warnings\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pprint\n",
        "from tensorflow.keras.layers import Bidirectional,LSTM,Dense,Dropout,BatchNormalization,Flatten,Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from numpy import array\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPD8_fgd8PhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will store the params as we go along in this object\n",
        "params = {}\n",
        "project_path = \"/content/drive/My Drive/AIML-MRC-Capstone/datasets/Squad2.0/TrainingDataset/\"\n",
        "model_path = \"/content/drive/My Drive/AIML-MRC-Capstone/models/\"\n",
        "tensorboard_logpath  = \"/content/drive/My Drive/AIML-MRC-Capstone/models/tensorboard-logs/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09eGk3oWWCaP",
        "colab_type": "text"
      },
      "source": [
        "# Objective - LSTM Baseline 0 \n",
        "\n",
        "*   **Inputs: A question q = {q1, ..., qQ} of length Q and a context paragraph p = {p1, ..., pP } of length P.**\n",
        "*   **Output: An answer span {as, ae} where as is the index of the first answer token in p, ae is the index of the last answer token in p, 0 <= as, ae >= m, and ae >= as.** \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGZp4iLEHE8n",
        "colab_type": "text"
      },
      "source": [
        "## 2 Load Squad Data - Cleaned and curated (output of preprocessing step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFPpoxl7Afls",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_u1n8G3fge_",
        "colab_type": "code",
        "outputId": "13557a3f-e762-4272-97d7-4551785b25a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "squad_df = pd.read_csv(project_path+'squad_data_final.csv')\n",
        "squad_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "squad_df.head(2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer</th>\n",
              "      <th>plausible_answer_start</th>\n",
              "      <th>plausible_answer</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>clean_context</th>\n",
              "      <th>clean_question</th>\n",
              "      <th>clean_answer</th>\n",
              "      <th>answer_len</th>\n",
              "      <th>answer_end</th>\n",
              "      <th>answer_span</th>\n",
              "      <th>answer_word_span</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>269</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>beyonc giselle knowlescarter bijnse beeyonsay ...</td>\n",
              "      <td>when did beyonce start becoming popular</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>17</td>\n",
              "      <td>286</td>\n",
              "      <td>(269, 286)</td>\n",
              "      <td>(-1, -1)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Beyoncé</td>\n",
              "      <td>Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ b...</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>207</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>beyonc giselle knowlescarter bijnse beeyonsay ...</td>\n",
              "      <td>what areas did beyonce compete in when she was...</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>19</td>\n",
              "      <td>226</td>\n",
              "      <td>(207, 226)</td>\n",
              "      <td>(21, 23)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     title  ... answer_word_span\n",
              "0  Beyoncé  ...         (-1, -1)\n",
              "1  Beyoncé  ...         (21, 23)\n",
              "\n",
              "[2 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U35P3ZvGAQQD",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Create Train, Validation and Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M9hW4xy-Suj",
        "colab_type": "code",
        "outputId": "bbf3e587-5806-4d6f-c988-b664c9d6bcbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample,shuffle\n",
        "\n",
        "# train = resample(train)\n",
        "# train = shuffle(train,n_samples =50000)\n",
        "\n",
        "train,test = train_test_split(squad_df,test_size = 0.2)\n",
        "train,val = train_test_split(train,test_size=0.25)\n",
        "\n",
        "print(train.shape)\n",
        "print(val.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(78183, 16)\n",
            "(26061, 16)\n",
            "(26062, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sAKjoZ7CIJ_",
        "colab_type": "code",
        "outputId": "be00cc43-75cf-40cc-a164-ab456f684031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "train[\"answer_word_span\"] = train[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "test[\"answer_word_span\"] = test[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "val[\"answer_word_span\"] = val[\"answer_word_span\"].apply(lambda x :eval(x))\n",
        "train.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 78183 entries, 24370 to 128876\n",
            "Data columns (total 16 columns):\n",
            " #   Column                  Non-Null Count  Dtype  \n",
            "---  ------                  --------------  -----  \n",
            " 0   title                   78183 non-null  object \n",
            " 1   context                 78183 non-null  object \n",
            " 2   question                78183 non-null  object \n",
            " 3   id                      78183 non-null  object \n",
            " 4   answer_start            78183 non-null  int64  \n",
            " 5   answer                  52080 non-null  object \n",
            " 6   plausible_answer_start  26102 non-null  float64\n",
            " 7   plausible_answer        26102 non-null  object \n",
            " 8   is_impossible           78183 non-null  bool   \n",
            " 9   clean_context           78183 non-null  object \n",
            " 10  clean_question          78183 non-null  object \n",
            " 11  clean_answer            78183 non-null  object \n",
            " 12  answer_len              78183 non-null  int64  \n",
            " 13  answer_end              78183 non-null  int64  \n",
            " 14  answer_span             78183 non-null  object \n",
            " 15  answer_word_span        78183 non-null  object \n",
            "dtypes: bool(1), float64(1), int64(3), object(11)\n",
            "memory usage: 9.6+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6dcPCGiJrz1",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Build Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxoMIrC6EIgS",
        "colab_type": "code",
        "outputId": "0cc8edc9-d98a-4e9b-f03b-04f38c557020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "params['tokenizer_num_words'] = 80000\n",
        "tokenizer = preprocessing.text.Tokenizer(num_words=params['tokenizer_num_words'])\n",
        "\n",
        "# NOTE: tokenizer is been made out of original dataset\n",
        "for text in tqdm([squad_df['clean_context'], squad_df['clean_question']]):  \n",
        "  tokenizer.fit_on_texts(text.values)\n",
        "\n",
        "# total tokenizer words\n",
        "params['vocab_size'] = len(tokenizer.word_index)\n",
        "\n",
        "### SAVE TOKENIZERS\n",
        "with open(model_path + \"tokenizer.pkl\",\"wb\") as f:\n",
        "    pickle.dump(tokenizer,f)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:07<00:00,  3.57s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7fIbj3tatHH",
        "colab_type": "code",
        "outputId": "7f4d96e7-15bf-4ca3-8af7-98e8782d0de2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenizer.word_index['how']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gXSy_y36IFb",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 Update parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2whUHcmX5PwS",
        "colab_type": "code",
        "outputId": "fe2f733e-4667-4067-9dd8-27b963b2eeba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# From the EDA and historgrams we can conclude that - \n",
        "# 99% percentile of context word length = 285\n",
        "# 99% percentile or question word lengt = 20\n",
        "context_length = 285\n",
        "question_length = 20\n",
        "params['train_shape'] = train.shape\n",
        "params['val_shape'] = val.shape\n",
        "params['test_shape'] = test.shape\n",
        "params['context_length_99'] = context_length # initialize with a high percentile\n",
        "params['question_length_99'] = question_length # initialize with a high percentile\n",
        "params['embedding_size'] = 100\n",
        "params['rnn_units'] = 256\n",
        "params['context_pad_seq'] = 'pre'\n",
        "params['question_pad_seq'] = 'pre'\n",
        "\n",
        "pprint.pprint(params)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'val_shape': (26061, 16),\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtBcy2Qwm2-c",
        "colab_type": "text"
      },
      "source": [
        "### 2.5 Custom function for preprocessing of context and question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFr3-S_Gm9FX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "7ec21cf4-5fca-4103-face-ec8da135f80a"
      },
      "source": [
        "# remove unwanted chars\n",
        "# convert to lowercase\n",
        "# remove unwanted spaces\n",
        "# remove stop words\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize \n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "def preprocess_text(corpus, text_lower_case=True, \n",
        "                      special_char_removal=True, stopword_removal=True, remove_digits=False):    \n",
        "    normalized_text = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # lowercase the text    \n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        # remove special characters and\\or digits    \n",
        "        if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them    \n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = remove_special_characters(doc, remove_digits=remove_digits) \n",
        "\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc)\n",
        "\n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)            \n",
        "        normalized_text.append(doc)\n",
        "        \n",
        "    return normalized_text\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    #Using regex\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "def remove_stopwords(text):  \n",
        "    word_tokens = word_tokenize(text) \n",
        "    filtered_sentence = [w for w in word_tokens if not w in stop_words]   \n",
        "    filtered_sentence = [] \n",
        "    for w in word_tokens: \n",
        "        if w not in stop_words: \n",
        "            filtered_sentence.append(w)                 \n",
        "    return ' '.join(filtered_sentence)"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lRuVtp_7y51",
        "colab_type": "text"
      },
      "source": [
        "## 3 Vectorization / Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEJqzOfW9ARO",
        "colab_type": "text"
      },
      "source": [
        "#### 3.1 Integer Sequence of Context and Question "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhuvjYmZ7m6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_clean_context_sequence = tokenizer.texts_to_sequences(train[\"clean_context\"].values)\n",
        "test_clean_context_sequence = tokenizer.texts_to_sequences(test[\"clean_context\"].values)\n",
        "val_clean_context_sequence = tokenizer.texts_to_sequences(val[\"clean_context\"].values)\n",
        "\n",
        "\n",
        "train_clean_question_sequence = tokenizer.texts_to_sequences(train[\"clean_question\"].values)\n",
        "test_clean_question_sequence = tokenizer.texts_to_sequences(test[\"clean_question\"].values)\n",
        "val_clean_question_sequence = tokenizer.texts_to_sequences(val[\"clean_question\"].values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXQNaAxehrDa",
        "colab_type": "code",
        "outputId": "83034390-1ce8-478d-8b21-e6fe86c368fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "train_clean_question_sequence[5:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[71, 11, 33390, 6, 2134],\n",
              " [2, 1743, 1445, 263, 6388, 3343, 288],\n",
              " [2, 29, 16, 2728, 766, 147, 336, 59, 2436],\n",
              " [2, 14, 933, 41, 71, 1, 2870, 98, 1202, 98, 1875, 5123],\n",
              " [39, 9, 2617, 140, 26, 989, 563, 288]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kmRjkqRlExD",
        "colab_type": "code",
        "outputId": "b1a1bb85-a2b0-425d-b95e-04e4c408f262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "train['clean_question'][5:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "72672                         when was mdrtb first observed\n",
              "129071    what electronic charge do cellular molecules have\n",
              "106489    what year did cd players become available for ...\n",
              "76709     what is it called when the variations are text...\n",
              "90219       how many courses does a mandolin commonly have \n",
              "Name: clean_question, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQpM6dGtoGkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u41PKZTFcwm",
        "colab_type": "text"
      },
      "source": [
        "#### 3.2 Find Max Sequence length of Context and Question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwh8HGs0Fbp2",
        "colab_type": "code",
        "outputId": "66e656f7-12b4-49e9-8eed-4737e01a8d06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# max length of context\n",
        "params['context_max_length'] = max(max(len(txt) for txt in train_clean_context_sequence),\n",
        "                                  max(len(txt) for txt in test_clean_context_sequence),\n",
        "                                  max(len(txt) for txt in val_clean_context_sequence))\n",
        "\n",
        "params['question_max_length'] = max(max(len(txt) for txt in train_clean_question_sequence),\n",
        "                                  max(len(txt) for txt in test_clean_question_sequence),\n",
        "                                  max(len(txt) for txt in val_clean_question_sequence))\n",
        "\n",
        "\n",
        "pprint.pprint(params)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'val_shape': (26061, 16),\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhnBwThFIA0H",
        "colab_type": "text"
      },
      "source": [
        "#### 3.3 Padding of the sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGvIf7xU9rIJ",
        "colab_type": "code",
        "outputId": "25d5eef8-eee6-4bc2-8406-352660fc3d03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_context_sequence = preprocessing.sequence.pad_sequences(train_clean_context_sequence,maxlen=params['context_max_length'])\n",
        "test_context_sequence = preprocessing.sequence.pad_sequences(test_clean_context_sequence,maxlen=params['context_max_length'])\n",
        "val_context_sequence = preprocessing.sequence.pad_sequences(val_clean_context_sequence,maxlen=params['context_max_length'])\n",
        "\n",
        "print(train_context_sequence.shape)\n",
        "print(test_context_sequence.shape)\n",
        "print(val_context_sequence.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(78183, 426)\n",
            "(26062, 426)\n",
            "(26061, 426)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NxpIGIb9p5T",
        "colab_type": "code",
        "outputId": "86e7984a-5b7f-4e82-a597-7d8ed1bfb95e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "train_question_sequence = preprocessing.sequence.pad_sequences(train_clean_question_sequence,maxlen=params['question_max_length'])\n",
        "test_question_sequence = preprocessing.sequence.pad_sequences(test_clean_question_sequence,maxlen=params['question_max_length'])\n",
        "val_question_sequence = preprocessing.sequence.pad_sequences(val_clean_question_sequence,maxlen=params['question_max_length'])\n",
        "\n",
        "print(train_question_sequence.shape)\n",
        "print(test_question_sequence.shape)\n",
        "print(val_question_sequence.shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(78183, 40)\n",
            "(26062, 40)\n",
            "(26061, 40)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgCKNbH6_-yd",
        "colab_type": "text"
      },
      "source": [
        "#### 3.4 Create Answer Sequence "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv0yC_w8AF4x",
        "colab_type": "text"
      },
      "source": [
        "Encode y_trues as big array consisting of ans_start + ans_end. This has to be used in loss function as well. We will use the answer_word_span feature\n",
        "\n",
        "**y_true = answer_start + answer_end**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHPxRRHuAnSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for train data\n",
        "y_train = []\n",
        "span_ofr = 0;\n",
        "params['train_span_outofrange'] = 0\n",
        "params['test_span_outofrange'] = 0\n",
        "params['val_span_outofrange'] = 0\n",
        "\n",
        "for i in range(len(train)):    \n",
        "    s = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    e = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    start, end = train[\"answer_word_span\"].iloc[i]    \n",
        "    s[start] = 1\n",
        "    e[end] = 1\n",
        "    y_train.append(np.concatenate((s,e)))    \n",
        "\n",
        "params['train_span_outofrange'] = span_ofr\n",
        "span_ofr = 0;\n",
        "\n",
        "# for test data\n",
        "y_test = []\n",
        "for i in range(len(test)):    \n",
        "    s = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    e = np.zeros(params['context_max_length'],dtype = \"int\")        \n",
        "    start,end = test[\"answer_word_span\"].iloc[i]    \n",
        "    s[start] = 1\n",
        "    e[end] = 1\n",
        "    y_test.append(np.concatenate((s,e)))\n",
        "\n",
        "params['test_span_outofrange'] = span_ofr\n",
        "span_ofr = 0;\n",
        "                \n",
        "# for val data\n",
        "y_val = []\n",
        "for i in range(len(val)):\n",
        "    s = np.zeros(params['context_max_length'],dtype = \"int\")\n",
        "    e = np.zeros(params['context_max_length'],dtype = \"int\")        \n",
        "    start,end = val[\"answer_word_span\"].iloc[i]    \n",
        "    s[start] = 1\n",
        "    e[end] = 1      \n",
        "    y_val.append(np.concatenate((s,e)))\n",
        "\n",
        "params['val_span_outofrange'] = span_ofr    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgPgnMl0VZCd",
        "colab_type": "code",
        "outputId": "b54a8e7b-de34-42db-9f2b-872af9d3e586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(len(y_train),len(y_train[0]))\n",
        "print(len(y_test),len(y_test[0]))\n",
        "print(len(y_val),len(y_val[0]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "78183 852\n",
            "26062 852\n",
            "26061 852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbM8z2AEjxKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(sentence):\n",
        "    \"\"\"\n",
        "    Returns tokenised words.\n",
        "    \"\"\"\n",
        "    return nltk.word_tokenize(sentence)\n",
        "\n",
        "def answer_span(context,ans):\n",
        "    \"\"\"\n",
        "    This funtion returns anwer span start index and end index.\n",
        "    \"\"\"\n",
        "    ans_token = tokenize(ans)\n",
        "    print(ans_token)\n",
        "    con_token = tokenize(context)\n",
        "    print(con_token)\n",
        "    print('21 ',con_token[21])\n",
        "    print('22 ',con_token[22])\n",
        "    print('23 ',con_token[23])\n",
        "    print('24 ',con_token[24])\n",
        "    ans_len = len(ans_token)\n",
        "    \n",
        "    if ans_len!=0 and ans_token[0] in con_token:\n",
        "    \n",
        "        indices = [i for i, x in enumerate(con_token) if x == ans_token[0]]\n",
        "        print(indices)\n",
        "        try:\n",
        "\n",
        "            if(len(indices)>1):\n",
        "                start = [i for i in indices if (con_token[i:i+ans_len] == ans_token) ]\n",
        "                end = start[0] + ans_len - 1\n",
        "                return start[0],end\n",
        "\n",
        "            else:\n",
        "                start = con_token.index(ans_token[0])\n",
        "                end = start + ans_len - 1\n",
        "                return start,end\n",
        "        except:\n",
        "            return -1,-1\n",
        "    else:\n",
        "        return -1,-1\n",
        "\n",
        "def span_to_answer(span, context):\n",
        "  con_token = tokenize(context)  \n",
        "  return ' '.join(con_token[span[0]:span[1]+1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFuJMid-iT7a",
        "colab_type": "text"
      },
      "source": [
        "### 3.5 Check 1 value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXXhftC5kE8A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "757ceabd-60c5-4bd3-dfad-02f2fcaad4e4"
      },
      "source": [
        "index = 1\n",
        "answer_span(train['clean_context'].iloc[index],train['clean_answer'].iloc[index])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['85', '2', 'percent']\n",
            "['state', 'among', 'best', 'prekindergarten', 'education', 'national', 'institute', 'early', 'education', 'research', 'rated', 'first', 'united', 'states', 'regard', 'standards', 'quality', 'access', 'prekindergarten', 'education', '2004', 'calling', 'model', 'early', 'childhood', 'schooling', 'high', 'school', 'dropout', 'rate', 'decreased', '3', '1', '2', '5', 'percent', '2007', '2008', 'oklahoma', 'ranked', 'among', '18', 'states', '3', 'percent', 'less', 'dropout', 'rate', '2004', 'state', 'ranked', '36th', 'nation', 'relative', 'number', 'adults', 'high', 'school', 'diplomas', 'though', '85', '2', 'percent', 'highest', 'rate', 'among', 'southern', 'states']\n",
            "21  calling\n",
            "22  model\n",
            "23  early\n",
            "24  childhood\n",
            "[60]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60, 62)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAXS9YCpiVkn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "6d184246-8f22-42f9-ef36-ea74f4f2d99b"
      },
      "source": [
        "print(\"Ori Cont = \")\n",
        "pprint.pprint(train['context'].iloc[index])\n",
        "print(\"CLean Cont = \")\n",
        "pprint.pprint(train['clean_context'].iloc[index])\n",
        "print('Question = ',train['question'].iloc[index])\n",
        "print('Clean Question = ',train['clean_question'].iloc[index])\n",
        "print('Answer = ',train['answer'].iloc[index])\n",
        "print('Clean Answer = ',train['clean_answer'].iloc[index])\n",
        "print('AS,AE = ',train['answer_word_span'].iloc[index])\n",
        "print(\"encoded \", y_train[index])\n",
        "print(span_to_answer([60,62],train['clean_context'].iloc[index]))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ori Cont = \n",
            "('The state is among the best in pre-kindergarten education, and the National '\n",
            " 'Institute for Early Education Research rated it first in the United States '\n",
            " 'with regard to standards, quality, and access to pre-kindergarten education '\n",
            " 'in 2004, calling it a model for early childhood schooling. High school '\n",
            " 'dropout rate decreased from 3.1 to 2.5 percent between 2007 and 2008 with '\n",
            " 'Oklahoma ranked among 18 other states with 3 percent or less dropout rate. '\n",
            " 'In 2004, the state ranked 36th in the nation for the relative number of '\n",
            " 'adults with high school diplomas, though at 85.2 percent, it had the highest '\n",
            " 'rate among southern states.')\n",
            "CLean Cont = \n",
            "('state among best prekindergarten education national institute early '\n",
            " 'education research rated first united states regard standards quality access '\n",
            " 'prekindergarten education 2004 calling model early childhood schooling high '\n",
            " 'school dropout rate decreased 3 1 2 5 percent 2007 2008 oklahoma ranked '\n",
            " 'among 18 states 3 percent less dropout rate 2004 state ranked 36th nation '\n",
            " 'relative number adults high school diplomas though 85 2 percent highest rate '\n",
            " 'among southern states')\n",
            "Question =  What percent of Oklahomans have graduated high school?\n",
            "Clean Question =  what percent of oklahomans have graduated high school\n",
            "Answer =  85.2 percent\n",
            "Clean Answer =  85 2 percent\n",
            "AS,AE =  (60, 62)\n",
            "encoded  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0]\n",
            "85 2 percent\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rZCyBydBZW1",
        "colab_type": "code",
        "outputId": "78f52cf9-5d90-4369-c2aa-c5234a69702e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "pprint.pprint(params)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'test_span_outofrange': 0,\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'train_span_outofrange': 0,\n",
            " 'val_shape': (26061, 16),\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDzXjXa3MpP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(squad_df['clean_context'][10])\n",
        "print(train_context_sequence[110])\n",
        "print(squad_df['clean_question'][10])\n",
        "print(train_question_sequence[10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sveyxKK5j8q",
        "colab_type": "text"
      },
      "source": [
        "### 3.6 Update and persist params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K170rfN15qGP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3eb661a-8d65-4e23-ec73-c3127db5687e"
      },
      "source": [
        "### SAVE PARAMS\n",
        "# Writing to sample.json \n",
        "\n",
        "def updateparams():\n",
        "  with open(model_path + \"params.json\", \"w\") as p: \n",
        "    p.write(json.dumps(params))\n",
        "  print(\"params.jsop updated and can be found in \", model_path + \"params.json\")  \n",
        "\n",
        "updateparams()"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "params.jsop updated and can be found in  /content/drive/My Drive/AIML-MRC-Capstone/models/params.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYMDmMJNU9mw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def showparams():\n",
        "  pprint.pprint(params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0TSjNkdXzuR",
        "colab_type": "text"
      },
      "source": [
        "### 3.7 Create a common function to generate sequences (useful in prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq97Vq4kXTST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to generate sequences withg appropiate padding\n",
        "def generate_question_context_sequence(context, question):\n",
        "  question_seq = tokenizer.texts_to_sequences(question)\n",
        "  context_seq = tokenizer.texts_to_sequences(context)\n",
        "  question_seq = preprocessing.sequence.pad_sequences(question_seq,maxlen=params['question_max_length'])\n",
        "  context_seq = preprocessing.sequence.pad_sequences(context_seq,maxlen=params['context_max_length'])\n",
        "  return context_seq, question_seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkpMiW1HaqrL",
        "colab_type": "code",
        "outputId": "fd16bb3e-20ac-4fbe-cb8e-72907cf5d42b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print(train[\"clean_question\"].iloc[1])\n",
        "\n",
        "c='state among best prekindergarten education national institute early education research rated first united states regard standards quality access prekindergarten education 2004 calling model early childhood schooling high school dropout rate decreased 3 1 2 5 percent 2007 2008 oklahoma ranked among 18 states 3 percent less dropout rate 2004 state ranked 36th nation relative number adults high school diplomas though 85 2 percent highest rate among southern states'\n",
        "q='what percent of oklahomans have graduated high school'\n",
        "cs,qs = generate_question_context_sequence([c],[q])\n",
        "print(cs.shape,qs.shape)\n",
        "train_question_sequence[1] == qs"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "what percent of oklahomans have graduated high school\n",
            "(1, 426) (1, 40)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nl55IUkVck82",
        "colab_type": "code",
        "outputId": "ce18e39f-0db4-4327-d900-ecffa828c728",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "train_question_sequence[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     2,   373,     3, 28363,\n",
              "         288,  8928,    75,    74], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wt51Se0SchIp",
        "colab_type": "code",
        "outputId": "747501bb-e20f-416f-9161-f6de100bbb0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "q"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     2,   373,     3, 28363,\n",
              "          288,  8928,    75,    74]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaXROQ6Zcd2-",
        "colab_type": "code",
        "outputId": "2c9896f4-fe2a-413e-bf57-130c510c1c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "train_context_sequence[1] == c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "         True,  True,  True]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6FSl5UDL_qD",
        "colab_type": "text"
      },
      "source": [
        "## 4 Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS6fKEvieWrX",
        "colab_type": "text"
      },
      "source": [
        "**Implements a baseline 0 in Deep Learning based approach per our project synopsis. This baseline model uses the following layers **\n",
        "0.   Input layer\n",
        "1.   Embedding Layer\n",
        "2.   List LSTM\n",
        "3.   a custom Bilinear Similarity layer \n",
        "4.   Prediction Layer\n",
        "5.   Output layer \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9JFn3oWiU4y",
        "colab_type": "text"
      },
      "source": [
        "### 4.2 Building Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLGurD9eijTh",
        "colab_type": "text"
      },
      "source": [
        "**For Questions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLqUQHSjeVwq",
        "colab_type": "code",
        "outputId": "71f4dbd0-a388-497d-bf17-f42bbaadde04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# question embedding\n",
        "q_input = layers.Input(shape=(params['question_max_length'],),name=\"QUESTION_INPUT\")\n",
        "q_emb = layers.Embedding(input_dim=params['vocab_size']+1,\n",
        "                  output_dim=params['embedding_size'],\n",
        "                  name=\"QUESTION_EMBEDDING\")(q_input)\n",
        "\n",
        "# encoder \n",
        "q_output = layers.LSTM(units=params['rnn_units'], \n",
        "                     name='QUESTION_LSTM')(q_emb)\n",
        "print(q_output.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eJ2ykC1megw",
        "colab_type": "text"
      },
      "source": [
        "**For Context**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-glhG509P5Yh",
        "colab_type": "code",
        "outputId": "0d6f8824-a988-4bcc-bb29-563188ab16ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "c_input = layers.Input(shape=(params['context_max_length'],),name=\"CONTEXT_INPUT\")\n",
        "\n",
        "# context embedding\n",
        "c_emb = layers.Embedding(input_dim=params['vocab_size']+1,\n",
        "                  output_dim=params['embedding_size'],\n",
        "                  name=\"CONTEXT_EMBEDDING\")(c_input)\n",
        "\n",
        "\n",
        "\n",
        "# exact_match\n",
        "# ex_ = Input(shape=(CON_LEN,2))\n",
        "\n",
        "# # pos tags\n",
        "# pos_ = Input(shape=(CON_LEN,len(tag_to_num)+1))\n",
        "\n",
        "# # term frequency\n",
        "# term_ = Input(shape=(CON_LEN,1)) \n",
        "\n",
        "# concatenate input\n",
        "# concat = concatenate([c_emb,ex_,pos_,term_])\n",
        "\n",
        "c_output = layers.LSTM(params['rnn_units'],\n",
        "                 name='CONTEXT_LSTM',return_sequences=True)(c_emb)\n",
        "\n",
        "print(\"final output to bilinear \",c_output.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "final output to bilinear  (None, 426, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g93054zrp9yp",
        "colab_type": "text"
      },
      "source": [
        "**Bilinear Term**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNsrWO_tpppa",
        "colab_type": "code",
        "outputId": "d7d5773d-44a5-4125-88f2-ed4373cb40b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Reference -- https://github.com/kellywzhang/reading-comprehension/blob/master/attention.py\n",
        "# bilinear term ####\n",
        "print(\"Question context shape \",q_output.shape)\n",
        "print(\"final o/p of context \",c_output.shape)\n",
        "\n",
        "################ start prediction ######################\n",
        "start = layers.Dense(params['rnn_units'])(q_output)\n",
        "\n",
        "# ading time_slice to question (batch_size,1,hidden)\n",
        "# shape (64,128) --> (64,1,128)\n",
        "hidden_start_time_axis = tf.expand_dims(start, -1)\n",
        "\n",
        "# squeeze remooves time slice we added before\n",
        "# final shape = (batch_size,decoder_timesteps)\n",
        "start_ = tf.squeeze(tf.matmul(c_output,hidden_start_time_axis),2)\n",
        "    \n",
        "start_ = tf.nn.softmax(start_,axis = 1)\n",
        "    \n",
        "################ end prediction ######################\n",
        "end = layers.Dense(params['rnn_units'])(q_output)\n",
        "\n",
        "hidden_end_time_axis = tf.expand_dims(end, -1)\n",
        "\n",
        "# squeeze remooves time slice we added before\n",
        "# final shape = (batch_size,decoder_timesteps)\n",
        "end_ = tf.squeeze(tf.matmul(c_output,hidden_end_time_axis),2)\n",
        "end_ = tf.nn.softmax(end_,axis=1)\n",
        "\n",
        "prob_token_span = tf.concat((start_,end_),axis = 1)\n",
        "print(\"Logits shape \",prob_token_span.shape)\n",
        "\n",
        "\n",
        "# logits = BilinearSimilarity(UNITS)(q_cont,c_)\n",
        "# Y_prob = Prediction()(logits)\n",
        "# print(\"Logits shape \",logits.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question context shape  (None, 256)\n",
            "final o/p of context  (None, 426, 256)\n",
            "Logits shape  (None, 852)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibcGeRqUxzKN",
        "colab_type": "text"
      },
      "source": [
        "**Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFbuhviAyX7l",
        "colab_type": "code",
        "outputId": "efb2f883-6725-428f-d204-69d1cd08065d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "####### Prediction ### \n",
        "token_span = 20\n",
        "start_prob = prob_token_span[:,:params['context_max_length']]\n",
        "end_prob = prob_token_span[:,params['context_max_length']:]\n",
        "\n",
        "# do the outer product\n",
        "outer = tf.matmul(tf.expand_dims(start_prob, axis=2),tf.expand_dims(end_prob, axis=1))\n",
        "\n",
        "outer = tf.linalg.band_part(outer, 0, token_span)\n",
        "\n",
        "#print(outer.shape)\n",
        "\n",
        "# start_position will have shape of (batch_size,)\n",
        "start_position = tf.reduce_max(outer, axis=2)\n",
        "#end position will have shape of (batch_size,)\n",
        "end_position = tf.reduce_max(outer, axis=1)\n",
        "\n",
        "y_probab = tf.concat([start_position,end_position],axis=1)\n",
        "\n",
        "print(y_probab.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 852)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLtlPSfLxyzB",
        "colab_type": "text"
      },
      "source": [
        "### 4.3 Custom Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlOkeMveyCWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logits_loss(y_true,logits):\n",
        "    \"\"\"\n",
        "    Custom loss function which minimises log_loss.\n",
        "    Referance https://stackoverflow.com/questions/50063613/add-loss-function-in-keras\n",
        "    \"\"\"\n",
        "    \n",
        "    #y_true = tf.cast(y_true,dtype=tf.int32)\n",
        "    #logits = tf.cast(logits,dtype=tf.float32)\n",
        "    \n",
        "    # breaking the tensor into two half's to get start and end label.\n",
        "    start_label = y_true[:,:params['context_max_length']]\n",
        "    end_label = y_true[:,params['context_max_length']:]\n",
        "    \n",
        "    # braking the logits tensor into start and end part for loss calcultion.\n",
        "    start_logit = logits[:,:params['context_max_length']]\n",
        "    end_logit = logits[:,params['context_max_length']:]\n",
        "    \n",
        "    start_loss = tf.keras.backend.categorical_crossentropy(start_label,start_logit)\n",
        "    end_loss = tf.keras.backend.categorical_crossentropy(end_label,end_logit)\n",
        "    \n",
        "#     start_loss = tf.losses.sparse_softmax_cross_entropy(labels=start_label, logits=start_logit)\n",
        "#     end_loss = tf.losses.sparse_softmax_cross_entropy(labels=end_label, logits=end_logit)\n",
        "    \n",
        "    # as per paer\n",
        "    \n",
        "    loss = start_loss + end_loss\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNiC79-tyLmq",
        "colab_type": "text"
      },
      "source": [
        "### 4.4 Model Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv9aw2EewJbr",
        "colab_type": "code",
        "outputId": "81ac618d-01d3-4e75-e959-55bc3fa7c21a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Model(inputs = [q_input,c_input],outputs =y_probab)\n",
        "model.summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "QUESTION_INPUT (InputLayer)     [(None, 40)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_EMBEDDING (Embedding)  (None, 40, 100)      10085100    QUESTION_INPUT[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_INPUT (InputLayer)      [(None, 426)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "QUESTION_LSTM (LSTM)            (None, 256)          365568      QUESTION_EMBEDDING[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_EMBEDDING (Embedding)   (None, 426, 100)     10085100    CONTEXT_INPUT[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 256)          65792       QUESTION_LSTM[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 256)          65792       QUESTION_LSTM[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "CONTEXT_LSTM (LSTM)             (None, 426, 256)     365568      CONTEXT_EMBEDDING[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims (TensorF [(None, 256, 1)]     0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_1 (Tenso [(None, 256, 1)]     0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2 (Tens [(None, 426, 1)]     0           CONTEXT_LSTM[0][0]               \n",
            "                                                                 tf_op_layer_ExpandDims[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_1 (Te [(None, 426, 1)]     0           CONTEXT_LSTM[0][0]               \n",
            "                                                                 tf_op_layer_ExpandDims_1[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze (TensorFlow [(None, 426)]        0           tf_op_layer_BatchMatMulV2[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Squeeze_1 (TensorFl [(None, 426)]        0           tf_op_layer_BatchMatMulV2_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softmax (TensorFlow [(None, 426)]        0           tf_op_layer_Squeeze[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Softmax_1 (TensorFl [(None, 426)]        0           tf_op_layer_Squeeze_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat (TensorFlowO [(None, 852)]        0           tf_op_layer_Softmax[0][0]        \n",
            "                                                                 tf_op_layer_Softmax_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice (Tens [(None, 426)]        0           tf_op_layer_concat[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_1 (Te [(None, 426)]        0           tf_op_layer_concat[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_2 (Tenso [(None, 426, 1)]     0           tf_op_layer_strided_slice[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_ExpandDims_3 (Tenso [(None, 1, 426)]     0           tf_op_layer_strided_slice_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_BatchMatMulV2_2 (Te [(None, 426, 426)]   0           tf_op_layer_ExpandDims_2[0][0]   \n",
            "                                                                 tf_op_layer_ExpandDims_3[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_MatrixBandPart (Ten [(None, 426, 426)]   0           tf_op_layer_BatchMatMulV2_2[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Max (TensorFlowOpLa [(None, 426)]        0           tf_op_layer_MatrixBandPart[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Max_1 (TensorFlowOp [(None, 426)]        0           tf_op_layer_MatrixBandPart[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_concat_1 (TensorFlo [(None, 852)]        0           tf_op_layer_Max[0][0]            \n",
            "                                                                 tf_op_layer_Max_1[0][0]          \n",
            "==================================================================================================\n",
            "Total params: 21,032,920\n",
            "Trainable params: 21,032,920\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Biepd_j518BQ",
        "colab_type": "text"
      },
      "source": [
        "### 4.5 Model Compile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAqT6LWbXxWE",
        "colab_type": "text"
      },
      "source": [
        "**Tensorboard Logs and Model compilation** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTB5bE8I2Al-",
        "colab_type": "code",
        "outputId": "e59c5503-872d-4c69-fd74-8e8a214c8117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# using tensorboard instance for callbacks\n",
        "from time import time\n",
        "from datetime import datetime\n",
        "from tensorflow.python.keras.callbacks import TensorBoard\n",
        "\n",
        "log_dir = tensorboard_logpath +\"lstm-baseline0\"\n",
        "print('Tensprflow logs ',log_dir)\n",
        "tensorboard = TensorBoard(log_dir=log_dir,histogram_freq=1)\n",
        "\n",
        "# model compilation\n",
        "model.compile(optimizer=\"adamax\",loss=logits_loss,metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensprflow logs  /content/drive/My Drive/AIML-MRC-Capstone/models/tensorboard-logs/lstm-baseline0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WuaPHlU8hnp",
        "colab_type": "text"
      },
      "source": [
        "### 4.6 Generator Function for use in Model.fit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYIgojpb8g82",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Reference \n",
        "def generator_function(length,batch_size = 64,data_type = 'Train'):\n",
        "    \"\"\"\n",
        "    This function is generates batches of data to avoid strain on memory.\n",
        "    \"\"\"\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    flag = True\n",
        "    if data_type == 'Val':\n",
        "        flag = False\n",
        "    n = 0\n",
        "    # loop forever over datapoints.\n",
        "    while 1:\n",
        "        for i in range(length):\n",
        "            n += 1\n",
        "            if flag:\n",
        "                X1.append(train_question_sequence[i])\n",
        "                X2.append(train_context_sequence[i])                \n",
        "                y.append(y_train[i])\n",
        "            else:\n",
        "                X1.append(val_question_sequence[i])\n",
        "                X2.append(val_context_sequence[i])                \n",
        "                y.append(y_val[i])\n",
        "            if n == batch_size:\n",
        "                yield ((array(X1),array(X2)),array(y))\n",
        "                X1,X2, y = list(), list(), list()\n",
        "                n=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlGf9CAe-ZW-",
        "colab_type": "text"
      },
      "source": [
        "### 4.7 Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bO-l9AhD-fPM",
        "colab_type": "code",
        "outputId": "313a959d-c74f-4c69-9c7e-b3e26166b2b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "params['training.epochs']=25\n",
        "params['training.batch_size']=64\n",
        "params['training.train_length']=len(y_train)\n",
        "params['training.val_length']=len(y_val)\n",
        "params['training.train_steps']=params['training.train_length']//params['training.batch_size']\n",
        "params['training.val_steps']=params['training.val_length']//32\n",
        "\n",
        "pprint.pprint(params)\n",
        "\n",
        "### SAVE PARAMS\n",
        "# Writing to sample.json \n",
        "updateparams()"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'test_span_outofrange': 0,\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'train_span_outofrange': 0,\n",
            " 'training.accuracy.score': 0.3729184252935308,\n",
            " 'training.batch_size': 64,\n",
            " 'training.epochs': 25,\n",
            " 'training.macrof1.score': 0.005964796370488863,\n",
            " 'training.microf1.score': 0.2713582316274684,\n",
            " 'training.train_length': 78183,\n",
            " 'training.train_steps': 1221,\n",
            " 'training.val_length': 26061,\n",
            " 'training.val_steps': 814,\n",
            " 'val_shape': (26061, 16),\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 100850}\n",
            "params.jsop updated and can be found in  /content/drive/My Drive/AIML-MRC-Capstone/models/params.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfbn-RnO_EOc",
        "colab_type": "code",
        "outputId": "fef1bf16-45a1-4e23-d188-f63c28d155f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "for i in range(params['training.epochs']):\n",
        "    print(\"Epoch {} start at time \".format(i),datetime.now())\n",
        "    \n",
        "    train_generator = generator_function(params['training.train_length'],\n",
        "                                         params['training.batch_size'])\n",
        "    \n",
        "    val_generator = generator_function(params['training.val_length'],\n",
        "                                       32,\n",
        "                                       \"Val\")\n",
        "    model.fit(x=train_generator, epochs=1, \n",
        "                        steps_per_epoch=params['training.train_steps'],\n",
        "                        verbose=1,\n",
        "                        callbacks=[tensorboard],\n",
        "                        validation_data=val_generator,\n",
        "                        validation_steps=params['training.val_steps'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 start at time  2020-05-31 02:06:29.202658\n",
            "1221/1221 [==============================] - 136s 111ms/step - loss: 8.3468 - accuracy: 0.2365 - val_loss: 8.1262 - val_accuracy: 0.2791\n",
            "Epoch 1 start at time  2020-05-31 02:08:48.412338\n",
            "1113/1221 [==========================>...] - ETA: 10s - loss: 7.3605 - accuracy: 0.3453"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRoOUajk492o",
        "colab_type": "text"
      },
      "source": [
        "### 4.6 Serialize and Persist Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnnjf-hX2FD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(model_path + \"model_epoch_{}.h5\".format(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl0YsArymDdF",
        "colab_type": "text"
      },
      "source": [
        "### 4.7 Load existing models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBwGJN07mEGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(model_path + \"model_epoch_24.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tVohk7bORpH",
        "colab_type": "text"
      },
      "source": [
        "### 4.8 Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7pxr5yHOV6m",
        "colab_type": "text"
      },
      "source": [
        "#### 4.8.1 Eval on Test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VEUBrVHN6c_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_prediction = model.predict([test_question_sequence,test_context_sequence])\n",
        "# print y_prediction[0] should return probabilty of of each index been a start and end token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnVyS4ONOiNO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# y_test was a list changing to numpy array\n",
        "y_test_fixed = np.array(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_XCfB-gOpuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# argmax is used to get the index where the max value in a list appears, and hence \n",
        "# for every index i, we can get the place of start and end token of the max probab\n",
        "start_pred = []\n",
        "end_pred = []\n",
        "for i in range(26062):\n",
        "    start_pred.append(np.argmax(y_prediction[i,:params['context_max_length']]))\n",
        "    end_pred.append(np.argmax(y_prediction[i,params['context_max_length']:]))\n",
        "    \n",
        "# compute for y_test though in this case it the max of 0 and 1 for \n",
        "# the frist half od array size for start, and rest for end\n",
        "start = []\n",
        "end = []\n",
        "for i in range(26062):\n",
        "    start.append(np.argmax(y_test_fixed[i,:params['context_max_length']]))\n",
        "    end.append(np.argmax(y_test_fixed[i,params['context_max_length']:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "138CmIXnR7LS",
        "colab_type": "code",
        "outputId": "cd8241f5-1a45-4e06-e6b4-109a9b14c1bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(start[100:120])\n",
        "print(end[100:120])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[425, 425, 425, 20, 36, 3, 84, 425, 425, 425, 425, 70, 425, 19, 24, 8, 25, 74, 1, 49]\n",
            "[425, 425, 425, 22, 40, 3, 85, 425, 425, 425, 425, 70, 425, 20, 26, 9, 28, 75, 1, 54]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPx7_OKMR8fX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_predicted_new = np.zeros((26062,params['context_max_length']))\n",
        "for i in range(26062):\n",
        "    y_predicted_new[i,start_pred[i]:end_pred[i]+1] = 1\n",
        "    \n",
        "y_test_new = np.zeros((26062,params['context_max_length']))\n",
        "for i in range(26062):\n",
        "    y_test_new[i,start[i]:end[i]+1] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tD4wUQ09SRb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5291b840-d33f-4ab6-f7a4-23cecb5e2b49"
      },
      "source": [
        "len(y_test_new[testindex])"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "426"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v78fhG3Tdhx5",
        "colab_type": "text"
      },
      "source": [
        "#### 4.8.2 Create a common function to predict and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6KDfM25dhZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predit_test(context, question):\n",
        "  # get sequence for context and question\n",
        "  c,q = generate_question_context_sequence([context], [question])  \n",
        "  y_ = model.predict([q,c])    \n",
        "  # # for i in range(26062):\n",
        "  s = np.argmax(y_[0,:params['context_max_length']])\n",
        "  e = np.argmax(y_[0,params['context_max_length']:])\n",
        "  answer = span_to_answer((s,e),context)\n",
        "  print(c.shape,q.shape,y_.shape,s,e,answer)  \n",
        "  # print(s, e)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqKrcmCae0z8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "be2bd2c6-7ca4-4129-c133-8a02536bf801"
      },
      "source": [
        "c = 'Mary went to the bathroom. John is in the playground.John moved to the hallway. John picked up the football.Mary travelled to the office'\n",
        "q = 'Where is Mary?'\n",
        "predit_test(c,q)"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 426) (1, 40) (1, 852) 22 22 to\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHxti5JDP4UD",
        "colab_type": "text"
      },
      "source": [
        "#### 4.8.3 See true vs predic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9qs-no68n52",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        },
        "outputId": "af90b268-e257-40bf-ab4b-2aafb83d9d2b"
      },
      "source": [
        "testindex = 44\n",
        "print(\"Ori Cont = \")\n",
        "pprint.pprint(test['context'].iloc[testindex])\n",
        "print(\"CLean Cont = \")\n",
        "pprint.pprint(test['clean_context'].iloc[testindex])\n",
        "print('Question = ',test['question'].iloc[testindex])\n",
        "print('Clean Question = ',test['clean_question'].iloc[testindex])\n",
        "print('Answer = ',test['answer'].iloc[testindex])\n",
        "print('Clean Answer = ',test['clean_answer'].iloc[testindex])\n",
        "print('AS,AE = ',test['answer_word_span'].iloc[testindex])\n",
        "print('pAS,pAE = ',(start_pred[testindex],end_pred[testindex]))\n",
        "print(\"Predict answer =\",span_to_answer([start_pred[testindex],end_pred[testindex]],test['clean_context'].iloc[testindex]))\n",
        "# print(\"encoded len\", len(y_train[testindex]))\n",
        "# print(\"encoded \", len(y_test[testindex]))\n",
        "print(\"test data encoded \",y_test_new[testindex])\n",
        "print(\"predict data  encoded \",y_predicted_new[testindex])"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ori Cont = \n",
            "('A working group consisting of Leon van de Kerkhof (The Netherlands), Gerhard '\n",
            " 'Stoll (Germany), Leonardo Chiariglione (Italy), Yves-François Dehery '\n",
            " '(France), Karlheinz Brandenburg (Germany) and James D. Johnston (USA) took '\n",
            " 'ideas from ASPEC, integrated the filter bank from Layer 2, added some of '\n",
            " 'their own ideas and created MP3, which was designed to achieve the same '\n",
            " 'quality at 128 kbit/s as MP2 at 192 kbit/s.')\n",
            "CLean Cont = \n",
            "('working group consisting leon van de kerkhof netherlands gerhard stoll '\n",
            " 'germany leonardo chiariglione italy yvesfranois dehery france karlheinz '\n",
            " 'brandenburg germany james johnston usa took ideas aspec integrated filter '\n",
            " 'bank layer 2 added ideas created mp3 designed achieve quality 128 kbits mp2 '\n",
            " '192 kbits')\n",
            "Question =  Where was the filter bank taken from?\n",
            "Clean Question =  where was the filter bank taken from\n",
            "Answer =  Layer 2\n",
            "Clean Answer =  layer 2\n",
            "AS,AE =  (29, 30)\n",
            "pAS,pAE =  (37, 37)\n",
            "Predict answer = quality\n",
            "test data encoded  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "predict data  encoded  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYHNErgrQILC",
        "colab_type": "text"
      },
      "source": [
        "#### 4.8.4 Accuracy Metrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmwzLM3WSriL",
        "colab_type": "code",
        "outputId": "093267a2-da2c-47d5-e226-5ea10b73a603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.metrics import f1_score,accuracy_score,precision_score\n",
        "params['prediction.accuracy.score'] = accuracy_score(y_test_new,y_predicted_new)\n",
        "params['prediction.macrof1.score'] = f1_score(y_test_new,y_predicted_new,average=\"macro\")\n",
        "params['prediction.microf1.score'] = f1_score(y_test_new,y_predicted_new,average=\"micro\")\n",
        "\n",
        "print(\"Micro f1-score on test data is \",params['prediction.microf1.score'])\n",
        "print(\"Macro f1-score on test data is \",params['prediction.macrof1.score'])\n",
        "print(\"Accuracy on test data is \",params['prediction.accuracy.score'])\n",
        "\n",
        "# update params\n",
        "updateparams()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Micro f1-score on test data is  0.2713582316274684\n",
            "Macro f1-score on test data is  0.005964796370488863\n",
            "Accuracy on test data is  0.3729184252935308\n",
            "params.jsop updated and can be found in  /content/drive/My Drive/AIML-MRC-Capstone/models/params.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY02U6dQSCMU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "c51ce625-5ab2-40e4-f048-dceb3cf17059"
      },
      "source": [
        "pprint.pprint(params)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'prediction.accuracy.score': 0.3729184252935308,\n",
            " 'prediction.macrof1.score': 0.005964796370488863,\n",
            " 'prediction.microf1.score': 0.2713582316274684,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'test_span_outofrange': 0,\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'train_span_outofrange': 0,\n",
            " 'training.accuracy.score': 0.3729184252935308,\n",
            " 'training.batch_size': 64,\n",
            " 'training.epochs': 25,\n",
            " 'training.macrof1.score': 0.005964796370488863,\n",
            " 'training.microf1.score': 0.2713582316274684,\n",
            " 'training.train_length': 78183,\n",
            " 'training.train_steps': 1221,\n",
            " 'training.val_length': 26061,\n",
            " 'training.val_steps': 814,\n",
            " 'val_shape': (26061, 16),\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5T9BSNgS-uG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "summary = PrettyTable()\n",
        "summary.title = \"Test vs Prediction\"\n",
        "summary.field_names = [\"ID\",\n",
        "                       \"Clean Question\",\n",
        "                       \"Clean Context\",\n",
        "                       \"True Answer\",\n",
        "                       \"True AS and AE\",\n",
        "                       \"Predict Answer\",\n",
        "                       \"Predict AS and AE\"]\n",
        "result_df = pd.DataFrame(columns=summary.field_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI40rd0bQZgn",
        "colab_type": "text"
      },
      "source": [
        "#### 4.8.5 Store the result to build more meterics "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TdUqImSC1_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in tqdm(range(26062)):  \n",
        "  values = [test['id'].iloc[i], \n",
        "            test['clean_question'].iloc[i], \n",
        "            test['clean_context'].iloc[i], \n",
        "            test['clean_answer'].iloc[i], \n",
        "            test['answer_word_span'].iloc[i],\n",
        "            span_to_answer([start_pred[i],end_pred[i]],test['clean_context'].iloc[i]),\n",
        "            (start_pred[i],end_pred[i])]\n",
        "  zipped = zip(summary.field_names, values)\n",
        "  a_dictionary = dict(zipped)\n",
        "  result_df = result_df.append(a_dictionary,ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TySMZHmbJB1r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "4448df1a-fd32-4bf5-c3c0-8d9ad90a05d4"
      },
      "source": [
        "result_df.to_csv(model_path + \"results.csv\")  \n",
        "result_df.head()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Clean Question</th>\n",
              "      <th>Clean Context</th>\n",
              "      <th>True Answer</th>\n",
              "      <th>True AS and AE</th>\n",
              "      <th>Predict Answer</th>\n",
              "      <th>Predict AS and AE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>571a5a3d10f8ca1400304fed</td>\n",
              "      <td>who made rough calculations and implied that a...</td>\n",
              "      <td>estimated 11th century ashkenazi jews composed...</td>\n",
              "      <td>sergio dellapergola</td>\n",
              "      <td>(42, 43)</td>\n",
              "      <td></td>\n",
              "      <td>(425, 425)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5ad1854a645df0001a2d1e85</td>\n",
              "      <td>what does not have electric current running th...</td>\n",
              "      <td>incandescent light bulb incandescent lamp inca...</td>\n",
              "      <td>IMPOSSIBLE</td>\n",
              "      <td>(-1, -1)</td>\n",
              "      <td></td>\n",
              "      <td>(425, 425)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>57310a09e6313a140071cb8c</td>\n",
              "      <td>what are air force officer promotions overseen by</td>\n",
              "      <td>air force officer promotions governed defense ...</td>\n",
              "      <td>defense officer personnel management act of 1980</td>\n",
              "      <td>(5, 11)</td>\n",
              "      <td>air</td>\n",
              "      <td>(0, 0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570c7e1eb3d812140066d215</td>\n",
              "      <td>what type of supporters did barcelona attract</td>\n",
              "      <td>traditionally espanyol seen vast majority barc...</td>\n",
              "      <td>catalonias new arrivals</td>\n",
              "      <td>(26, 28)</td>\n",
              "      <td>espanyol</td>\n",
              "      <td>(36, 36)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5727cab93acd2414000dec75</td>\n",
              "      <td>who must sign a white ticket</td>\n",
              "      <td>opposite show rip substandard work sometimes t...</td>\n",
              "      <td>all his teachers</td>\n",
              "      <td>(-1, -1)</td>\n",
              "      <td></td>\n",
              "      <td>(425, 425)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                         ID  ... Predict AS and AE\n",
              "0  571a5a3d10f8ca1400304fed  ...        (425, 425)\n",
              "1  5ad1854a645df0001a2d1e85  ...        (425, 425)\n",
              "2  57310a09e6313a140071cb8c  ...            (0, 0)\n",
              "3  570c7e1eb3d812140066d215  ...          (36, 36)\n",
              "4  5727cab93acd2414000dec75  ...        (425, 425)\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCKNvmOaQtx_",
        "colab_type": "text"
      },
      "source": [
        "## 5 More Evaluations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2emUc95Q53a",
        "colab_type": "text"
      },
      "source": [
        "**Read the result dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geCm-V3SQ0v7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_df = result_df.read_csv(model_path + \"results.csv\")  \n",
        "result_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz6uetkTRLrz",
        "colab_type": "text"
      },
      "source": [
        "### 5.1 EM (Exact Match)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uTD7fZZUhyw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "f076b5ac-b4af-497d-b7c5-31c30d90d01c"
      },
      "source": [
        "result_df[result_df['Predict Answer'] == result_df['True Answer']]"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>Clean Question</th>\n",
              "      <th>Clean Context</th>\n",
              "      <th>True Answer</th>\n",
              "      <th>True AS and AE</th>\n",
              "      <th>Predict Answer</th>\n",
              "      <th>Predict AS and AE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>56d1f2b4e7d4791d009025b1</td>\n",
              "      <td>in what sutras are the buddha dharma and sangh...</td>\n",
              "      <td>mahayana buddha tends viewed merely human eart...</td>\n",
              "      <td>mahayana</td>\n",
              "      <td>(0, 0)</td>\n",
              "      <td>mahayana</td>\n",
              "      <td>(0, 0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>572f876aa23a5019007fc6ef</td>\n",
              "      <td>what is a large domain of prokaryotic microorg...</td>\n",
              "      <td>bacteria ibktri singular bacterium constitute ...</td>\n",
              "      <td>bacteria</td>\n",
              "      <td>(0, 0)</td>\n",
              "      <td>bacteria</td>\n",
              "      <td>(19, 19)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>5731812105b4da19006bd1f3</td>\n",
              "      <td>first nations and inuit are labels for what pe...</td>\n",
              "      <td>aboriginal peoples canada comprise first natio...</td>\n",
              "      <td>aboriginal</td>\n",
              "      <td>(0, 0)</td>\n",
              "      <td>aboriginal</td>\n",
              "      <td>(0, 0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>547</th>\n",
              "      <td>570d7b36fed7b91900d461b6</td>\n",
              "      <td>which french minister traveled to versailles t...</td>\n",
              "      <td>28 january 1871 government national defence ba...</td>\n",
              "      <td>favre</td>\n",
              "      <td>(23, 23)</td>\n",
              "      <td>favre</td>\n",
              "      <td>(23, 23)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>5735d85d012e2f140011a0b5</td>\n",
              "      <td>what is it called to kill or trap an animal</td>\n",
              "      <td>hunting practice killing trapping animal pursu...</td>\n",
              "      <td>hunting</td>\n",
              "      <td>(0, 0)</td>\n",
              "      <td>hunting</td>\n",
              "      <td>(0, 0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25858</th>\n",
              "      <td>5731cae1b9d445190005e563</td>\n",
              "      <td>when did the supreme court address the issue o...</td>\n",
              "      <td>1962 supreme court addressed issue officiallys...</td>\n",
              "      <td>1962</td>\n",
              "      <td>(0, 0)</td>\n",
              "      <td>1962</td>\n",
              "      <td>(17, 17)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25977</th>\n",
              "      <td>57319878e17f3d1400422258</td>\n",
              "      <td>what god was the father of romulus and remus</td>\n",
              "      <td>myth trojan founding greek influence reconcile...</td>\n",
              "      <td>mars</td>\n",
              "      <td>(49, 49)</td>\n",
              "      <td>mars</td>\n",
              "      <td>(49, 49)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25979</th>\n",
              "      <td>57264dedf1498d1400e8db8e</td>\n",
              "      <td>when was the berne convention</td>\n",
              "      <td>german equivalent used founding north german c...</td>\n",
              "      <td>1886</td>\n",
              "      <td>(28, 28)</td>\n",
              "      <td>1886</td>\n",
              "      <td>(28, 28)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26015</th>\n",
              "      <td>56dfa9c67aa994140058dfb4</td>\n",
              "      <td>what year was it discovered that petroleum cou...</td>\n",
              "      <td>1849 dr abraham gesner canadian geologist devi...</td>\n",
              "      <td>1849</td>\n",
              "      <td>(0, 0)</td>\n",
              "      <td>1849</td>\n",
              "      <td>(0, 0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26058</th>\n",
              "      <td>57266b70708984140094c568</td>\n",
              "      <td>how old was victorias oldest daughter when she...</td>\n",
              "      <td>eleven days orsinis assassination attempt fran...</td>\n",
              "      <td>17</td>\n",
              "      <td>(30, 30)</td>\n",
              "      <td>17</td>\n",
              "      <td>(30, 30)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>164 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                             ID  ... Predict AS and AE\n",
              "39     56d1f2b4e7d4791d009025b1  ...            (0, 0)\n",
              "260    572f876aa23a5019007fc6ef  ...          (19, 19)\n",
              "375    5731812105b4da19006bd1f3  ...            (0, 0)\n",
              "547    570d7b36fed7b91900d461b6  ...          (23, 23)\n",
              "568    5735d85d012e2f140011a0b5  ...            (0, 0)\n",
              "...                         ...  ...               ...\n",
              "25858  5731cae1b9d445190005e563  ...          (17, 17)\n",
              "25977  57319878e17f3d1400422258  ...          (49, 49)\n",
              "25979  57264dedf1498d1400e8db8e  ...          (28, 28)\n",
              "26015  56dfa9c67aa994140058dfb4  ...            (0, 0)\n",
              "26058  57266b70708984140094c568  ...          (30, 30)\n",
              "\n",
              "[164 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm4YmnfQNqQC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ematch = result_df[result_df['Predict Answer'] == result_df['True Answer']].shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BF44727PN1dq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07ae19c9-de5b-490b-db83-1448bfd9e3d1"
      },
      "source": [
        "params['prediction.em.score'] = ematch / params['test_shape'][0]\n",
        "updateparams()"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "params.jsop updated and can be found in  /content/drive/My Drive/AIML-MRC-Capstone/models/params.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbO5yYzRTTyu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "540afafe-9314-48fc-c4f8-8b33a6231d2f"
      },
      "source": [
        "showparams()"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'context_length_99': 285,\n",
            " 'context_max_length': 426,\n",
            " 'context_pad_seq': 'pre',\n",
            " 'embedding_size': 100,\n",
            " 'prediction.accuracy.score': 0.3729184252935308,\n",
            " 'prediction.em.score': 0.006292686670247871,\n",
            " 'prediction.macrof1.score': 0.005964796370488863,\n",
            " 'prediction.microf1.score': 0.2713582316274684,\n",
            " 'question_length_99': 20,\n",
            " 'question_max_length': 40,\n",
            " 'question_pad_seq': 'pre',\n",
            " 'results.em.score': 0.006292686670247871,\n",
            " 'rnn_units': 256,\n",
            " 'test_shape': (26062, 16),\n",
            " 'test_span_outofrange': 0,\n",
            " 'tokenizer_num_words': 80000,\n",
            " 'train_shape': (78183, 16),\n",
            " 'train_span_outofrange': 0,\n",
            " 'training.accuracy.score': 0.3729184252935308,\n",
            " 'training.batch_size': 64,\n",
            " 'training.epochs': 25,\n",
            " 'training.macrof1.score': 0.005964796370488863,\n",
            " 'training.microf1.score': 0.2713582316274684,\n",
            " 'training.train_length': 78183,\n",
            " 'training.train_steps': 1221,\n",
            " 'training.val_length': 26061,\n",
            " 'training.val_steps': 814,\n",
            " 'val_shape': (26061, 16),\n",
            " 'val_span_outofrange': 0,\n",
            " 'vocab_size': 100850}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipfmu14-VDCJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0N4ZrYofWyVQ",
        "colab_type": "text"
      },
      "source": [
        "### 5.2 Basic factoid QA with single supporting fact"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxoSChGjWzpU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}