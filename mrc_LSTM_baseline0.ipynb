{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mrc-LSTM-baseline0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanbasu/aiml-capstone/blob/master/mrc_LSTM_baseline0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DA8sHlEbfYnn",
        "colab_type": "code",
        "outputId": "9aab83fb-8f6a-4289-bd65-aac717ceda3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGZp4iLEHE8n",
        "colab_type": "text"
      },
      "source": [
        "# Import Libraries and Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_u1n8G3fge_",
        "colab_type": "code",
        "outputId": "1aed8d2d-5bdd-48a8-a190-dc3c0125112d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import preprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "project_path = \"/content/drive/My Drive/AIML-MRC-Capstone/datasets/Squad2.0/TrainingDataset/\"\n",
        "\n",
        "squad_df = pd.read_csv(project_path+'squad_data_final.csv')\n",
        "squad_df.drop('Unnamed: 0',axis=1,inplace=True)\n",
        "squad_df.tail(5)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer</th>\n",
              "      <th>plausible_answer_start</th>\n",
              "      <th>plausible_answer</th>\n",
              "      <th>is_impossible</th>\n",
              "      <th>clean_context</th>\n",
              "      <th>clean_question</th>\n",
              "      <th>clean_answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>130301</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>Physics has broadly agreed on the definition o...</td>\n",
              "      <td>5a7e070b70df9f001a875439</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>485.0</td>\n",
              "      <td>matter</td>\n",
              "      <td>True</td>\n",
              "      <td>term matter used throughout physics bewilderin...</td>\n",
              "      <td>physics has broadly agreed on the definition o...</td>\n",
              "      <td>IMPOSSIBLE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130302</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>Who coined the term partonic matter?</td>\n",
              "      <td>5a7e070b70df9f001a87543a</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>327.0</td>\n",
              "      <td>Alfvén</td>\n",
              "      <td>True</td>\n",
              "      <td>term matter used throughout physics bewilderin...</td>\n",
              "      <td>who coined the term partonic matter</td>\n",
              "      <td>IMPOSSIBLE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130303</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>What is another name for anti-matter?</td>\n",
              "      <td>5a7e070b70df9f001a87543b</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>350.0</td>\n",
              "      <td>Gk. common matter</td>\n",
              "      <td>True</td>\n",
              "      <td>term matter used throughout physics bewilderin...</td>\n",
              "      <td>what is another name for antimatter</td>\n",
              "      <td>IMPOSSIBLE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130304</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>Matter usually does not need to be used in con...</td>\n",
              "      <td>5a7e070b70df9f001a87543c</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>529.0</td>\n",
              "      <td>a specifying modifier</td>\n",
              "      <td>True</td>\n",
              "      <td>term matter used throughout physics bewilderin...</td>\n",
              "      <td>matter usually does not need to be used in con...</td>\n",
              "      <td>IMPOSSIBLE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130305</th>\n",
              "      <td>Matter</td>\n",
              "      <td>The term \"matter\" is used throughout physics i...</td>\n",
              "      <td>What field of study has a variety of unusual c...</td>\n",
              "      <td>5a7e070b70df9f001a87543d</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>37.0</td>\n",
              "      <td>physics</td>\n",
              "      <td>True</td>\n",
              "      <td>term matter used throughout physics bewilderin...</td>\n",
              "      <td>what field of study has a variety of unusual c...</td>\n",
              "      <td>IMPOSSIBLE</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         title  ... clean_answer\n",
              "130301  Matter  ...   IMPOSSIBLE\n",
              "130302  Matter  ...   IMPOSSIBLE\n",
              "130303  Matter  ...   IMPOSSIBLE\n",
              "130304  Matter  ...   IMPOSSIBLE\n",
              "130305  Matter  ...   IMPOSSIBLE\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK5ZkxKeHbu5",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "855WpIGFHfMr",
        "colab_type": "text"
      },
      "source": [
        "# Lets use 2 LSTM network for context and question and use a concat layer to merge. y is the answer "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6dcPCGiJrz1",
        "colab_type": "text"
      },
      "source": [
        "# Step 1 Tokenize and Vectorize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcNCdQgK_sE9",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizing context and question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cCWphXNKoHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "context_tokenize = preprocessing.text.Tokenizer()\n",
        "context_tokenize.fit_on_texts(squad_df['clean_context']) #Fit it on clean_context\n",
        "\n",
        "questions_tokenize= preprocessing.text.Tokenizer()\n",
        "questions_tokenize.fit_on_texts(squad_df['clean_question'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2iQ-9ZZNWhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert the context text to indexes\n",
        "context_sequence= context_tokenize.texts_to_sequences(squad_df['clean_context'])\n",
        "# convert the questions to indexes\n",
        "questions_sequence= questions_tokenize.texts_to_sequences(squad_df['clean_question'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn5paTsLONKE",
        "colab_type": "code",
        "outputId": "8a703b77-8d83-42fd-a0a5-b252980ee9eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(squad_df['clean_context'][2000])\n",
        "print(context_sequence[3000])\n",
        "print(squad_df['clean_question'][2000])\n",
        "print(questions_sequence[2000])"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "october 21 2008 apple reported 14 21 total revenue fiscal quarter 4 year 2008 came ipods september 9 2009 keynote presentation apple event phil schiller announced total cumulative sales ipods exceeded 220 million continual decline ipod sales since 2009 surprising trend apple corporation apple cfo peter oppenheimer explained june 2009 expect traditional mp3 players decline time cannibalize ipod touch iphone since 2009 companys ipod sales continually decreased every financial quarter 2013 new model introduced onto market\n",
            "[360, 464, 56909, 483, 454, 21, 7582, 7198, 1401, 2632, 2957, 4, 14, 1401, 5514, 56910, 1051, 2806, 4752, 249, 1260, 9282, 2806, 5514, 7725, 2752, 1600, 3891, 692, 1892, 5514, 204, 2129, 4752, 26680, 78, 578, 5194, 118, 1401, 618, 5590, 118, 5514, 2, 56911, 1482, 11598, 4122, 16977, 229, 56912, 962, 3628, 4968, 23304, 1030, 2632, 2957, 207, 1339, 92, 6491, 162, 1512, 337, 4752, 26680, 578, 8296, 29, 3198, 10, 397, 2728, 525, 170, 578, 65, 8, 10870, 51, 3279, 584]\n",
            "who was chief financial officer of apple in july of 2009\n",
            "[10, 6, 886, 620, 2250, 3, 762, 4, 1224, 3, 320]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTFqLp3iAqjq",
        "colab_type": "text"
      },
      "source": [
        "### Find Max Sequence Length for both the Sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "di181GmIOSq3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "53f5ab29-241f-4ee1-ed79-80dd51c7ac12"
      },
      "source": [
        "# max length of context\n",
        "max_context_seq_length= max(len(txt) for txt in context_sequence)\n",
        "print('max_context_seq_length=',max_context_seq_length)\n",
        "\n",
        "# vocab size of context\n",
        "context_vocab_size=len(context_tokenize.word_index)\n",
        "print('context_vocab_size=',context_vocab_size)\n",
        "\n",
        "max_question_seq_length=max(len(txt) for txt in questions_sequence)\n",
        "print('max_question_seq_length=',max_question_seq_length)\n",
        "\n",
        "# vocab size of questions\n",
        "questions_vocab_size=len(questions_tokenize.word_index)\n",
        "print('questions_vocab_size=',questions_vocab_size)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_context_seq_length= 426\n",
            "context_vocab_size= 93529\n",
            "max_question_seq_length= 40\n",
            "questions_vocab_size= 47289\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHh0g7SDLOjL",
        "colab_type": "text"
      },
      "source": [
        "### Padding the sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVRGBRsXLU2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# padding context\n",
        "context_input_data= tf.keras.preprocessing.sequence.pad_sequences(context_sequence, maxlen=max_context_seq_length, padding='pre')\n",
        "# padding question\n",
        "question_input_data=tf.keras.preprocessing.sequence.pad_sequences(questions_sequence, maxlen=max_question_seq_length,padding='pre')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6FSl5UDL_qD",
        "colab_type": "text"
      },
      "source": [
        "# Build the LSTM Model for both Sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-glhG509P5Yh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size = 50\n",
        "rnn_units=100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOYjXu-vCZyB",
        "colab_type": "text"
      },
      "source": [
        "### LSTM for Context and Question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_LGPo_6QKtT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CONTEXT LSTM\n",
        "# input layer\n",
        "context_input=layers.Input(shape=(max_context_seq_length,))\n",
        "# Build Embedding layer and Get Embedding Layer output\n",
        "context_embedding_output=layers.Embedding(context_vocab_size+1, embedding_size)(context_input)\n",
        "#LSTM layer and its output\n",
        "# CONTEXT_LSTM= layers.LSTM(rnn_units,name='CONTEXT_LSTM')(context_embedding_output)\n",
        "\n",
        "c_output,c_h, c_s = layers.LSTM(rnn_units,name='CONTEXT_LSTM', return_state=True)(context_embedding_output)\n",
        "\n",
        "#build a list to feed for concatenation\n",
        "context_states= [c_h, c_s]\n",
        "\n",
        "# QUESTION LSTM\n",
        "#input layer\n",
        "question_input=layers.Input(shape=(max_question_seq_length,))\n",
        "#Embedding layer and #Embedding layer output\n",
        "question_embedding_output=layers.Embedding(input_dim=questions_vocab_size+1, output_dim=embedding_size)(question_input)\n",
        "#LSTM2 layer \n",
        "# QUESTION_LSTM= tf.keras.layers.LSTM(rnn_units,name='QUESTION_LSTM')(question_embedding_output)\n",
        "q_output,q_h, q_s= tf.keras.layers.LSTM(rnn_units,name='QUESTION_LSTM',return_state=True)(question_embedding_output)\n",
        "context_questions= [q_h, q_s]\n",
        "\n",
        "\n",
        "# print(CONTEXT_LSTM)\n",
        "# print(QUESTION_LSTM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wE36OJYsgHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0f16d5c3-96fb-405e-f913-781b70785d59"
      },
      "source": [
        "context_states,context_questions"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<tf.Tensor 'CONTEXT_LSTM_6/Identity_1:0' shape=(None, 256) dtype=float32>,\n",
              "  <tf.Tensor 'CONTEXT_LSTM_6/Identity_2:0' shape=(None, 256) dtype=float32>],\n",
              " [<tf.Tensor 'QUESTION_LSTM_6/Identity_1:0' shape=(None, 256) dtype=float32>,\n",
              "  <tf.Tensor 'QUESTION_LSTM_6/Identity_2:0' shape=(None, 256) dtype=float32>])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRcPtZqFXabR",
        "colab_type": "text"
      },
      "source": [
        "# Concat the two LSTM layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbyWtSg8XeZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MERGED_cell_state =layers.concatenate([context_states[0],context_questions[0]])\n",
        "MERGED_hidden_state =layers.concatenate([context_states[1],context_questions[1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGqKHNq3rEPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_initial_state = [MERGED_cell_state,MERGED_hidden_state]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkqzHf00dSfI",
        "colab_type": "text"
      },
      "source": [
        "# Create Decoder for Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVjDhKLUd9jM",
        "colab_type": "text"
      },
      "source": [
        "# Add  Start and  End tokens to Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEf_85IGsYIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "squad_df['answer_start_end']= '<start>' + squad_df['clean_answer'] + '<end>'\n",
        "squad_df['answer_start_end']=squad_df['answer_start_end'].astype(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S25hREZeNIW",
        "colab_type": "text"
      },
      "source": [
        "#Tokenize the Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-5MBMrOyTsC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answers_tokenize=tf.keras.preprocessing.text.Tokenizer()\n",
        "answers_tokenize.fit_on_texts(squad_df['answer_start_end'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1by-Iw8yteW",
        "colab_type": "code",
        "outputId": "f5e10d35-4974-4c94-beda-5cd478a87603",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Vocab\n",
        "print(len(answers_tokenize.word_index))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sbN9pYI_GqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Convert sentences to numbers \n",
        "answers_seq = answers_tokenize.texts_to_sequences(squad_df['answer_start_end']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEmnUNaw_jyH",
        "colab_type": "code",
        "outputId": "4f8d6522-1a1f-4fcc-cf73-392d740800c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(squad_df['answer_start_end'][2000])\n",
        "print(answers_seq[2000])"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start>peter oppenheimer<end>\n",
            "[2, 924, 7781, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24mDpru-e624",
        "colab_type": "text"
      },
      "source": [
        "# Get maximum length and pad the sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4R8QCYI_ywa",
        "colab_type": "code",
        "outputId": "51e9ea9f-9d04-4de8-bc27-a285d543f47e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "max_answers_seq_length=max(len(txt) for txt in squad_df['answer_start_end'])\n",
        "print('max_answers_seq_length=',max_answers_seq_length)\n",
        "\n",
        "answers_vocab_size=len(answers_tokenize.word_index)\n",
        "print('answers_vocab_size=',answers_vocab_size)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "max_answers_seq_length= 248\n",
            "answers_vocab_size= 41475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqJpe8rTAUHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pad pre\n",
        "answers_input_data= tf.keras.preprocessing.sequence.pad_sequences(answers_seq,maxlen=max_answers_seq_length,padding='pre')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC5nH0YKizaj",
        "colab_type": "text"
      },
      "source": [
        "# Building Decoder Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byD2aHWjBfng",
        "colab_type": "code",
        "outputId": "bf8e4190-54f2-4018-86f4-74351154ab11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "answers_input_data.shape"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(130306, 248)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRMx8dPPB-jB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize array\n",
        "answers_target_data = np.zeros((answers_input_data.shape[0], #number of sentences 130306\n",
        "                                answers_input_data.shape[1])) #number of words in each sentence 248\n",
        "\n",
        "#Shift Target output by one word\n",
        "for i in range(answers_input_data.shape[0]):\n",
        "    for j in range(1,answers_input_data.shape[1]):\n",
        "        answers_target_data[i][j-1] = answers_input_data[i][j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbFQKKXkDGaP",
        "colab_type": "code",
        "outputId": "e3bf0e45-788a-4c60-da32-cde6894124ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "print(squad_df['answer_start_end'][2000])\n",
        "print(answers_input_data[2000])"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start>peter oppenheimer<end>\n",
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    2  924 7781    1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtEzDD_Ai2Rx",
        "colab_type": "text"
      },
      "source": [
        "# Convert Answers to one-hot vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN4VGg0ECo3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Crashing !!\n",
        "answers_target_data_one_hot= np.zeros((answers_input_data.shape[0], #number of sentences\n",
        "                                       answers_input_data.shape[1], #Number of words in each sentence\n",
        "                                       len(answers_tokenize.word_index)+1)) #Vocab size + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkuJpMPXpwS2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7c2aa6bf-e256-413f-f5f4-b2a2942391be"
      },
      "source": [
        "print(answers_input_data.shape)\n",
        "print(len(answers_tokenize.word_index)+1)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(130306, 248)\n",
            "41476\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhCFV20ZLOmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "answers_embedding_size = 50\n",
        "rnn_units = 256"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xfx0f0mii58",
        "colab_type": "text"
      },
      "source": [
        "# Build Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k38FCLDrJTlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#input layer\n",
        "answers_inputs=tf.keras.layers.Input(shape=(None,))\n",
        "\n",
        "#Embedding\n",
        "answers_embedding=tf.keras.layers.Embedding(answers_vocab_size+1, answers_embedding_size)\n",
        "answers_embedding_output=answers_embedding(answers_inputs)\n",
        "\n",
        "#lstm layer\n",
        "answers_lstm= tf.keras.layers.LSTM(rnn_units,return_sequences=True, return_state=True)\n",
        "\n",
        "#LSTM Output, State initialization from Encoder states(concat of question and answer)\n",
        "#Output will be all hidden sequences, last 'h' state and last 'c' state\n",
        "\n",
        "lstm3=answers_lstm(answers_embedding_output,initial_state=decoder_initial_state)\n",
        "\n",
        "#dense layer\n",
        "lstm3_dense= tf.keras.layers.Dense(answers_vocab_size+1,activation='softmax',)\n",
        "\n",
        "#answer output\n",
        "answer_outputs=lstm3_dense(lstm3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14J7GXH2jBcO",
        "colab_type": "text"
      },
      "source": [
        "# Build Model using Encoder ( output of concat) and Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFhlppB6qsgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFboskisjumN",
        "colab_type": "text"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDB4_KhCiqDp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xw8T5_pdOIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcF8YXJ4ZocR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}